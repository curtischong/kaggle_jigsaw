{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_hdf('../input/train.h5')#[:100]\n",
    "test = pd.read_hdf('../input/test.h5')#[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1804874\n",
      "97320\n",
      "18.545766543362106\n"
     ]
    }
   ],
   "source": [
    "print(len(train))\n",
    "print(len(test))\n",
    "print(len(train)/len(test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = train[\"comment_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     This is so cool. It's like, 'would you want yo...\n",
       "1     Thank you!! This would make my life a lot less...\n",
       "2     This is such an urgent design problem; kudos t...\n",
       "3     Is this something I'll be able to install on m...\n",
       "4                  haha you guys are a bunch of losers.\n",
       "5                                  ur a sh*tty comment.\n",
       "6                           hahahahahahahahhha suck it.\n",
       "7                                   FFFFUUUUUUUUUUUUUUU\n",
       "8     The ranchers seem motivated by mostly by greed...\n",
       "9     It was a great show. Not a combo I'd of expect...\n",
       "10                              Wow, that sounds great.\n",
       "11    This is a great story. Man. I wonder if the pe...\n",
       "12       This seems like a step in the right direction.\n",
       "13    It's ridiculous that these guys are being call...\n",
       "14    This story gets more ridiculous by the hour! A...\n",
       "15    I agree; I don't want to grant them the legiti...\n",
       "16    Interesting. I'll be curious to see how this w...\n",
       "17                      Awesome! I love Civil Comments!\n",
       "18    I'm glad you're working on this, and I look fo...\n",
       "19    Angry trolls, misogynists and Racists\", oh my....\n",
       "20    Nice to some attempts to try to make comments ...\n",
       "21    One would hope that the purpose of introducing...\n",
       "22    Comments will be randomly chosen and be review...\n",
       "23    She would be a major improvement for city coun...\n",
       "24    I agree! Comments have so much potential to be...\n",
       "25    Great question! It's one we're asked a lot. We...\n",
       "26    Thanks, Christa!  Will you be adding any featu...\n",
       "27    Our aim is actually the opposite: we want spir...\n",
       "28    Thanks! We're really going to try â€” not only t...\n",
       "29    I applaud Civil's efforts to create some new t...\n",
       "                            ...                        \n",
       "70    Interesting idea. I tend to use content blocke...\n",
       "71    This was quite a comprehensive list. Goody for...\n",
       "72    Ryan Bundy seems like a nice, responsible  hum...\n",
       "73    All the framed boxes must take up extra code a...\n",
       "74    Ah, so part of the \"back end\" and \"algorithms\"...\n",
       "75    To anyone reading this article. That's me and ...\n",
       "76    Thanks, David! And yes, that's a known issue, ...\n",
       "77    Hey i5guy, that's a good point about \"more rep...\n",
       "78    They're providing a service to the sites they'...\n",
       "79    let me some up the heavy vibers feelings:\\n\\nG...\n",
       "80    I thought Willamette week was better than this...\n",
       "81    One more drink related item you missed IPA (th...\n",
       "82    Tuesday Jan. 13, members of \\nthis socalled 'm...\n",
       "83    AND Left wing trolls!\\n\\nJust because someone ...\n",
       "84    Tuesday Jan. 13, members of \\nthis socalled 'm...\n",
       "85    Kay's is one of the best bars in the city, but...\n",
       "86    Did you even go on Hawthorne, or only Yelp it?...\n",
       "87    Has anyone in LDS leadership expressed their d...\n",
       "88    I think Wweek is in a strange spot with that. ...\n",
       "89    I saw @christa_m mention this a little below, ...\n",
       "90    Get rid of the \"more messages\". Especially wit...\n",
       "91    If they offered a Gluten Free Crust we would b...\n",
       "92    Happy to see that the folks \"in the room\" are ...\n",
       "93    Hopefully there will be enough commenters who ...\n",
       "94    Nice. :) Although it's a shame the resolution ...\n",
       "95    If it is effective in slowing us down in order...\n",
       "96    Hi David, we did consider all of these possibi...\n",
       "97                       But *I* want to be MrWhiskers!\n",
       "98    I'm so glad Portland Development Commission, K...\n",
       "99    Sorry, Rod, you'll have to be Mr_Whiskers or M...\n",
       "Name: comment_text, Length: 100, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loads, generate features, then cleans\n",
    "\n",
    "#Generate features\n",
    "#df = pd.concat([train.loc[:, 'qid' : 'question_text'], test], sort = 'False')\n",
    "\n",
    "print(\"--- Generating non_eng\")\n",
    "train[\"non_eng\"] = train[\"question_text\"].map(lambda x: contains_non_english(x))\n",
    "test[\"non_eng\"] = test[\"question_text\"].map(lambda x: contains_non_english(x))\n",
    "print(\"--- Generating first_word\")\n",
    "train[\"first_word\"] = train[\"question_text\"].map(lambda x: get_first_word(x))\n",
    "test[\"first_word\"] = test[\"question_text\"].map(lambda x: get_first_word(x))\n",
    "print(\"--- Generating total_length (num chars)\")\n",
    "train['total_length'] = train['question_text'].apply(len)\n",
    "test['total_length'] = test['question_text'].apply(len)\n",
    "print(\"--- Generating capitals\")\n",
    "train['capitals'] = train['question_text'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n",
    "test['capitals'] = test['question_text'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n",
    "\n",
    "print(\"--- Generating caps_vs_length\")\n",
    "train['caps_vs_length'] = train.apply(lambda row: get_cap_vs_length(row),axis=1)\n",
    "test['caps_vs_length'] = test.apply(lambda row: get_cap_vs_length(row),axis=1)\n",
    "\n",
    "#print(\"--- Generating num_exclamation_marks\")\n",
    "#train['num_exclamation_marks'] = train['question_text'].apply(lambda comment: comment.count('!'))\n",
    "#test['num_exclamation_marks'] = test['question_text'].apply(lambda comment: comment.count('!'))\n",
    "\n",
    "print(\"--- Generating num_question_marks\")\n",
    "train['num_question_marks'] = train['question_text'].apply(lambda comment: comment.count('?'))\n",
    "test['num_question_marks'] = test['question_text'].apply(lambda comment: comment.count('?'))\n",
    "\n",
    "print(\"--- Generating num_punctuation\")\n",
    "train['num_punctuation'] = train['question_text'].apply(lambda comment: sum(comment.count(w) for w in '.,;:'))\n",
    "test['num_punctuation'] = test['question_text'].apply(lambda comment: sum(comment.count(w) for w in '.,;:'))\n",
    "\n",
    "#print(\"--- Generating num_symbols\")\n",
    "#train['num_symbols'] = train['question_text'].apply(lambda comment: sum(comment.count(w) for w in '*&$%'))\n",
    "#test['num_symbols'] = test['question_text'].apply(lambda comment: sum(comment.count(w) for w in '*&$%'))\n",
    "\n",
    "print(\"--- Generating num_words\")\n",
    "train['num_words'] = train['question_text'].apply(lambda comment: len(re.sub(r'[^\\w\\s]','',comment).split(\" \")))\n",
    "test['num_words'] = test['question_text'].apply(lambda comment: len(re.sub(r'[^\\w\\s]','',comment).split(\" \")))\n",
    "\n",
    "print(\"--- Generating num_unique_words\")\n",
    "train['num_unique_words'] = train['question_text'].apply(lambda comment: len(set(w for w in comment.split())))\n",
    "test['num_unique_words'] = test['question_text'].apply(lambda comment: len(set(w for w in comment.split())))\n",
    "\n",
    "print(\"--- Generating words_vs_unique\")\n",
    "train['words_vs_unique'] = train['num_unique_words'] / train['num_words']\n",
    "test['words_vs_unique'] = test['num_unique_words'] / test['num_words']\n",
    "\n",
    "#print(\"--- Generating num_smilies\")\n",
    "#train['num_smilies'] = train['question_text'].apply(lambda comment: sum(comment.count(w) for w in (':-)', ':)', ';-)', ';)')))\n",
    "#test['num_smilies'] = test['question_text'].apply(lambda comment: sum(comment.count(w) for w in (':-)', ':)', ';-)', ';)')))\n",
    "\n",
    "print(\"--- Generating num_sentences\")\n",
    "train['num_sentences'] = train['question_text'].apply(lambda comment: len(re.split(r'[.!?]+', comment)))\n",
    "test['num_sentences'] = test['question_text'].apply(lambda comment: len(re.split(r'[.!?]+', comment)))\n",
    "\n",
    "print(\"--- Generating max_word_len\")\n",
    "train['max_word_len'] = train['question_text'].apply(lambda comment: calc_max_word_len(re.sub(r'[^\\w\\s]','',comment).split(\" \")))\n",
    "test['max_word_len'] = test['question_text'].apply(lambda comment: calc_max_word_len(re.sub(r'[^\\w\\s]','',comment).split(\" \")))\n",
    "\n",
    "print(\"cleaning text\")\n",
    "train[\"question_text\"] = train[\"question_text\"].apply(lambda x: clean_text(x))\n",
    "test[\"question_text\"] = test[\"question_text\"].apply(lambda x: clean_text(x))\n",
    "\n",
    "print(\"remove single characters\")\n",
    "train[\"question_text\"] = train[\"question_text\"].apply(lambda x: remove_singles(x))\n",
    "test[\"question_text\"] = test[\"question_text\"].apply(lambda x: remove_singles(x))\n",
    "\n",
    "print(\"cleaning numbers\")\n",
    "train[\"question_text\"] = train[\"question_text\"].apply(lambda x: clean_numbers(x))\n",
    "test[\"question_text\"] = test[\"question_text\"].apply(lambda x: clean_numbers(x))\n",
    "\n",
    "print(\"cleaning misspellings\")\n",
    "train[\"question_text\"] = train[\"question_text\"].apply(lambda x: replace_typical_misspell(x))\n",
    "test[\"question_text\"] = test[\"question_text\"].apply(lambda x: replace_typical_misspell(x))\n",
    "\n",
    "print(\"filling missing values\")\n",
    "#clean chinese, korean, japanese characters\n",
    "print('cleaning characters')\n",
    "train[\"question_text\"] = train[\"question_text\"].map(lambda x: remove_non_english(x))\n",
    "test[\"question_text\"] = test[\"question_text\"].map(lambda x: remove_non_english(x))\n",
    "\n",
    "## fill up the missing values\n",
    "train[\"question_text\"].fillna(\"\").values\n",
    "test[\"question_text\"].fillna(\"\").values\n",
    "\n",
    "#for getting num good and bad words\n",
    "from wordcloud import STOPWORDS\n",
    "from collections import defaultdict\n",
    "import operator\n",
    "\n",
    "train1_df = train[train[\"target\"]==1]\n",
    "train0_df = train[train[\"target\"]==0]\n",
    "\n",
    "## custom function for ngram generation ##\n",
    "def generate_ngrams(text, n_gram=1):\n",
    "    token = [token for token in text.lower().split(\" \") if token != \"\" if token not in STOPWORDS]\n",
    "    ngrams = zip(*[token[i:] for i in range(n_gram)])\n",
    "    return [\" \".join(ngram) for ngram in ngrams]\n",
    "\n",
    "freq_dict_bad = defaultdict(int)\n",
    "for sent in train1_df[\"question_text\"]:\n",
    "    for word in generate_ngrams(sent):\n",
    "        freq_dict_bad[word] += 1\n",
    "freq_dict_bad = dict(freq_dict_bad)\n",
    "\n",
    "freq_dict_good = defaultdict(int)\n",
    "for sent in train0_df[\"question_text\"]:\n",
    "    for word in generate_ngrams(sent):\n",
    "        freq_dict_good[word] += 1\n",
    "freq_dict_good = dict(freq_dict_good)\n",
    "\n",
    "bad_words = sorted(freq_dict_bad, key=freq_dict_bad.get, reverse=True)[:1000]\n",
    "good_words = sorted(freq_dict_good, key=freq_dict_good.get, reverse=True)[:1000]\n",
    "\n",
    "print(\"--- Generating num_bad_words\")\n",
    "train[\"num_bad_words\"] = train[\"question_text\"].map(lambda x: num_bad_words(x))\n",
    "test[\"num_bad_words\"] = test[\"question_text\"].map(lambda x: num_bad_words(x))\n",
    "\n",
    "print(\"--- Generating num_good_words\")\n",
    "train[\"num_good_words\"] = train[\"question_text\"].map(lambda x: num_good_words(x))\n",
    "test[\"num_good_words\"] = test[\"question_text\"].map(lambda x: num_good_words(x))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
