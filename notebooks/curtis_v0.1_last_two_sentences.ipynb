{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "import random\n",
    "from tqdm._tqdm_notebook import tqdm_notebook as tqdm\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "lc = LancasterStemmer()\n",
    "from nltk.stem import SnowballStemmer\n",
    "sb = SnowballStemmer(\"english\")\n",
    "from keras.preprocessing import text, sequence\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from torch.nn import functional as F\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import spacy\n",
    "import re\n",
    "import gensim\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable progress bars when submitting\n",
    "def is_interactive():\n",
    "   return 'SHLVL' not in os.environ\n",
    "\n",
    "if not is_interactive():\n",
    "    def nop(it, *a, **k):\n",
    "        return it\n",
    "\n",
    "    tqdm = nop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CRAWL_EMBEDDING_PATH = '../input/crawl-300d-2M.vec'\n",
    "GLOVE_EMBEDDING_PATH = '../input/glove.840B.300d.txt'\n",
    "NUM_MODELS = 2\n",
    "LSTM_UNITS = 128\n",
    "DENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\n",
    "MAX_LEN = 220"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nstart_time = time.time()\\nspell_model = gensim.models.KeyedVectors.load_word2vec_format(\\'../input/wiki-news-300d-1M.vec\\')\\nwords = spell_model.index2word\\nw_rank = {}\\nfor i,word in enumerate(words):\\n    w_rank[word] = i\\nWORDS = w_rank\\ndel words\\ndel w_rank\\ndel spell_model\\ngc.collect()\\nprint(\"--- %s seconds ---\" % (time.time() - start_time))\\n\\n# Use fast text as vocabulary\\ndef words(text): return re.findall(r\\'\\\\w+\\', text.lower())\\ndef P(word): \\n    \"Probability of `word`.\"\\n    # use inverse of rank as proxy\\n    # returns 0 if the word isn\\'t in the dictionary\\n    return - WORDS.get(word, 0)\\ndef correction(word): \\n    \"Most probable spelling correction for word.\"\\n    return max(candidates(word), key=P)\\ndef candidates(word): \\n    \"Generate possible spelling corrections for word.\"\\n    return (known([word]) or known(edits1(word)) or [word])\\ndef known(words): \\n    \"The subset of `words` that appear in the dictionary of WORDS.\"\\n    return set(w for w in words if w in WORDS)\\ndef edits1(word):\\n    \"All edits that are one edit away from `word`.\"\\n    letters    = \\'abcdefghijklmnopqrstuvwxyz\\'\\n    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\\n    deletes    = [L + R[1:]               for L, R in splits if R]\\n    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\\n    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\\n    inserts    = [L + c + R               for L, R in splits for c in letters]\\n    return set(deletes + transposes + replaces + inserts)\\ndef edits2(word): \\n    \"All edits that are two edits away from `word`.\"\\n    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\\ndef singlify(word):\\n    return \"\".join([letter for i,letter in enumerate(word) if i == 0 or letter != word[i-1]])\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "start_time = time.time()\n",
    "spell_model = gensim.models.KeyedVectors.load_word2vec_format('../input/wiki-news-300d-1M.vec')\n",
    "words = spell_model.index2word\n",
    "w_rank = {}\n",
    "for i,word in enumerate(words):\n",
    "    w_rank[word] = i\n",
    "WORDS = w_rank\n",
    "del words\n",
    "del w_rank\n",
    "del spell_model\n",
    "gc.collect()\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# Use fast text as vocabulary\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "def P(word): \n",
    "    \"Probability of `word`.\"\n",
    "    # use inverse of rank as proxy\n",
    "    # returns 0 if the word isn't in the dictionary\n",
    "    return - WORDS.get(word, 0)\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or [word])\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "def singlify(word):\n",
    "    return \"\".join([letter for i,letter in enumerate(word) if i == 0 or letter != word[i-1]])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef build_matrix(word_dict, lemma_dict, path):\\n    embed_size = 300\\n    embeddings_index = load_embeddings(path)\\n    embedding_matrix = np.zeros((len(word_dict) + 1, embed_size), dtype=np.float32)\\n    unknown_words = []\\n    unknown_vector = np.zeros((embed_size,), dtype=np.float32) - 1\\n    \\n    for key in tqdm(word_dict):\\n        word = key\\n        embedding_vector = embeddings_index.get(word)\\n        if embedding_vector is not None:\\n            embedding_matrix[word_dict[key]] = embedding_vector\\n            continue\\n        word = key.lower()\\n        embedding_vector = embeddings_index.get(word)\\n        if embedding_vector is not None:\\n            embedding_matrix[word_dict[key]] = embedding_vector\\n            continue\\n        word = key.upper()\\n        embedding_vector = embeddings_index.get(word)\\n        if embedding_vector is not None:\\n            embedding_matrix[word_dict[key]] = embedding_vector\\n            continue\\n        word = key.capitalize()\\n        embedding_vector = embeddings_index.get(word)\\n        if embedding_vector is not None:\\n            embedding_matrix[word_dict[key]] = embedding_vector\\n            continue\\n        word = ps.stem(key)\\n        embedding_vector = embeddings_index.get(word)\\n        if embedding_vector is not None:\\n            embedding_matrix[word_dict[key]] = embedding_vector\\n            continue\\n        word = lc.stem(key)\\n        embedding_vector = embeddings_index.get(word)\\n        if embedding_vector is not None:\\n            embedding_matrix[word_dict[key]] = embedding_vector\\n            continue\\n        word = sb.stem(key)\\n        embedding_vector = embeddings_index.get(word)\\n        if embedding_vector is not None:\\n            embedding_matrix[word_dict[key]] = embedding_vector\\n            continue\\n        word = lemma_dict[key]\\n        embedding_vector = embeddings_index.get(word)\\n        if embedding_vector is not None:\\n            embedding_matrix[word_dict[key]] = embedding_vector\\n            continue\\n        if len(key) > 1:\\n            word = correction(key)\\n            embedding_vector = embeddings_index.get(word)\\n            if embedding_vector is not None:\\n                embedding_matrix[word_dict[key]] = embedding_vector\\n                continue\\n        \\n        #Unknown word, does not exist in dictionary\\n        embedding_matrix[word_dict[key]] = unknown_vector\\n        unknown_words.append(word)\\n    return embedding_matrix, unknown_words\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "def load_embeddings(path):\n",
    "    with open(path) as f:\n",
    "        return dict(get_coefs(*line.strip().split(' ')) for line in tqdm(f))\n",
    "    \n",
    "def build_matrix(word_index, path):\n",
    "    embedding_index = load_embeddings(path)\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "    unknown_words = []\n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        try:\n",
    "            embedding_matrix[i] = embedding_index[word]\n",
    "        except KeyError:\n",
    "            unknown_words.append(word)\n",
    "    return embedding_matrix, unknown_words\n",
    "\n",
    "'''\n",
    "def build_matrix(word_dict, lemma_dict, path):\n",
    "    embed_size = 300\n",
    "    embeddings_index = load_embeddings(path)\n",
    "    embedding_matrix = np.zeros((len(word_dict) + 1, embed_size), dtype=np.float32)\n",
    "    unknown_words = []\n",
    "    unknown_vector = np.zeros((embed_size,), dtype=np.float32) - 1\n",
    "    \n",
    "    for key in tqdm(word_dict):\n",
    "        word = key\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = key.lower()\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = key.upper()\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = key.capitalize()\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = ps.stem(key)\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = lc.stem(key)\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = sb.stem(key)\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = lemma_dict[key]\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        if len(key) > 1:\n",
    "            word = correction(key)\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[word_dict[key]] = embedding_vector\n",
    "                continue\n",
    "        \n",
    "        #Unknown word, does not exist in dictionary\n",
    "        embedding_matrix[word_dict[key]] = unknown_vector\n",
    "        unknown_words.append(word)\n",
    "    return embedding_matrix, unknown_words\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceBucketCollator():\n",
    "    def __init__(self, choose_length, sequence_index, length_index, label_index=None):\n",
    "        self.choose_length = choose_length\n",
    "        self.sequence_index = sequence_index\n",
    "        self.length_index = length_index\n",
    "        self.label_index = label_index\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        batch = [torch.stack(x) for x in list(zip(*batch))]\n",
    "        \n",
    "        sequences = batch[self.sequence_index]\n",
    "        lengths = batch[self.length_index]\n",
    "        \n",
    "        length = self.choose_length(lengths)\n",
    "        mask = torch.arange(start=maxlen, end=0, step=-1) < length\n",
    "        padded_sequences = sequences[:, mask]\n",
    "        \n",
    "        batch[self.sequence_index] = padded_sequences\n",
    "        \n",
    "        if self.label_index is not None:\n",
    "            return [x for i, x in enumerate(batch) if i != self.label_index], batch[self.label_index]\n",
    "    \n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "filepath = './model_files/checkpoint.pth'\n",
    "\n",
    "def train_model(model, train, test, loss_fn, output_dim, lr=0.001,\n",
    "                batch_size=512, n_epochs=4, n_epochs_embed=2,\n",
    "                enable_checkpoint_ensemble=True):\n",
    "    \n",
    "    train_collator = SequenceBucketCollator(lambda lengths: lengths.max(), \n",
    "                                            sequence_index=0, \n",
    "                                            length_index=1, \n",
    "                                            label_index=2)\n",
    "    \n",
    "    param_lrs = [{'params': param, 'lr': lr} for param in model.parameters()]\n",
    "    optimizer = torch.optim.Adam(param_lrs, lr=lr)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda epoch: 0.6 ** epoch)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True, collate_fn=train_collator)\n",
    "    val_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False, collate_fn=train_collator)\n",
    "    all_test_preds = []\n",
    "    checkpoint_weights = [2 ** epoch for epoch in range(n_epochs)]\n",
    "    \n",
    "    best_loss = 1\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        model.train() #set model to train mode\n",
    "        avg_loss = 0.\n",
    "        \n",
    "        for data in tqdm(train_loader, disable=False):\n",
    "            \n",
    "            #training loop\n",
    "            x_batch = data[:-1]\n",
    "            #print(\"First: \", x_batch[0][0])\n",
    "            #print(\"Second: \", x_batch[0][1])\n",
    "            first = x_batch[0][0]\n",
    "            second = x_batch[0][1]\n",
    "            \n",
    "            y_batch = data[-1]\n",
    "\n",
    "            y_pred = model(first, second)  #feed data into model          \n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            \n",
    "            #calculate error and adjust model params\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item() / len(train_loader)\n",
    "        \n",
    "        #Check if loss is better than current best loss, if so, save the model\n",
    "        is_best = (avg_loss < best_loss)\n",
    "        \n",
    "        if is_best:\n",
    "            print (\"=> Saving a new best\")\n",
    "            best_loss = avg_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_loss': best_loss\n",
    "            }, filepath)  # save checkpoint\n",
    "        else:\n",
    "            print (\"=> Model Accuracy did not improve\")\n",
    "            \n",
    "        \n",
    "        model.eval() #set model to eval mode for test data\n",
    "        test_preds = np.zeros((len(test), output_dim))\n",
    "        \n",
    "    \n",
    "        for i, x_batch in enumerate(val_loader):\n",
    "            #print(\"X_Batch: \", x_batch)\n",
    "            data_param = x_batch[0][0]\n",
    "            lengths_param = x_batch[0][1]\n",
    "            y_pred = sigmoid(model(data_param, lengths_param).detach().cpu().numpy()) #feed data into model\n",
    "\n",
    "            test_preds[i * batch_size:(i+1) * batch_size, :] = y_pred #get test predictions\n",
    "        \n",
    "        #test_preds has the predictions for the entire test set now\n",
    "        all_test_preds.append(test_preds) #append predictions to the record of all past predictions\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print('Epoch {}/{} \\t loss={:.4f} \\t time={:.2f}s'.format(\n",
    "              epoch + 1, n_epochs, avg_loss, elapsed_time))\n",
    "        \n",
    "    #Make embeddings layer only layer unfreezed, train again (literally run through the n_epochs)\n",
    "    #maybe define a n_epochs_embedding\n",
    "    \n",
    "    #parameters = model.parameters()\n",
    "    #for param in parameters:\n",
    "    #        param.requires_grad = False\n",
    "    #parameters[0].requires_grad = True\n",
    "    \n",
    "    '''\n",
    "    ct = 0\n",
    "    for child in model.children():\n",
    "        if ct == 0:\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = True\n",
    "        else:\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = False\n",
    "        ct += 1\n",
    "    \n",
    "    for epoch in range(n_epochs_embed):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        model.train() #set model to train mode\n",
    "        avg_loss = 0.\n",
    "        \n",
    "        for data in tqdm(train_loader, disable=False):\n",
    "            \n",
    "            #training loop\n",
    "            x_batch = data[:-1]\n",
    "            y_batch = data[-1]\n",
    "\n",
    "            y_pred = model(*x_batch)  #feed data into model          \n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            \n",
    "            #calculate error and adjust model params\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item() / len(train_loader) #gets the loss per epoch\n",
    "        \n",
    "            \n",
    "        model.eval() #set model to eval mode for test data\n",
    "        test_preds = np.zeros((len(test), output_dim))\n",
    "    \n",
    "        for i, x_batch in enumerate(test_loader):\n",
    "            y_pred = sigmoid(model(*x_batch).detach().cpu().numpy()) #feed data into model\n",
    "\n",
    "            test_preds[i * batch_size:(i+1) * batch_size, :] = y_pred #get test predictions\n",
    "        \n",
    "        #test_preds has the predictions for the entire test set now\n",
    "        #all_test_preds.append(test_preds) #append predictions to the record of all past predictions\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print('[EMBEDDING TRAINING] Epoch {}/{} \\t loss={:.4f} \\t time={:.2f}s '.format(\n",
    "              epoch + 1, n_epochs_embed, avg_loss, elapsed_time))\n",
    "    '''\n",
    "    \n",
    "    #PREDICTION CODE\n",
    "    '''\n",
    "    if enable_checkpoint_ensemble:\n",
    "        #if our approach is an ensemble then we average it amongst all the historical predictions\n",
    "        test_preds = np.average(all_test_preds, weights=checkpoint_weights, axis=0)    \n",
    "    else:\n",
    "        #if our approach is not an ensemble then we just take the last set of predictions\n",
    "        test_preds = all_test_preds[-1]\n",
    "        \n",
    "    return test_preds\n",
    "    '''\n",
    "    \n",
    "    #return trained model\n",
    "    return model\n",
    "\n",
    "def predict(model, test, output_dim, batch_size=512, pred_type=\"val\"):\n",
    "    checkpoint = torch.load(filepath)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    #optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    if pred_type == \"test\":\n",
    "        test_collator = SequenceBucketCollator(lambda lengths: lengths.max(), sequence_index=0, length_index=1)\n",
    "        test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False, collate_fn=test_collator)\n",
    "\n",
    "        model.eval() #set model to eval mode for test data\n",
    "        test_preds = np.zeros((len(test), output_dim))\n",
    "\n",
    "        for i, x_batch in enumerate(test_loader):\n",
    "            #print(x_batch[0])\n",
    "            data_param = x_batch[0]\n",
    "            lengths_param = x_batch[1]\n",
    "            y_pred = sigmoid(model(data_param, lengths_param).detach().cpu().numpy()) #feed data into model\n",
    "            test_preds[i * batch_size:(i+1) * batch_size, :] = y_pred #get test predictions\n",
    "\n",
    "        return test_preds\n",
    "    else:\n",
    "        test_collator = SequenceBucketCollator(lambda lengths: lengths.max(), sequence_index=0, length_index=1, label_index=2)\n",
    "        test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False, collate_fn=test_collator)\n",
    "\n",
    "        model.eval() #set model to eval mode for test data\n",
    "        test_preds = np.zeros((len(test), output_dim))\n",
    "\n",
    "        for i, x_batch in enumerate(test_loader):\n",
    "            #print(x_batch)\n",
    "            data_param = x_batch[0][0]\n",
    "            lengths_param = x_batch[0][1]\n",
    "            y_pred = sigmoid(model(data_param, lengths_param).detach().cpu().numpy()) #feed data into model\n",
    "            test_preds[i * batch_size:(i+1) * batch_size, :] = y_pred #get test predictions\n",
    "\n",
    "        return test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialDropout(nn.Dropout2d):\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(2)    # (N, T, 1, K)\n",
    "        x = x.permute(0, 3, 2, 1)  # (N, K, 1, T)\n",
    "        #call the forward method in Dropout2d (super function specifies the subclass and instance)\n",
    "        x = super(SpatialDropout, self).forward(x)  # (N, K, 1, T), some features are masked\n",
    "        x = x.permute(0, 3, 2, 1)  # (N, T, 1, K)\n",
    "        x = x.squeeze(2)  # (N, T, K)\n",
    "        return x\n",
    "    \n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, embedding_matrix, num_aux_targets):\n",
    "        #call the init mthod in Module (super function specifies the subclass and instance)\n",
    "        super(NeuralNet, self).__init__() \n",
    "        embed_size = embedding_matrix.shape[1]\n",
    "        \n",
    "        self.embedding = nn.Embedding(max_features, embed_size)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.embedding_dropout = SpatialDropout(0.3)\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(embed_size, LSTM_UNITS, bidirectional=True, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(LSTM_UNITS * 2, LSTM_UNITS, bidirectional=True, batch_first=True)\n",
    "    \n",
    "        self.linear1 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n",
    "        self.linear2 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n",
    "        \n",
    "        self.linear_out = nn.Linear(DENSE_HIDDEN_UNITS, 1)\n",
    "        self.linear_aux_out = nn.Linear(DENSE_HIDDEN_UNITS, num_aux_targets)\n",
    "        \n",
    "    def forward(self, x, lengths=None):\n",
    "        h_embedding = self.embedding(x)\n",
    "        h_embedding = self.embedding_dropout(h_embedding)\n",
    "        \n",
    "        #first variable h_(lstm #) holds the output, _ is the (hidden state, cell state)\n",
    "        h_lstm1, _ = self.lstm1(h_embedding) \n",
    "        h_lstm2, _ = self.lstm2(h_lstm1)\n",
    "        \n",
    "        # global average pooling\n",
    "        avg_pool = torch.mean(h_lstm2, 1) #get the mean value of the first dimension in h_lstm2\n",
    "        # global max pooling\n",
    "        max_pool, _ = torch.max(h_lstm2, 1) #get the max value of the first dimension in h_lstm2\n",
    "        \n",
    "        h_conc = torch.cat((max_pool, avg_pool), 1)\n",
    "        h_conc_linear1  = F.relu(self.linear1(h_conc))\n",
    "        h_conc_linear2  = F.relu(self.linear2(h_conc))\n",
    "        \n",
    "        hidden = h_conc + h_conc_linear1 + h_conc_linear2\n",
    "        \n",
    "        result = self.linear_out(hidden)\n",
    "        aux_result = self.linear_aux_out(hidden)\n",
    "        out = torch.cat([result, aux_result], 1)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def preprocess(data):\n",
    "\n",
    "    punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~`\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n",
    "    def clean_special_chars(text, punct):\n",
    "        for p in punct:\n",
    "            text = text.replace(p, ' ')\n",
    "        return text\n",
    "\n",
    "    data = data.astype(str).apply(lambda x: clean_special_chars(x, punct))\n",
    "    return data\n",
    "'''\n",
    "symbols_to_isolate = '.,?!-;*\"…:—()%#$&_/@＼・ω+=”“[]^–>\\\\°<~•≠™ˈʊɒ∞§{}·τα❤☺ɡ|¢→̶`❥━┣┫┗Ｏ►★©―ɪ✔®\\x96\\x92●£♥➤´¹☕≈÷♡◐║▬′ɔː€۩۞†μ✒➥═☆ˌ◄½ʻπδηλσερνʃ✬ＳＵＰＥＲＩＴ☻±♍µº¾✓◾؟．⬅℅»Вав❣⋅¿¬♫ＣＭβ█▓▒░⇒⭐›¡₂₃❧▰▔◞▀▂▃▄▅▆▇↙γ̄″☹➡«φ⅓„✋：¥̲̅́∙‛◇✏▷❓❗¶˚˙）сиʿ✨。ɑ\\x80◕！％¯−ﬂﬁ₁²ʌ¼⁴⁄₄⌠♭✘╪▶☭✭♪☔☠♂☃☎✈✌✰❆☙○‣⚓年∎ℒ▪▙☏⅛ｃａｓǀ℮¸ｗ‚∼‖ℳ❄←☼⋆ʒ⊂、⅔¨͡๏⚾⚽Φ×θ￦？（℃⏩☮⚠月✊❌⭕▸■⇌☐☑⚡☄ǫ╭∩╮，例＞ʕɐ̣Δ₀✞┈╱╲▏▕┃╰▊▋╯┳┊≥☒↑☝ɹ✅☛♩☞ＡＪＢ◔◡↓♀⬆̱ℏ\\x91⠀ˤ╚↺⇤∏✾◦♬³の｜／∵∴√Ω¤☜▲↳▫‿⬇✧ｏｖｍ－２０８＇‰≤∕ˆ⚜☁'\n",
    "symbols_to_delete = '\\n🍕\\r🐵😑\\xa0\\ue014\\t\\uf818\\uf04a\\xad😢🐶️\\uf0e0😜😎👊\\u200b\\u200e😁عدويهصقأناخلىبمغر😍💖💵Е👎😀😂\\u202a\\u202c🔥😄🏻💥ᴍʏʀᴇɴᴅᴏᴀᴋʜᴜʟᴛᴄᴘʙғᴊᴡɢ😋👏שלוםבי😱‼\\x81エンジ故障\\u2009🚌ᴵ͞🌟😊😳😧🙀😐😕\\u200f👍😮😃😘אעכח💩💯⛽🚄🏼ஜ😖ᴠ🚲‐😟😈💪🙏🎯🌹😇💔😡\\x7f👌ἐὶήιὲκἀίῃἴξ🙄Ｈ😠\\ufeff\\u2028😉😤⛺🙂\\u3000تحكسة👮💙فزط😏🍾🎉😞\\u2008🏾😅😭👻😥😔😓🏽🎆🍻🍽🎶🌺🤔😪\\x08‑🐰🐇🐱🙆😨🙃💕𝘊𝘦𝘳𝘢𝘵𝘰𝘤𝘺𝘴𝘪𝘧𝘮𝘣💗💚地獄谷улкнПоАН🐾🐕😆ה🔗🚽歌舞伎🙈😴🏿🤗🇺🇸мυтѕ⤵🏆🎃😩\\u200a🌠🐟💫💰💎эпрд\\x95🖐🙅⛲🍰🤐👆🙌\\u2002💛🙁👀🙊🙉\\u2004ˢᵒʳʸᴼᴷᴺʷᵗʰᵉᵘ\\x13🚬🤓\\ue602😵άοόςέὸתמדףנרךצט😒͝🆕👅👥👄🔄🔤👉👤👶👲🔛🎓\\uf0b7\\uf04c\\x9f\\x10成都😣⏺😌🤑🌏😯ех😲Ἰᾶὁ💞🚓🔔📚🏀👐\\u202d💤🍇\\ue613小土豆🏡❔⁉\\u202f👠》कर्मा🇹🇼🌸蔡英文🌞🎲レクサス😛外国人关系Сб💋💀🎄💜🤢َِьыгя不是\\x9c\\x9d🗑\\u2005💃📣👿༼つ༽😰ḷЗз▱ц￼🤣卖温哥华议会下降你失去所有的钱加拿大坏税骗子🐝ツ🎅\\x85🍺آإشء🎵🌎͟ἔ油别克🤡🤥😬🤧й\\u2003🚀🤴ʲшчИОРФДЯМюж😝🖑ὐύύ特殊作戦群щ💨圆明园קℐ🏈😺🌍⏏ệ🍔🐮🍁🍆🍑🌮🌯🤦\\u200d𝓒𝓲𝓿𝓵안영하세요ЖљКћ🍀😫🤤ῦ我出生在了可以说普通话汉语好极🎼🕺🍸🥂🗽🎇🎊🆘🤠👩🖒🚪天一家⚲\\u2006⚭⚆⬭⬯⏖新✀╌🇫🇷🇩🇪🇮🇬🇧😷🇨🇦ХШ🌐\\x1f杀鸡给猴看ʁ𝗪𝗵𝗲𝗻𝘆𝗼𝘂𝗿𝗮𝗹𝗶𝘇𝗯𝘁𝗰𝘀𝘅𝗽𝘄𝗱📺ϖ\\u2000үսᴦᎥһͺ\\u2007հ\\u2001ɩｙｅ൦ｌƽｈ𝐓𝐡𝐞𝐫𝐮𝐝𝐚𝐃𝐜𝐩𝐭𝐢𝐨𝐧Ƅᴨןᑯ໐ΤᏧ௦Іᴑ܁𝐬𝐰𝐲𝐛𝐦𝐯𝐑𝐙𝐣𝐇𝐂𝐘𝟎ԜТᗞ౦〔Ꭻ𝐳𝐔𝐱𝟔𝟓𝐅🐋ﬃ💘💓ё𝘥𝘯𝘶💐🌋🌄🌅𝙬𝙖𝙨𝙤𝙣𝙡𝙮𝙘𝙠𝙚𝙙𝙜𝙧𝙥𝙩𝙪𝙗𝙞𝙝𝙛👺🐷ℋ𝐀𝐥𝐪🚶𝙢Ἱ🤘ͦ💸ج패티Ｗ𝙇ᵻ👂👃ɜ🎫\\uf0a7БУі🚢🚂ગુજરાતીῆ🏃𝓬𝓻𝓴𝓮𝓽𝓼☘﴾̯﴿₽\\ue807𝑻𝒆𝒍𝒕𝒉𝒓𝒖𝒂𝒏𝒅𝒔𝒎𝒗𝒊👽😙\\u200cЛ‒🎾👹⎌🏒⛸公寓养宠物吗🏄🐀🚑🤷操美𝒑𝒚𝒐𝑴🤙🐒欢迎来到阿拉斯ספ𝙫🐈𝒌𝙊𝙭𝙆𝙋𝙍𝘼𝙅ﷻ🦄巨收赢得白鬼愤怒要买额ẽ🚗🐳𝟏𝐟𝟖𝟑𝟕𝒄𝟗𝐠𝙄𝙃👇锟斤拷𝗢𝟳𝟱𝟬⦁マルハニチロ株式社⛷한국어ㄸㅓ니͜ʖ𝘿𝙔₵𝒩ℯ𝒾𝓁𝒶𝓉𝓇𝓊𝓃𝓈𝓅ℴ𝒻𝒽𝓀𝓌𝒸𝓎𝙏ζ𝙟𝘃𝗺𝟮𝟭𝟯𝟲👋🦊多伦🐽🎻🎹⛓🏹🍷🦆为和中友谊祝贺与其想象对法如直接问用自己猜本传教士没积唯认识基督徒曾经让相信耶稣复活死怪他但当们聊些政治题时候战胜因圣把全堂结婚孩恐惧且栗谓这样还♾🎸🤕🤒⛑🎁批判检讨🏝🦁🙋😶쥐스탱트뤼도석유가격인상이경제황을렵게만들지않록잘관리해야합다캐나에서대마초와화약금의품런성분갈때는반드시허된사용🔫👁凸ὰ💲🗯𝙈Ἄ𝒇𝒈𝒘𝒃𝑬𝑶𝕾𝖙𝖗𝖆𝖎𝖌𝖍𝖕𝖊𝖔𝖑𝖉𝖓𝖐𝖜𝖞𝖚𝖇𝕿𝖘𝖄𝖛𝖒𝖋𝖂𝕴𝖟𝖈𝕸👑🚿💡知彼百\\uf005𝙀𝒛𝑲𝑳𝑾𝒋𝟒😦𝙒𝘾𝘽🏐𝘩𝘨ὼṑ𝑱𝑹𝑫𝑵𝑪🇰🇵👾ᓇᒧᔭᐃᐧᐦᑳᐨᓃᓂᑲᐸᑭᑎᓀᐣ🐄🎈🔨🐎🤞🐸💟🎰🌝🛳点击查版🍭𝑥𝑦𝑧ＮＧ👣\\uf020っ🏉ф💭🎥Ξ🐴👨🤳🦍\\x0b🍩𝑯𝒒😗𝟐🏂👳🍗🕉🐲چی𝑮𝗕𝗴🍒ꜥⲣⲏ🐑⏰鉄リ事件ї💊「」\\uf203\\uf09a\\uf222\\ue608\\uf202\\uf099\\uf469\\ue607\\uf410\\ue600燻製シ虚偽屁理屈Г𝑩𝑰𝒀𝑺🌤𝗳𝗜𝗙𝗦𝗧🍊ὺἈἡχῖΛ⤏🇳𝒙ψՁմեռայինրւդձ冬至ὀ𝒁🔹🤚🍎𝑷🐂💅𝘬𝘱𝘸𝘷𝘐𝘭𝘓𝘖𝘹𝘲𝘫کΒώ💢ΜΟΝΑΕ🇱♲𝝈↴💒⊘Ȼ🚴🖕🖤🥘📍👈➕🚫🎨🌑🐻𝐎𝐍𝐊𝑭🤖🎎😼🕷ｇｒｎｔｉｄｕｆｂｋ𝟰🇴🇭🇻🇲𝗞𝗭𝗘𝗤👼📉🍟🍦🌈🔭《🐊🐍\\uf10aლڡ🐦\\U0001f92f\\U0001f92a🐡💳ἱ🙇𝗸𝗟𝗠𝗷🥜さようなら🔼'\n",
    "\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "nltk_tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "\n",
    "isolate_dict = {ord(c):f' {c} ' for c in symbols_to_isolate}\n",
    "remove_dict = {ord(c):f'' for c in symbols_to_delete}\n",
    "\n",
    "\n",
    "def handle_punctuation(x):\n",
    "    x = x.translate(remove_dict)\n",
    "    x = x.translate(isolate_dict)\n",
    "    return x\n",
    "\n",
    "def handle_contractions(x):\n",
    "    x = nltk_tokenizer.tokenize(x)\n",
    "    return x\n",
    "\n",
    "def fix_quote(x):\n",
    "    x = [x_[1:] if x_.startswith(\"'\") else x_ for x_ in x]\n",
    "    x = ' '.join(x)\n",
    "    return x\n",
    "\n",
    "symbols_iv = \"\"\"?,./-()\"$=…*&+′[ɾ̃]%:^\\xa0\\\\{}–“”;!<`®ạ°#²|~√_α→>—£，。´×@π÷？ʿ€の↑∞ʻ℅в•−а年！∈∩⊆§℃θ±≤͡⁴™си≠∂³ி½△¿¼∆≥⇒¬∨∫▾Ω＾γµº♭ー̂ɔ∑εντσ日Γ∪φβ¹∘¨″⅓ɑː✅✓（）∠«»்ுλ∧∀،＝ɨʋδɒ¸☹μΔʃɸηΣ₅₆◦·ВΦ☺❤♨✌≡ʌʊா≈⁰‛：ﬁ„¾ρ⟨⟩˂⅔≅－＞¢⁸ʒは⬇♀؟¡⋅ɪ₁₂ɤ◌ʱ、▒ْ；☉＄∴✏ωɹ̅।ـ☝♏̉̄♡₄∼́̀⁶⁵¦¶ƒˆ‰©¥∅・ﾟ⊥ª†ℕ│ɡ∝♣／☁✔❓∗➡ℝ位⎛⎝¯⎞⎠↓ɐ∇⋯˚⁻ˈ₃⊂˜̸̵̶̷̴̡̲̳̱̪̗̣̖̎̿͂̓̑̐̌̾̊̕\\x92\"\"\"        \n",
    "\n",
    "def split_off_symbols_iv(x):\n",
    "    for punct in symbols_iv:\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    return x\n",
    "\n",
    "alphabets= \"([A-Za-z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr|Prof|Capt|Cpt|Lt|Mt)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov|me|edu)\"\n",
    "digits = \"([0-9])\"\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = re.sub(digits + \"[.]\" + digits,\"\\\\1<prd>\\\\2\",text)\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    if \"...\" in text: text = text.replace(\"...\",\"<prd><prd><prd>\")\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    \n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    return sentences\n",
    "\n",
    "def preprocess(x):\n",
    "    x = re.sub(r'[\\?\\.\\!]+(?=[\\?\\.\\!])', '', x)\n",
    "    x = split_off_symbols_iv(x) # HERE WE CLEAN THE TEXT BEFORE WE SPLIT|\n",
    "    x = ' '.join(split_into_sentences(x)[-2:])\n",
    "    print(x)\n",
    "    #x = handle_punctuation(x)\n",
    "    #x = handle_contractions(x)\n",
    "    #x = fix_quote(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's like ,  'would you want your mother to read this ? ' Really great idea ,  well done !\n",
      "This would make my life a lot less anxiety - inducing . Keep it up ,  and don't let anyone get in your way !\n",
      "This is such an urgent design problem ;  kudos to you for taking it on . Very impressive !\n",
      "Is this something I'll be able to install on my site ? When will you be releasing it ?\n",
      "haha you guys are a bunch of losers .\n",
      "ur a sh * tty comment .\n",
      "hahahahahahahahhha suck it .\n",
      "\n",
      "The ranchers seem motivated by mostly by greed ;  no one should have the right to allow their animals destroy public land .\n",
      "It was a great show . Not a combo I'd of expected to be good together but it was .\n",
      "Wow ,  that sounds great .\n",
      "I wonder if the person who yelled  \" shut the fuck up ! \"  at him ever heard it .\n",
      "This seems like a step in the right direction .\n",
      "It's ridiculous that these guys are being called  \" protesters \"  . Being armed is a threat of violence ,  which makes them terrorists .\n",
      "And ,  I love that people are sending these guys dildos in the mail now . But …  if they really think there's a happy ending in this for any of them ,  I think they're even more deluded than all of the jokes about them assume .\n",
      "They're greedy ,  small - minded people who somehow seem to share the mass delusion that this is not only a good idea for themselves as individuals ,  but is the right thing to do for ranchers at large . Basically :  take something that currently belongs to everyone ,  and give it to a select group of people ,  so they can profit .\n",
      "I'll be curious to see how this works out . I often refrain from commenting because I don't have the time or desire to engage with the couple of resident trolls who seem to jump on every active WW comment thread .\n",
      "Awesome ! I love Civil Comments !\n",
      "One of the reasons I rely on Reddit as a platform for news and local discussions is that there's a sense of community interaction that's often lacking in my  \" real life ,  \"  hectic discussions . But hopefully we won't be tempted to silence those who take unpopular stances .\n",
      "From now on ,  winning arguments against any member of diversity will be considered offensive language . facts ,  cogent ,  linear posts and Math are now verboten .\n",
      "Nice to some attempts to try to make comments better — it feels like any innovation in commenting communities ended with the launch of Disqus nearly a decade ago .\n",
      "I'm believing that this is the intention of your newspaper here ,  and I hope it's a movement that results in more ,  not less ,  commentary on current events . We need it .\n",
      "Comments will be randomly chosen and be reviewed by more than one person . But I bet the Civil people have an even better ,  more thorough answer to this question !\n",
      "She would be a major improvement for city council and she has a long history of giving all citizens a voice . She is a problem solver that is what Portland needs  .\n",
      "That's also why there are two separate questions :   \" is this a  * good *  comment ? \"  and  \" is this comment civil ?\n",
      "So ,  in addition to the peer reviews ,  there are algorithms on the backend doing a lot of meta - analysis . I'm sure the system isn't 100 %  perfect yet ,  but we know from months of beta testing that it's a really solid start ,  and we'll keep working to improve it !\n",
      "Will you be adding any features to allow overall  \" upvotes \"  of the article itself ? Also ,  notification settings for users ?\n",
      "There's a lot going on on the backend to prevent abuse of the system . So ,  everyone is free to speak their mind ,  even if their opinion is unpopular — so long as they treat each other with respect .\n",
      "Thanks ! We're really going to try  —  not only to improve civility ,  but also to make comments more dynamic and interesting .\n",
      "I applaud Civil's efforts to create some new technology in this field . Hoping for more thoughtful discussions moving forward .\n",
      "I understand that you work for WW . ( but that hardly seems like an excuse when Fred Armisen recorded radio voices for Grand Theft Auto IV with the same number of people .\n",
      "So why is it okay to smear an entire religion over these few idiots ? Or is this because it's okay to bash Christian sects ?\n",
      "It's a brand - new way to approach comments ,  and we're going to continue to improve it . We welcome all suggestions and feedback !\n",
      "Troll free since 2016 .\n",
      "This bitch is nuts . Who would read a book by a woman .\n",
      "Awesome !\n",
      "Not for long !\n",
      "Commenting for the sake of commenting to rate other comments to better comment on our new comment system .\n",
      "Interesting concept . How does Civil Comments plan to monetize their operation ?\n",
      "The Mash Tun was a favorite beer bar for all the delicious tempeh stuff . Excited the staff is still there ,  can't wait to try the new beers .\n",
      "com  -  they have enough online visitors and an established  \" troll \"  base to really need this service . With as little online activity as wweek has  -  it will discourage further growth here .\n",
      "Very cool project !\n",
      "Signed up just to give this a shot . good luck to your enterprise !\n",
      "disqus has way more functionality and obviously a huge following  -  it can increase visitor count significantly . disqus also has a moderator feature  -  a feature that wweek either never wanted to pay their staff to undertake ,  or never developed a group of people they could trust to do it on their behalf on a volunteer basis .\n",
      "PORTLAND HAS BEEN GOING DOWNHILL FOR YEARS ,  NO WONDER THE TEA PARTY IS MAKING A COMEBACK . WHATS NEXT FLOURAIDE IN THE WATER SUPPLY ?\n",
      "I'm interested in where this system works and where it's having problems . Do you have a link you could share that has comments like you mentioned ?\n",
      "I'm crazy about that illustration ! I never would have thought to pitch everything so yellow and orange .\n",
      "Not to mention a killer new veggie burger ,  vegan tacos ,  and several nice salads . Hope to see you there soon !\n",
      "I love WW content but the comment section has polluted for far too long . The lack of community moderation gave a loud megaphone to a small number of right wing trolls .\n",
      "Probably because they consistently waste funds on trendy projects like green bike boxes  -  -  which cost a fortune and have to be repainted every couple of years . Oy .\n",
      "The system is set up so if traffic is slow ,  an algorithm takes over until it picks up .\n",
      "Because the people who drive cars more are the ones who cause more wear and tear on the roads ? Pretty fair and straightforward .\n",
      "Affordable housing gets built pursuant to tax  # 3 ,  immediately becomes less affordable pursuant to tax  # 2 . Brilliant !\n",
      "Feels like panopticomments . I suspect you won't go back ,  but I prefer Disqus because of the ubiquity and notifications system .\n",
      "3 . Did this comment totally just win the argument ?\n",
      "I had a little trouble getting signed up ,  but after a brief email exchange with the folks at Civil everything appears to be working now . If they're successful in their goal ,  I'll be extremely pleased .\n",
      "Are you taxed as a resident of the state you were in when you bought the ticket ,  or as a resident of the state you are in when you claim the ticket ? If it's the latter ,  I suppose a theoretical lucky Oregonian will have to decide whether it's worth  $ 90m in exchange for being stuck with the hated  \" Californian \"  moniker .\n",
      "Read and learn ,  read and learn  . great history of western livestock abuses  .\n",
      "Mormons have had a complicated relationship with federal law .\n",
      "Send more dildos ,  STAT .\n",
      "I've been loosely following Civil Comments since it was announced late last year ;  I even tried a beta demonstration . One thing I've been wondering is :  will users will have to sign up for new profiles on any site that uses Civil ,  or do you have one Civil profile that you'd use on any partner site ?\n",
      "My sign - up just now couldn't have been smoother !\n",
      "Comments on many sites too often get overtaken by trolls . It's the first I've heard of Civil ,  but I like its intent and encourage other publications in town to consider comments as a platform for engagement rather than a troll - driven click driver intended to boost online ad rates .\n",
      "We decided that it'd be best to be able to present yourself differently to different communities ,  though . I might ,  for example ,  want to be portland _ hipster15 here on Willamette Week ,  but prefer to be MrWhiskers on Cat Enthusiast Digest !\n",
      "Enterprise licenses are available now ,  and we'll have tiered small business plans available very soon ! We will also have a free ,  public version out later this year .\n",
      "This is certainly not good journalism by any stretch of the imagination ,  but they are words ,  and if this is the metric by which Willamette Week chooses to publish a journalist ,  then by all means publish Brace Belden .\n",
      "What are the economics of their situation  -  -  are they in the same situation as chicken farmers ,  where they have had their livelihoods squeezed away  . I'd like to understand what drives someone who seems to have so much to be so alienated ,  and their economic situation must play into that .\n",
      "I hope you'll stick around and let us know what you think ! And yes ,  the software architect ,  Aja ,  is a Foucault fan .\n",
      "org /  . You will with out much difficulty find the list of Connection Groups and their times and locations .\n",
      "Can't wait to see it on more sites . Might even bring back comments on my personal site once it's available !\n",
      "I tend to use content blockers on sites that I know don't moderate comments very well . If they adopted a system like this I might stop doing that .\n",
      "This was quite a comprehensive list . Goody for the writers and the people that researched for this guide .\n",
      "And ,   repulsive as their  cammo costumes are  (  all available at Cabela's and Andy and Bax )  with their brandishing weapons it's a very immature male form of exhibitionism . cowboys and indians for middle aged men .\n",
      "What percentage of your users wait more than 1 / 2 second before randomly clicking on a response ? Oh well ,  I digress .\n",
      "I’ll just randomly click on yes sorta or no to make the buttons go away . Problems were detected in the ratings you left for your peers .\n",
      "And if that wasn't bad enough the whole second half is trashing furries ? Maybe next time you should think before writing an article antagonizing teenagers trying to do their Christmas tradition of getting pictures with Santa .\n",
      "And yes ,  that's a known issue ,  new to installing on WW . Working on it !\n",
      "Hey i5guy ,  that's a good point about  \" more replies \"  appearing too soon in the thread . What do you think about changing it so that ,  say ,  it takes four comments before that button shows up ?\n",
      "They're providing a service to the sites they're on ,  I assume those sites pay for that service .\n",
      "let me some up the heavy vibers feelings :   GO BACK TO CALIFORNIA .\n",
      "In fact looking  at the Loyd center piece ,  the pictures could been take around January 2 - 3 . We had a furry meet up  Loyd's Center and the  \" reporter \"  could had easily missed us .\n",
      "One more drink related item you missed IPA  ( that is looking at my grocery store beer  cooler )  . I like a good IPA but there is more to beer than IPA .\n",
      "19 :  check out  # SupportMalheur . And yes ,  I have lived in both rural  Or  ( 25 years )  and urban Or .\n",
      "AND Left wing trolls ! Just because someone agrees with you politically doesn't mean they're doing so civilly !\n",
      "19 :  check out  # SupportMalheur . And yes ,  I have lived in both rural  Or  ( 25 years )  and urban Or .\n",
      "Kay's is one of the best bars in the city ,  but you don't have a section for the neighborhood .\n",
      "Seriously ? I am very sorry for you .\n",
      "NO ! So could the cult be supporting that action of the Bundy clan and their minions ?\n",
      "The results of volunteers may . vary ,  to be polite .\n",
      "well pretty much anyone that isn't white ,  or say supporting the Kleins in the Sweet Cakes by Melissa saga . How do you convince people that an opinion which is to some degree derived from some form of prejudice is still  \" civil \"  and deserving of at least being heard ?\n",
      "Get rid of the  \" more messages \"  . Especially within a thread .\n",
      "Thanks Papa Murphy's down the street and Pizzicato in Hillsdale for GF options . We'll stick with them .\n",
      "Happy to see that the folks  \" in the room \"  are themselves  from varying backgrounds . .\n",
      "I'm just speculating however only time will tell . I also would not be surprised if I was totally off base on all of this .\n",
      "I like the design of the 2000s guy in the third strip . That's a guy who grew up watching Kevin Smith movies .\n",
      "If it is effective in slowing us down in order to reflect upon how we engage with others and draws attention to the humanity of each recipient ,  then I approve .\n",
      "I hear you on the  \" more replies \"  button ;  I don't think we'll remove it entirely ,  but I agree it's appearing too often ,  after too few comments . We'll be pushing updates very soon !\n",
      "But  * I *  want to be MrWhiskers !\n",
      "The Portland startup scene has the chance to set itself apart here ,  and this is definitely a step in the right direction ! Very cool .\n",
      "Gotta act fast on those usernames ! Cat Enthusiast Digest is no place to dilly - dally .\n",
      "Jeff Sessions is another one of Trump's Orwellian choices . He believes and has believed his entire career the exact opposite of what the position requires .\n",
      "Exactly Zero projects that had been identified in previous inspection reports had been funded by the federal government ,  and the entire band was housed in ATCO trailers . Clearly the Harper Conservatives had already reduced the cash his band was sent to zero .\n",
      "For the 100 th time  ,  Walker cited the cost of drug users treatment as being lost with Obamacare  . I laugh every time I hear a liberal claim republicans want to hurt people  ,  and that's why they dumped Obamacare .\n",
      "I do not wish for my city to provide for nor encourage idle and harassing behaviors in our city core . Enough is enough !\n",
      "this town is a pigpen . drive around and look for yourself ,  its pathetic .\n",
      "Why can't the Globe  &  Mail provide the symbols for these ETFs ? It is difficult to look up the precise ETF mentioned because one  wrong word  can give you the wrong ETF .\n",
      "That's already been happening ,  Carl ,  it's called Fake News .\n",
      "and not much is expected to be accomplished . so why bother with the waste ?\n",
      "Also ,  federal employees certify air safety . Civilian are not required to certify the same equipment .\n",
      "If I sound confused that's because I am . Our government sounds more dangerous to me than anyone else .\n",
      "You're quite the one to talk after the kind of remarks you have made about indigenous peoples .\n",
      "If it walks like a duck ,  and quacks like a duck .\n",
      "At about this point :  It participated with the Federation of South African Women in protests against the apartheid - era government ,  such as the 1952 Defiance Campaign and the Women's March of 9 August 1956 .\n",
      "But one side - effect is that the first thing that happens is  \" this is too hard ,  we're changing things up to make things easier ,  we're lowering the bar \"  . So people respond ,  and now we've got people who can't do hard stuff any more .\n",
      "If you take them out of the equation ,  the number jumps something like 8 - 10 %  ! if memory from the article I read is accurate .\n",
      "Today's fertility rates of 1 . 6 / woman are comparable to the mid - 80s ,  and slightly higher than 15 years ago .\n",
      "Extremists hate the west and our way of life ,  our secular society and freedoms as they try to impose their fundamentalist views through random terror attacks on innocents . Comparing terror attacks to cancer is a false equivalency ,  and a shameful comparison as if death by terror is trivial because relatively few die compared to cancer ,  implying this is an acceptable loss of life ,  despicable .\n",
      "Yes ,  it would be expensive ,  but to protect billions upon billions of dollars of infrastructure ,  population ,  and critical military bases in Hawaii would be a no - brainer and worth it . Even with the other type of interceptors based in Alaska ,  it would be smart to have this additional layer of defense .\n",
      "If a chemical weapons facility had been hit ,  the resulting explosion would most likely have caused the chemical to burn up ,  international weapons experts say . And a nerve agent like sarin is not likely to be stored in its active form in such a facility ,  and its components would need to be mixed to be lethal .\n",
      "Lol -  - this is funny and I am sure ,  intentional  -  >   \" Your speling is terrible . \"   I also appear unable to spell but normally it is from  \" cat assisted \"  or poor typing  :  )   Have a good one .\n",
      "BS  !\n",
      "The council should also vote that the Hawaii ERS Pension managers be mandated to buy any local municipal bonds at full face value !\n",
      "Why don't we just give everyone a diploma when they reach the age of five . Everyone is a winner there .\n",
      "I won't even mention lying ,  deceiving ,  hating . The Donald is king in all of that .\n",
      "Of the money ,  by the money and for the money . Brought to you by the jesus freaks cause birds of a feather flock together .\n",
      "Trust me ,  you're doing more due diligence on their project than they are !\n",
      "\"   Beware of the new McCarthyist witch hunters . different ideology … same MO .\n",
      "If the law forbids him from owning or possessing firearms ,  why did the court order HPD to give back his weapons after he plead guilty and was convicted of the offense they now say disqualifies him from gun ownership ?\n",
      "I'm a MYC Grad . class in mid 80's .\n",
      "Apparently the Russians were not happy with her grandparents association with the Nazi's . Trudeau needs to settle down and fix Canada before doing anymore international charity work .\n",
      "By rewarding these kind of great kids ,  they work hard and it rewards kids on the scout team . That way the scout team can attract more quality players .\n",
      "Trudeau could take a few lessons from The Donald ,  before Trudeau experiences  \" You're Fired ! \"  ;  unfortunately disappointed in Trudeau ;  perhaps he should have stuck to boxing instead .\n",
      "Right - wingers have themselves become sick and tiring . They are uncivil by definition ,  and ,  are basically just free - riders who cover themselves with self - serving language so that it might fool people into thinking that their good is the same thing as the common good .\n",
      "It's such an obvious response ,  that I'm not sure why we haven't heard more people with this idea . Maybe it's a bit early for this since the Libs seem determined to wreck any good idea that comes along ,  and they're otherwise a bit slow to react .\n",
      "Did it ever occur to you that the reason right - wing media continually push this book on you is that they hope you'll read it and employ its tactics to THEIR ends ? Seriously ,  the only people I ever hear bring it up are you guys .\n",
      "Ms . Smith ,  if the Mall smells ,  I suspect odors other than cigarette smoke are a contributing factor .\n",
      "in the Minister's head so that he or she will think favourably on your behalf the next time a related issue raises its ugly head . Close quarters lobbying I would call it .\n",
      "Do we really have a shortage of drivers ? Is this not an industry in search of a market that does not exist ?\n",
      "DNI has denied it ,  must be true .\n",
      "The three credit agency's have down graded our credit worthiness out of concern over spending and fiscal discipline . I think it's time that Walker quit spending and start sharing the truth with Alaskans .\n",
      "\"     What does the lack of a sprinkler system  -  which might have prevented the intense fire AFD referred to  -  have to do with where the fire started ? What I'm saying is that owners of buildings should be responsible ,  by law ,  for installing sprinklers in old buildings BEFORE there is a fire and people die .\n",
      "Go away Carl .\n",
      "Richards for your continued good service to your fellow Alaskans . Time will prove we are better for it .\n",
      "The American electoral system is so convoluted ,  there's no way  of telling . Keep religion out of it ,  it just complicates a complicated mess .\n",
      "”   https :  /  / www . theguardian .\n",
      "We need a national set of firearm regulations instead of 50 different ,  and confusing ones .\n",
      "Hodad . does your user handle refer to your daughter ?\n",
      "Give me one workable policy change that would enact your vision .\n",
      "\n",
      "Alf ,  is there really a difference in the ANC between doing the wrong thing and walking the party line ? They sound like pretty much the same thing !\n",
      "So 95 %  is not enough ?\n",
      "What alternative do we have to being plowed under by a mindless rush to strip the cost of labor from the cost of production without replacing the very income necessary for people to make in order to consume products and services ? Liberals and Conservatives could actually have meaningful economic discussions if Conservatives would accept the fact that economic systems belong to all members of society and that it doesn't matter how economies are organized as long as they achieve the purpose of providing necessities for all .\n",
      "If I had a nickle for every US President and / or right winger who made the same argument about every single  \" just like Hitler \"  foe they dredged up to motivate the populace for war  -  I'd be very rich . Do either Trump or Kim Jong Un realize we are all downwind from the radioactive fall out of nuclear war .\n",
      "On the national level ,  what have reepublicans done ? Basically nothing since Eisenhower ,   ( Interstate freeway system )  and Nixon  ( first proposed Obama care )   Frankly tired of these fake conservatives .\n",
      "This  \" review \"  sounds more like an attack from a very spoiled bully . Good thing I now know not to pay any attention to any of Dan Clapsons' opinions ,   he couldn't get any pettier if he tried .\n",
      "Fool .\n",
      "No one said anything about whites being innocent . We all have our bad eggs .\n",
      "Folllowing is a quotation from an excellent article addressing this issue . I've also provided the link so you or anyone else can read the entire article .\n",
      "[ Its also served certain assembly members well who have collected lots of contributions from cab company owners . ]  Source ?\n",
      "05 %  to the GET to raise billions of dollars for nice - to - have rail ,  it surely can afford to add another  . 05 %  for essentials like CHIP to take care of its own .\n",
      "They may have put much more into reporting this case than other media . But is that a bad thing ?\n",
      ". .\n",
      "I keep seeing your name but can't read any of your comments because  < this comment did not meet civility standards >   There's like 10 +  of these under your name on this article alone . What's the point of making soo many comments if nobody can read them ?\n",
      "The City Hall fiasco ,  all the MUPTEs ,  etc . have gotten the attention of many ;   I agree that trust in city government is at all time low .\n",
      "Unfortunately much of the old village has eroded into the sea . I wonder how far inland they penetrated and the extent of the contact the Thule had with other native cultures .\n",
      "Did Mark Shore lose his job ? I have not seen his guff for quite a while now .\n",
      "This isn't a political problem we need Rehab facilities for people who want to quit and for people convicted of low level drug crimes . Ignoring the problem will not solve it ,  and polarizing it politically is ignorant and destructive to society .\n",
      "The writing is making the ASSumption that if you don't buy a house ,  you'll somehow automatically be flush with cash . If only the rental market reflected that naive optimism .\n",
      "Wanna bet ?\n",
      "The conversations I hear are about getting ripped off at the gas pumps ,  the outrageous amount people pay for cable ,  internet and cell phones and how they haven't received a raise in years all while they complain about their bosses trip somewhere or weekend at the cottage . What world do you live in ?\n",
      "Perhaps there was voter fraud in 2016 . How else would a reality show host that is as phony as a  $ 3 bill get himself elected president ?\n",
      "Crime increased . Same thing is happening in Anchorage .\n",
      "If we ever get a baseball team ,  a new stadium which was purpose built for baseball would be most appropriate ,  not a rectangular sports field which just wasn't finished on one side for 80 years . Multipurpose stadiums just meant they were terrible for everyone and for that reason we stopped building them in the 60s .\n",
      "You mean there are STILL people hanging on to the Trumptanic . Brilliant !\n",
      "What Russian invasion ?\n",
      "Episode 1 :  A New Hope . the masterpiece that started it all .\n",
      "\n",
      "Go get 'em ,  Ladies ! And Bravo to the men who support them .\n",
      "But neither of them could completely destroy the other . The Parthian Empire was also a second level world empire .\n",
      "\"   They preached the gospel ,  which ,  if accepted ,  creates its own social reform . This effort ,  while maybe well - intentioned ,  is one of those  \" bypath meadows \"  which distracts us from our main goal :   preaching the 3 angels' messages .\n",
      "I like the option that we can use email to log in to leave a comment instead of our Facebook . My concern is the majority who try to censor us minorities .\n",
      "I hope you find peace by helping others . Go  “ Aquene’s Army ”  !\n",
      "You must wonder why drugs are illegal and why they don't allow them in prison .\n",
      "Clearly ,  this Premier has never run a business or even understands basic arithmetic . She has run Ontario into the ground on all her waste and mismanagement .\n",
      "You then plead ignorance and claim you'd rather discuss the issue of refugees rather than that of immigrants after pointing out that you are ignorance of the difference between the two . You can't make this stuff up .\n",
      "I doubt if I can continue anything at all in the future that looms . A hex upon the greedy developers ,  and Mayor Robertson ,  their pal .\n",
      "The Flames ,  being the incredibly astute business people they are ,  should simply buy a piece of land (  subject to successful permitting etc )  ,  build the facility ,  and go into business . Any conservative ought to understand that !\n",
      "A kid that lived in the neighborhood . You reserve judgement if you want ,  but please leave the pedantic speech for your kids .\n",
      "If you can't win an election honestly you don't deserve your office . Try to do better ,  please !\n",
      "Ever . And this may be it .\n",
      "He is gets in deeper and deeper over his head . Time to hold him up as an adult and stop allowing him to think and speak .\n",
      "7 %  unemployment . We need to get these guys into  construction .\n",
      "Moderate centrists must fight off polarizing populism from both flanks . It doesn't help to pretend it's only one side of the spectrum that plays those games .\n",
      "Police work is not a 'club' it is is a dangerous job ,  and it looks like they have hired and trained poorly qualified individuals . This makes the whole scenario ever scarier .\n",
      "If we are so unpleasant why waste time reading our comments ?\n",
      "Here comes the BGH milk . I'll drink it doesn't affect americans .\n",
      "I drive ALL DAY and I rarely see what you are talking about . where do you drive specifically ?\n",
      "A legislated conclusion to a field of scientific inquiry that our own government readily admits is still in its infancy is about the  most fundamentally flawed way of conducting science and clearly shows this is all based on ideology over knowledge or truth . Let the book burnings commence .\n",
      "He'll enjoy himself there . He'll be the little b *  *  * h he always wanted to be .\n",
      "Only in Alaska can this stuff be called normal ,  they will learn in time !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/curtis/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:18: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('../input/train.csv')[:100]\n",
    "test = pd.read_csv('../input/test.csv')[:100]\n",
    "#pd.read_csv(\"P00000001-ALL.csv\", nrows=20)\n",
    "#train = pd.read_hdf('../input/train.h5')\n",
    "#test = pd.read_hdf('../input/test.h5')\n",
    "#tqdm.pandas()\n",
    "\n",
    "identity_columns = [\n",
    "    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n",
    "    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n",
    "\n",
    "x_train = train['comment_text'].apply(lambda x:preprocess(x))\n",
    "y_train = np.where(train['target'] >= 0.5, 1, 0)\n",
    "num_train_data = y_train.shape[0]\n",
    "y_train_identity = np.where(train[identity_columns] >= 0.5, 1, 0)\n",
    "y_aux_train = train[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']]\n",
    "x_test = test['comment_text'].apply(lambda x:preprocess(x))\n",
    "y_aux_train = y_aux_train.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmax_features = None\\n\\n#Create the dictionary of all words that exist in our data\\nnlp = spacy.load(\"en_core_web_lg\", disable=[\\'parser\\',\\'ner\\',\\'tagger\\'])\\ntext_list = pd.concat([x_train, x_test])\\nnlp.vocab.add_flag(lambda s: s.lower() in spacy.lang.en.stop_words.STOP_WORDS, spacy.attrs.IS_STOP)\\nword_dict = {}\\nlemma_dict = {}\\nword_index = 1\\ndocs = nlp.pipe(text_list, n_threads = 2)\\nword_sequences = []\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "max_features = None\n",
    "\n",
    "#Create the dictionary of all words that exist in our data\n",
    "nlp = spacy.load(\"en_core_web_lg\", disable=['parser','ner','tagger'])\n",
    "text_list = pd.concat([x_train, x_test])\n",
    "nlp.vocab.add_flag(lambda s: s.lower() in spacy.lang.en.stop_words.STOP_WORDS, spacy.attrs.IS_STOP)\n",
    "word_dict = {}\n",
    "lemma_dict = {}\n",
    "word_index = 1\n",
    "docs = nlp.pipe(text_list, n_threads = 2)\n",
    "word_sequences = []\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nstart_time = time.time()\\nfor doc in tqdm(docs): #one doc is one comment(row)\\n    #print(count)\\n    word_seq = []\\n    for token in doc:\\n        if (token.text not in word_dict) and (token.pos_ is not \"PUNCT\"):\\n            word_dict[token.text] = word_index\\n            word_index += 1\\n            lemma_dict[token.text] = token.lemma_\\n        if token.pos_ is not \"PUNCT\":\\n            word_seq.append(word_dict[token.text])\\n    word_sequences.append(word_seq)\\n    #count+= 1\\n\\nprint(\"--- %s seconds ---\" % (time.time() - start_time))\\ndel docs\\ndel text_list\\ngc.collect()\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create dictionary of word mapping to integers as wel as lemma dictionary\n",
    "#count = 1\n",
    "'''\n",
    "start_time = time.time()\n",
    "for doc in tqdm(docs): #one doc is one comment(row)\n",
    "    #print(count)\n",
    "    word_seq = []\n",
    "    for token in doc:\n",
    "        if (token.text not in word_dict) and (token.pos_ is not \"PUNCT\"):\n",
    "            word_dict[token.text] = word_index\n",
    "            word_index += 1\n",
    "            lemma_dict[token.text] = token.lemma_\n",
    "        if token.pos_ is not \"PUNCT\":\n",
    "            word_seq.append(word_dict[token.text])\n",
    "    word_sequences.append(word_seq)\n",
    "    #count+= 1\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "del docs\n",
    "del text_list\n",
    "gc.collect()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 400000\n",
    "tokenizer = text.Tokenizer(num_words = max_features, filters='',lower=False)\n",
    "tokenizer.fit_on_texts(list(x_train) + list(x_test))\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_test = tokenizer.texts_to_sequences(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#max_features = max_features or len(word_dict) + 1\n",
    "max_features = max_features or len(tokenizer.word_index) + 1\n",
    "max_features #number of unique words there are in the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n unknown words (crawl):  46\n",
      "--- 77.28580236434937 seconds ---\n",
      "Size:  (1791, 300)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "#crawl_matrix, unknown_words_crawl = build_matrix(word_dict, lemma_dict, CRAWL_EMBEDDING_PATH)\n",
    "crawl_matrix, unknown_words_crawl = build_matrix(tokenizer.word_index, CRAWL_EMBEDDING_PATH)\n",
    "print('n unknown words (crawl): ', len(unknown_words_crawl))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print('Size: ', crawl_matrix.shape)\n",
    "del unknown_words_crawl\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n unknown words (glove):  54\n",
      "--- 87.43918204307556 seconds ---\n",
      "Size:  (1791, 300)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "#glove_matrix, unknown_words_glove = build_matrix(word_dict, lemma_dict, GLOVE_EMBEDDING_PATH)\n",
    "glove_matrix, unknown_words_glove = build_matrix(tokenizer.word_index, GLOVE_EMBEDDING_PATH)\n",
    "print('n unknown words (glove): ', len(unknown_words_glove))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print('Size: ', glove_matrix.shape)\n",
    "del unknown_words_glove\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = np.concatenate([crawl_matrix, glove_matrix], axis=-1)\n",
    "\n",
    "del crawl_matrix\n",
    "del glove_matrix\n",
    "#del word_dict\n",
    "#del lemma_dict\n",
    "#del WORDS\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JigsawEvaluator:\n",
    "\n",
    "    def __init__(self, y_true, y_identity, power=-5, overall_model_weight=0.25):\n",
    "        self.y = y_true\n",
    "        self.y_i = y_identity\n",
    "        self.n_subgroups = self.y_i.shape[1]\n",
    "        self.power = power\n",
    "        self.overall_model_weight = overall_model_weight\n",
    "\n",
    "    @staticmethod\n",
    "    def _compute_auc(y_true, y_pred):\n",
    "        #print(\"Here: \", y_true)\n",
    "        #print(y_pred)\n",
    "        try:\n",
    "            return roc_auc_score(y_true, y_pred)\n",
    "        except ValueError:\n",
    "            return np.nan\n",
    "\n",
    "    def _compute_subgroup_auc(self, i, y_pred):\n",
    "        mask = self.y_i[:, i] == 1\n",
    "        #print(self.y)\n",
    "        return self._compute_auc(self.y[mask], y_pred[mask])\n",
    "\n",
    "    def _compute_bpsn_auc(self, i, y_pred):\n",
    "        mask = self.y_i[:, i] + self.y == 1\n",
    "        return self._compute_auc(self.y[mask], y_pred[mask])\n",
    "\n",
    "    def _compute_bnsp_auc(self, i, y_pred):\n",
    "        mask = self.y_i[:, i] + self.y != 1\n",
    "        return self._compute_auc(self.y[mask], y_pred[mask])\n",
    "\n",
    "    def compute_bias_metrics_for_model(self, y_pred):\n",
    "        #print(y_pred)\n",
    "        records = np.zeros((3, self.n_subgroups))\n",
    "        for i in range(self.n_subgroups):\n",
    "            #print(y_pred)\n",
    "            records[0, i] = self._compute_subgroup_auc(i, y_pred)\n",
    "            records[1, i] = self._compute_bpsn_auc(i, y_pred)\n",
    "            records[2, i] = self._compute_bnsp_auc(i, y_pred)\n",
    "        return records\n",
    "\n",
    "    def _calculate_overall_auc(self, y_pred):\n",
    "        return roc_auc_score(self.y, y_pred)\n",
    "\n",
    "    def _power_mean(self, array):\n",
    "        total = sum(np.power(array, self.power))\n",
    "        return np.power(total / len(array), 1 / self.power)\n",
    "\n",
    "    def get_final_metric(self, y_pred):\n",
    "        bias_metrics = self.compute_bias_metrics_for_model(y_pred)\n",
    "        bias_score = np.average([\n",
    "            self._power_mean(bias_metrics[0]),\n",
    "            self._power_mean(bias_metrics[1]),\n",
    "            self._power_mean(bias_metrics[2])\n",
    "        ])\n",
    "        overall_score = self.overall_model_weight * self._calculate_overall_auc(y_pred)\n",
    "        bias_score = (1 - self.overall_model_weight) * bias_score\n",
    "        return overall_score + bias_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "torch.Size([100])\n",
      "55\n",
      "tensor(67)\n"
     ]
    }
   ],
   "source": [
    "#x_train = word_sequences[:num_train_data]\n",
    "#x_test = word_sequences[num_train_data:]\n",
    "lengths = torch.from_numpy(np.array([len(x) for x in x_train]))\n",
    "test_lengths = torch.from_numpy(np.array([len(x) for x in x_test]))\n",
    "print(len(x_train))\n",
    "print(lengths.shape)\n",
    "maxlen = int(np.percentile(lengths, 95)) \n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "print(maxlen)\n",
    "print(lengths.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 55)\n",
      "(100,)\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(num_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model  0\n",
      "=> Saving a new best\n",
      "Epoch 1/4 \t loss=0.6826 \t time=0.08s\n",
      "=> Saving a new best\n",
      "Epoch 2/4 \t loss=0.6383 \t time=0.08s\n",
      "=> Saving a new best\n",
      "Epoch 3/4 \t loss=0.6112 \t time=0.08s\n",
      "=> Saving a new best\n",
      "Epoch 4/4 \t loss=0.5964 \t time=0.06s\n",
      "\n",
      "Model  1\n",
      "=> Saving a new best\n",
      "Epoch 1/4 \t loss=0.6944 \t time=0.09s\n",
      "=> Saving a new best\n",
      "Epoch 2/4 \t loss=0.6474 \t time=0.08s\n",
      "=> Saving a new best\n",
      "Epoch 3/4 \t loss=0.6210 \t time=0.06s\n",
      "=> Saving a new best\n",
      "Epoch 4/4 \t loss=0.6048 \t time=0.07s\n",
      "\n",
      "Kaggle Score:  nan\n",
      "ROC score:  0.2368421052631579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/curtis/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:46: RuntimeWarning: divide by zero encountered in power\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============End-of-Fold================\n",
      "Model  0\n",
      "=> Saving a new best\n",
      "Epoch 1/4 \t loss=0.6829 \t time=0.07s\n",
      "=> Saving a new best\n",
      "Epoch 2/4 \t loss=0.6388 \t time=0.07s\n",
      "=> Saving a new best\n",
      "Epoch 3/4 \t loss=0.6123 \t time=0.07s\n",
      "=> Saving a new best\n",
      "Epoch 4/4 \t loss=0.5968 \t time=0.07s\n",
      "\n",
      "Model  1\n",
      "=> Saving a new best\n",
      "Epoch 1/4 \t loss=0.6941 \t time=0.06s\n",
      "=> Saving a new best\n",
      "Epoch 2/4 \t loss=0.6472 \t time=0.06s\n",
      "=> Saving a new best\n",
      "Epoch 3/4 \t loss=0.6211 \t time=0.07s\n",
      "=> Saving a new best\n",
      "Epoch 4/4 \t loss=0.6044 \t time=0.07s\n",
      "\n",
      "Kaggle Score:  nan\n",
      "ROC score:  0.75\n",
      "=============End-of-Fold================\n",
      "Model  0\n",
      "=> Saving a new best\n",
      "Epoch 1/4 \t loss=0.6829 \t time=0.06s\n",
      "=> Saving a new best\n",
      "Epoch 2/4 \t loss=0.6379 \t time=0.06s\n",
      "=> Saving a new best\n",
      "Epoch 3/4 \t loss=0.6106 \t time=0.06s\n",
      "=> Saving a new best\n",
      "Epoch 4/4 \t loss=0.5945 \t time=0.07s\n",
      "\n",
      "Model  1\n",
      "=> Saving a new best\n",
      "Epoch 1/4 \t loss=0.6944 \t time=0.07s\n",
      "=> Saving a new best\n",
      "Epoch 2/4 \t loss=0.6468 \t time=0.07s\n",
      "=> Saving a new best\n",
      "Epoch 3/4 \t loss=0.6203 \t time=0.07s\n",
      "=> Saving a new best\n",
      "Epoch 4/4 \t loss=0.6034 \t time=0.07s\n",
      "\n",
      "Kaggle Score:  nan\n",
      "ROC score:  0.6944444444444444\n",
      "=============End-of-Fold================\n",
      "Model  0\n",
      "=> Saving a new best\n",
      "Epoch 1/4 \t loss=0.6831 \t time=0.06s\n",
      "=> Saving a new best\n",
      "Epoch 2/4 \t loss=0.6385 \t time=0.07s\n",
      "=> Saving a new best\n",
      "Epoch 3/4 \t loss=0.6121 \t time=0.07s\n",
      "=> Saving a new best\n",
      "Epoch 4/4 \t loss=0.5960 \t time=0.07s\n",
      "\n",
      "Model  1\n",
      "=> Saving a new best\n",
      "Epoch 1/4 \t loss=0.6947 \t time=0.07s\n",
      "=> Saving a new best\n",
      "Epoch 2/4 \t loss=0.6469 \t time=0.07s\n",
      "=> Saving a new best\n",
      "Epoch 3/4 \t loss=0.6203 \t time=0.06s\n",
      "=> Saving a new best\n",
      "Epoch 4/4 \t loss=0.6036 \t time=0.07s\n",
      "\n",
      "Kaggle Score:  nan\n",
      "ROC score:  0.6111111111111112\n",
      "=============End-of-Fold================\n",
      "Model  0\n",
      "=> Saving a new best\n",
      "Epoch 1/4 \t loss=0.6827 \t time=0.07s\n",
      "=> Saving a new best\n",
      "Epoch 2/4 \t loss=0.6382 \t time=0.06s\n",
      "=> Saving a new best\n",
      "Epoch 3/4 \t loss=0.6114 \t time=0.07s\n",
      "=> Saving a new best\n",
      "Epoch 4/4 \t loss=0.5954 \t time=0.07s\n",
      "\n",
      "Model  1\n",
      "=> Saving a new best\n",
      "Epoch 1/4 \t loss=0.6942 \t time=0.06s\n",
      "=> Saving a new best\n",
      "Epoch 2/4 \t loss=0.6471 \t time=0.06s\n",
      "=> Saving a new best\n",
      "Epoch 3/4 \t loss=0.6205 \t time=0.06s\n",
      "=> Saving a new best\n",
      "Epoch 4/4 \t loss=0.6039 \t time=0.07s\n",
      "\n",
      "Kaggle Score:  nan\n",
      "ROC score:  0.2222222222222222\n",
      "=============End-of-Fold================\n",
      "Time:  23.80552077293396\n",
      "Final Kaggle Score:  nan\n",
      "Final ROC score:  0.5400815217391304\n"
     ]
    }
   ],
   "source": [
    "\n",
    "all_val_preds = []\n",
    "all_test_preds = []\n",
    "num_splits = 5\n",
    "\n",
    "#Add in K fold \n",
    "random_state = 2019\n",
    "\n",
    "#K fold splits\n",
    "splits = list(StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=random_state).split(x_train,y_train))\n",
    "\n",
    "#final validation predictions\n",
    "final_val_preds = np.zeros((x_train.shape[0]))\n",
    "\n",
    "#final test predictions to be stored in this var\n",
    "final_test_preds = np.zeros((x_test.shape[0]))\n",
    "\n",
    "start_time = time.time()\n",
    "for fold in range(num_splits):\n",
    "    tr_ind, val_ind = splits[fold]\n",
    "    all_val_preds = []\n",
    "    all_test_preds = []\n",
    "    #print('Training set size: ', len(tr_ind))\n",
    "    #print('Val set size: ', len(val_ind))\n",
    "    x_training = x_train[tr_ind]\n",
    "    y_training = y_train[tr_ind]\n",
    "    y_aux_training = y_aux_train[tr_ind]\n",
    "    \n",
    "    x_val = x_train[val_ind]\n",
    "    y_val = y_train[val_ind]\n",
    "    y_aux_val = y_aux_train[val_ind]\n",
    "    \n",
    "    \n",
    "    \n",
    "    x_train_torch = torch.tensor(x_training, dtype=torch.long).cuda()\n",
    "    x_val_torch = torch.tensor(x_val, dtype=torch.long).cuda()\n",
    "    y_train_torch = torch.tensor(np.hstack([y_training[:, np.newaxis], y_aux_training]), dtype=torch.float32).cuda()\n",
    "    y_val_torch = torch.tensor(np.hstack([y_val[:, np.newaxis], y_aux_val]), dtype=torch.float32).cuda()\n",
    "    \n",
    "    x_test_torch = torch.tensor(x_test, dtype=torch.long).cuda()\n",
    "    \n",
    "    ###\n",
    "    \n",
    "    #test_dataset = data.TensorDataset(x_test_torch, test_lengths)\n",
    "    #train_dataset = data.TensorDataset(x_train_torch, lengths, y_train_torch)\n",
    "    #val_dataset = data.TensorDataset(x_val_torch)\n",
    "\n",
    "    #train_collator = SequenceBucketCollator(lambda lengths: lengths.max(), sequence_index=0, length_index=1, label_index=2)\n",
    "    #test_collator = SequenceBucketCollator(lambda lengths: lengths.max(), sequence_index=0, length_index=1)\n",
    "    \n",
    "    ####\n",
    "    train_dataset = data.TensorDataset(x_train_torch, lengths[tr_ind], y_train_torch)\n",
    "    val_dataset = data.TensorDataset(x_val_torch, lengths[val_ind], y_val_torch)\n",
    "    test_dataset = data.TensorDataset(x_test_torch, test_lengths)\n",
    "    \n",
    "    #temp_dataset = data.Subset(train_dataset, indices=[0, 1])\n",
    "\n",
    "    for model_idx in range(NUM_MODELS):\n",
    "        print('Model ', model_idx)\n",
    "        seed_everything(1234 + model_idx)\n",
    "\n",
    "        model = NeuralNet(embedding_matrix, y_aux_train.shape[-1])\n",
    "        model.cuda()\n",
    "\n",
    "        #training using training and validation set\n",
    "        model = train_model(model, train_dataset, val_dataset, output_dim=y_train_torch.shape[-1], loss_fn=nn.BCEWithLogitsLoss(reduction='mean'))\n",
    "        \n",
    "        #prediction on validation set (used for score measurement)\n",
    "        val_pred = predict(model, val_dataset, output_dim=y_train_torch.shape[-1], pred_type=\"val\") #val preds on the val split\n",
    "        all_val_preds.append(val_pred)\n",
    "        #print(len(val_pred))\n",
    "        \n",
    "        #prediction on entire test set (actual predictions to be submitted)\n",
    "        test_pred = predict(model, test_dataset, output_dim=y_train_torch.shape[-1], pred_type=\"test\")\n",
    "        all_test_preds.append(test_pred)\n",
    "        \n",
    "        print()\n",
    "        \n",
    "    #average validation prediction amongst all models\n",
    "    avg_val = np.mean(all_val_preds, axis=0)[:, 0] #will be printed out per split\n",
    "    final_val_preds[val_ind] += avg_val\n",
    "    \n",
    "    avg_test = np.mean(all_test_preds, axis=0)[:, 0]\n",
    "    \n",
    "    final_test_preds += avg_test\n",
    "\n",
    "    y_true = y_train[val_ind] #true scores for this validation set\n",
    "    y_identity = y_train_identity[val_ind] #true scores for the identity groups for this validation set\n",
    "    evaluator = JigsawEvaluator(y_true, y_identity)\n",
    "    auc_score = evaluator.get_final_metric(avg_val)\n",
    "\n",
    "    roc_score = roc_auc_score(y_train[val_ind], avg_val)\n",
    "    print('Kaggle Score: ', auc_score)\n",
    "    print('ROC score: ', roc_score)\n",
    "    \n",
    "    del x_train_torch\n",
    "    del x_val_torch\n",
    "    del y_train_torch\n",
    "    del y_val_torch\n",
    "    del x_test_torch\n",
    "    del train_dataset\n",
    "    del val_dataset\n",
    "    del test_dataset\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print('=============End-of-Fold================')\n",
    "    \n",
    "end_time = time.time()\n",
    "print('Time: ', end_time - start_time)\n",
    "\n",
    "#Final combined score\n",
    "y_true = y_train\n",
    "y_identity = y_train_identity\n",
    "evaluator = JigsawEvaluator(y_true, y_identity)\n",
    "auc_score = evaluator.get_final_metric(final_val_preds)\n",
    "print('Final Kaggle Score: ', auc_score)\n",
    "print('Final ROC score: ', roc_auc_score(y_train, final_val_preds))\n",
    "\n",
    "#average test predictions AGAIN this time by number of splits\n",
    "final_test_preds /= num_splits\n",
    "#print(final_test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7000000</td>\n",
       "      <td>0.435709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7000001</td>\n",
       "      <td>0.427192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7000002</td>\n",
       "      <td>0.427120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7000003</td>\n",
       "      <td>0.434532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7000004</td>\n",
       "      <td>0.441903</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  prediction\n",
       "0  7000000    0.435709\n",
       "1  7000001    0.427192\n",
       "2  7000002    0.427120\n",
       "3  7000003    0.434532\n",
       "4  7000004    0.441903"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.DataFrame.from_dict({\n",
    "    'id': test['id'],\n",
    "    'prediction': final_test_preds\n",
    "})\n",
    "\n",
    "submission.to_csv('last_two_submission.csv', index=False)\n",
    "submission.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
