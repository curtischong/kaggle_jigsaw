{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hey all. This is a for of @bminixhofer Kernel. My only addition is to demonstrate the use variable batch size for accelerated training times, and of course I use my picked embeddings which load faster and help with memory management."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This kernel is a PyTorch version of the [Simple LSTM kernel](https://www.kaggle.com/thousandvoices/simple-lstm). All credit for architecture and preprocessing goes to @thousandvoices.\n",
    "There is a lot of discussion whether Keras, PyTorch, Tensorflow or the CUDA C API is best. But specifically between the PyTorch and Keras version of the simple LSTM architecture, there are 2 clear advantages of PyTorch:\n",
    "- Speed. The PyTorch version runs about 20 minutes faster.\n",
    "- Determinism. The PyTorch version is fully deterministic. Especially when it gets harder to improve your score later in the competition, determinism is very important.\n",
    "\n",
    "I was surprised to see that PyTorch is that much faster, so I'm not completely sure the steps taken are exactly the same. If you see any difference, we can discuss it in the comments :)\n",
    "\n",
    "The most likely reason the score of this kernel is higher than the @thousandvoices version is that the optimizer is not reinitialized after every epoch and thus the parameter-specific learning rates of Adam are not discarded after every epoch. That is the only difference between the kernels that is intended."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports & Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, time, gc, pickle, random\n",
    "from tqdm._tqdm_notebook import tqdm_notebook as tqdm\n",
    "from keras.preprocessing import text, sequence\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from torch.nn import functional as F\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel\n",
    "import apex\n",
    "import re\n",
    "import string\n",
    "\n",
    "SMALL_DATA = True\n",
    "\n",
    "# Logging for BERT\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable progress bars when submitting\n",
    "def is_interactive():\n",
    "    return 'SHLVL' not in os.environ\n",
    "\n",
    "if not is_interactive():\n",
    "    def nop(it, *a, **k):\n",
    "        return it\n",
    "\n",
    "    tqdm = nop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "CRAWL_EMBEDDING_PATH = '../input/pickled-crawl300d2m-for-kernel-competitions/crawl-300d-2M.pkl'\n",
    "GLOVE_EMBEDDING_PATH = '../input/pickled-glove840b300d-for-10sec-loading/glove.840B.300d.pkl'\n",
    "\n",
    "NUM_MODELS = 2\n",
    "LSTM_UNITS = 128\n",
    "DENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\n",
    "MAX_LEN = 220"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def build_matrix(word_index, emb_path, unknown_token='unknown'):\n",
    "    with open(emb_path, 'rb') as fp:\n",
    "        embedding_index = pickle.load(fp)\n",
    "    \n",
    "    # TODO: Build random token instead of using unknown\n",
    "    unknown_token = embedding_index[unknown_token].copy()\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "    unknown_words = []\n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        try:\n",
    "            embedding_matrix[i] = embedding_index[word].copy()\n",
    "        except KeyError:\n",
    "            embedding_matrix[i] = unknown_token\n",
    "            unknown_words.append(word)\n",
    "            \n",
    "    del embedding_index; gc.collect()\n",
    "    return embedding_matrix, unknown_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialDropout(nn.Dropout2d):\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(2)    # (N, T, 1, K)\n",
    "        x = x.permute(0, 3, 2, 1)  # (N, K, 1, T)\n",
    "        x = super(SpatialDropout, self).forward(x)  # (N, K, 1, T), some features are masked\n",
    "        x = x.permute(0, 3, 2, 1)  # (N, T, 1, K)\n",
    "        x = x.squeeze(2)  # (N, T, K)\n",
    "        return x\n",
    "    \n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, embedding_matrix, num_aux_targets):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        embed_size = embedding_matrix.shape[1]\n",
    "        \n",
    "        self.embedding = nn.Embedding(max_features, embed_size)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.embedding_dropout = SpatialDropout(0.3)\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(embed_size, LSTM_UNITS, bidirectional=True, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(LSTM_UNITS * 2, LSTM_UNITS, bidirectional=True, batch_first=True)\n",
    "    \n",
    "        self.linear1 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n",
    "        self.linear2 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n",
    "        \n",
    "        self.linear_out = nn.Linear(DENSE_HIDDEN_UNITS, 1)\n",
    "        self.linear_aux_out = nn.Linear(DENSE_HIDDEN_UNITS, num_aux_targets)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h_embedding = self.embedding(x)\n",
    "        h_embedding = self.embedding_dropout(h_embedding)\n",
    "        \n",
    "        h_lstm1, _ = self.lstm1(h_embedding)\n",
    "        h_lstm2, _ = self.lstm2(h_lstm1)\n",
    "        \n",
    "        # global average pooling\n",
    "        avg_pool = torch.mean(h_lstm2, 1)\n",
    "        # global max pooling\n",
    "        max_pool, _ = torch.max(h_lstm2, 1)\n",
    "        \n",
    "        h_conc = torch.cat((max_pool, avg_pool), 1)\n",
    "        h_conc_linear1  = F.relu(self.linear1(h_conc))\n",
    "        h_conc_linear2  = F.relu(self.linear2(h_conc))\n",
    "        \n",
    "        hidden = h_conc + h_conc_linear1 + h_conc_linear2\n",
    "        \n",
    "        result = self.linear_out(hidden)\n",
    "        aux_result = self.linear_aux_out(hidden)\n",
    "        out = torch.cat([result, aux_result], 1)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "string.printable\n",
    "ascii_chars = string.printable\n",
    "ascii_chars += \" áéíóúàèìòùâêîôûäëïöüñõç\"\n",
    "\n",
    "#checks if a string of text contains any nonenglish characters (excluding punctuations, spanish, and french characters)\n",
    "def contains_non_english(text):\n",
    "    if all(char in ascii_chars for char in text):\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "#clean non english characters from string of text\n",
    "def remove_non_english(text):\n",
    "    return ''.join(filter(lambda x: x in ascii_chars, text))\n",
    "\n",
    "\n",
    "def get_first_word(word):\n",
    "    if(type(word) != \"float\"):\n",
    "        return word.split(\" \")[0]\n",
    "    return \"-1\"\n",
    "\n",
    "def get_cap_vs_length(row):\n",
    "    if row[\"total_length\"] == 0:\n",
    "        return -1\n",
    "    return float(row['capitals'])/float(row['total_length'])\n",
    "\n",
    "def calc_max_word_len(sentence):\n",
    "    maxLen = 0\n",
    "    for word in sentence:\n",
    "        maxLen = max(maxLen, len(word))\n",
    "    return maxLen\n",
    "\n",
    "def calc_min_word_len(sentence):\n",
    "    minLen = 999999\n",
    "    for word in sentence:\n",
    "        minLen = min(minLen, len(word))\n",
    "    return minLen\n",
    "\n",
    "def calc_total_word_len(sentence):\n",
    "    cnt = 0\n",
    "    for x in sentence:\n",
    "        cnt+=len(x)\n",
    "    return cnt\n",
    "\n",
    "def calc_total_unique_word_len(sentence):\n",
    "    words = set(sentence)\n",
    "    return calc_total_word_len(words)\n",
    "\n",
    "#removes all single characters except for \"I\" and \"a\"\n",
    "def remove_singles(text):\n",
    "    return ' '.join( [w for w in text.split() if ((len(w)>1) or (w.lower() == \"i\") or (w.lower() == \"a\"))] )\n",
    "    \n",
    "#combines multiple whitespaces into single\n",
    "def clean_text(x):\n",
    "    x = str(x)\n",
    "    x = x.replace(\"-\", '')\n",
    "    x = x.replace(\"/\", '')\n",
    "    x = x.replace(\"'\",'')\n",
    "    x = re.sub( '\\s+', ' ', x).strip()\n",
    "    for punct in '&':\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    for punct in '?!.,\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~' + '“”’':\n",
    "        x = x.replace(punct, '')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    '''\n",
    "    Credit goes to https://www.kaggle.com/gpreda/jigsaw-fast-compact-solution\n",
    "    '''\n",
    "    \n",
    "    # Text cleaning\n",
    "    \n",
    "    punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~`\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n",
    "    def clean_special_chars(text, punct):\n",
    "        for p in punct:\n",
    "            text = text.replace(p, ' ')\n",
    "        return text\n",
    "    \n",
    "    # TODO: speed up this func\n",
    "    def pad_chars(text,punct):\n",
    "        for p in punct:\n",
    "            text = re.sub('(?<=\\w)([!?,])', r' \\1', text)\n",
    "        return text\n",
    "    \n",
    "    # Feature generation\n",
    "    \n",
    "    def gen_feats(df):\n",
    "        #df = pd.concat([train.loc[:, 'qid' : 'question_text'], test], sort = 'False')\n",
    "\n",
    "        print(\"--- Generating non_eng\")\n",
    "        df[\"non_eng\"] = df[\"question_text\"].map(lambda x: contains_non_english(x))\n",
    "\n",
    "        print(\"--- Generating first_word\")\n",
    "        df[\"first_word\"] = df[\"question_text\"].map(lambda x: get_first_word(x))\n",
    "\n",
    "        print(\"--- Generating total_length (num chars)\")\n",
    "        df['total_length'] = df['question_text'].apply(len)\n",
    "\n",
    "        print(\"--- Generating capitals\")\n",
    "        df['capitals'] = df['question_text'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n",
    "\n",
    "        print(\"--- Generating caps_vs_length\")\n",
    "        df['caps_vs_length'] = df.apply(lambda row: get_cap_vs_length(row),axis=1)\n",
    "\n",
    "        #print(\"--- Generating num_exclamation_marks\")\n",
    "        #df['num_exclamation_marks'] = df['question_text'].apply(lambda comment: comment.count('!'))\n",
    "\n",
    "        print(\"--- Generating num_question_marks\")\n",
    "        df['num_question_marks'] = df['question_text'].apply(lambda comment: comment.count('?'))\n",
    "\n",
    "        print(\"--- Generating num_punctuation\")\n",
    "        df['num_punctuation'] = df['question_text'].apply(lambda comment: sum(comment.count(w) for w in '.,;:'))\n",
    "\n",
    "        #print(\"--- Generating num_symbols\")\n",
    "        #df['num_symbols'] = df['question_text'].apply(lambda comment: sum(comment.count(w) for w in '*&$%'))\n",
    "\n",
    "        print(\"--- Generating num_words\")\n",
    "        df['num_words'] = df['question_text'].apply(lambda comment: len(re.sub(r'[^\\w\\s]','',comment).split(\" \")))\n",
    "\n",
    "        print(\"--- Generating num_unique_words\")\n",
    "        df['num_unique_words'] = df['question_text'].apply(lambda comment: len(set(w for w in comment.split())))\n",
    "\n",
    "        print(\"--- Generating unique_word_over_num_words\")\n",
    "        df['unique_word_over_num_words'] = d['num_unique_words'] / test['num_words']\n",
    "\n",
    "        #print(\"--- Generating num_smilies\")\n",
    "        #df['num_smilies'] = df['question_text'].apply(lambda comment: sum(comment.count(w) for w in (':-)', ':)', ';-)', ';)')))\n",
    "\n",
    "        print(\"--- Generating num_sentences\")\n",
    "        df['num_sentences'] = df['question_text'].apply(lambda comment: len(re.split(r'[.!?]+', comment)))\n",
    "\n",
    "        print(\"--- Generating max_word_len\")\n",
    "        df['max_word_len'] = df['question_text'].apply(lambda comment: calc_max_word_len(re.sub(r'[^\\w\\s]','',comment).split(\" \")))\n",
    "        \n",
    "        print(\"--- Generating min_word_len\")\n",
    "        df['max_word_len'] = df['question_text'].apply(lambda comment: calc_min_word_len(re.sub(r'[^\\w\\s]','',comment).split(\" \")))\n",
    "        \n",
    "        print(\"--- Generating total_word_length (num of chars in words)\")\n",
    "        df['total_word_length'] = df['question_text'].apply(lambda comment: calc_total_word_len(re.sub(r'[^\\w\\s]','',comment).split(\" \")))\n",
    "        \n",
    "        print(\"--- Generating avg_word_len\")\n",
    "        df['avg_word_len'] = df['total_word_length'] / df['num_words']\n",
    "        \n",
    "        print(\"--- Generating total_unique_word_length (num of chars in words)\")\n",
    "        df['total_unique_word_length'] = df['question_text'].apply(lambda comment: calc_total_unique_word_len(re.sub(r'[^\\w\\s]','',comment).split(\" \")))\n",
    "        \n",
    "        print(\"--- Generating avg_unique_word_len\")\n",
    "        df['avg_unique_word_len'] = df['total_unique_word_length'] / df['num_unique_words']\n",
    "        \n",
    "\n",
    "    def cleanText(df):\n",
    "        print(\"--- cleaning text\")\n",
    "        df[\"comment_text\"] = df[\"comment_text\"].apply(lambda x: clean_text(x))\n",
    "\n",
    "        print(\"--- remove single characters\")\n",
    "        df[\"comment_text\"] = df[\"comment_text\"].apply(lambda x: remove_singles(x))\n",
    "\n",
    "        print(\"--- cleaning numbers\")\n",
    "        df[\"comment_text\"] = df[\"comment_text\"].apply(lambda x: clean_numbers(x))\n",
    "\n",
    "        print(\"--- cleaning misspellings\")\n",
    "        df[\"comment_text\"] = df[\"comment_text\"].apply(lambda x: replace_typical_misspell(x))\n",
    "\n",
    "        print(\"--- filling missing values\")\n",
    "        #clean chinese, korean, japanese characters\n",
    "        print('cleaning characters')\n",
    "        df[\"comment_text\"] = df[\"comment_text\"].map(lambda x: remove_non_english(x))\n",
    "\n",
    "        ## fill up the missing values\n",
    "        df[\"comment_text\"].fillna(\"\").values\n",
    "\n",
    "        #for getting num good and bad words\n",
    "        from wordcloud import STOPWORDS\n",
    "        from collections import defaultdict\n",
    "        import operator\n",
    "\n",
    "        train1_df = train[train[\"target\"]==1]\n",
    "        train0_df = train[train[\"target\"]==0]\n",
    "\n",
    "        ## custom function for ngram generation ##\n",
    "        def generate_ngrams(text, n_gram=1):\n",
    "            token = [token for token in text.lower().split(\" \") if token != \"\" if token not in STOPWORDS]\n",
    "            ngrams = zip(*[token[i:] for i in range(n_gram)])\n",
    "            return [\" \".join(ngram) for ngram in ngrams]\n",
    "\n",
    "        freq_dict_bad = defaultdict(int)\n",
    "        for sent in train1_df[\"question_text\"]:\n",
    "            for word in generate_ngrams(sent):\n",
    "                freq_dict_bad[word] += 1\n",
    "        freq_dict_bad = dict(freq_dict_bad)\n",
    "\n",
    "        freq_dict_good = defaultdict(int)\n",
    "        for sent in train0_df[\"question_text\"]:\n",
    "            for word in generate_ngrams(sent):\n",
    "                freq_dict_good[word] += 1\n",
    "        freq_dict_good = dict(freq_dict_good)\n",
    "\n",
    "        bad_words = sorted(freq_dict_bad, key=freq_dict_bad.get, reverse=True)[:1000]\n",
    "        good_words = sorted(freq_dict_good, key=freq_dict_good.get, reverse=True)[:1000]\n",
    "\n",
    "        print(\"--- Generating num_bad_words\")\n",
    "        train[\"num_bad_words\"] = train[\"question_text\"].map(lambda x: num_bad_words(x))\n",
    "        test[\"num_bad_words\"] = test[\"question_text\"].map(lambda x: num_bad_words(x))\n",
    "\n",
    "        print(\"--- Generating num_good_words\")\n",
    "        train[\"num_good_words\"] = train[\"question_text\"].map(lambda x: num_good_words(x))\n",
    "        test[\"num_good_words\"] = test[\"question_text\"].map(lambda x: num_good_words(x))\n",
    "\n",
    "    data = gen_feats(data)\n",
    "    #data = data.astype(str).apply(lambda x: clean_special_chars(x, punct))\n",
    "    data = data.astype(str).apply(lambda x: pad_chars(x, punct))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing data ...\n",
      "--- Generating non_eng\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'question_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-cbdcb231eb7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Preprocessing data ...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mx_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'comment_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'comment_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--- %s seconds ---\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-3f0a61f88da3>\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_good_words\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"question_text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnum_good_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_feats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m     \u001b[0;31m#data = data.astype(str).apply(lambda x: clean_special_chars(x, punct))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpad_chars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpunct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-3f0a61f88da3>\u001b[0m in \u001b[0;36mgen_feats\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--- Generating non_eng\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"non_eng\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"question_text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontains_non_english\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--- Generating first_word\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    866\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   4373\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4374\u001b[0m             return self._engine.get_value(s, k,\n\u001b[0;32m-> 4375\u001b[0;31m                                           tz=getattr(series.dtype, 'tz', None))\n\u001b[0m\u001b[1;32m   4376\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4377\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mholds_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_boolean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.index.Int64Engine._check_type\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'question_text'"
     ]
    }
   ],
   "source": [
    "train = pd.read_hdf('../input/train.h5')\n",
    "test = pd.read_hdf('../input/test.h5')\n",
    "\n",
    "if (SMALL_DATA):\n",
    "    train = train[:100]\n",
    "    test = test[:100]\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"Preprocessing data ...\")\n",
    "x_test = preprocess(test['comment_text'])\n",
    "x_train = preprocess(train['comment_text'])\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "y_train = np.where(train['target'] >= 0.5, 1, 0)\n",
    "y_aux_train = train[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenized_text = []\n",
    "for x in enu= tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(list(x_train) + list(x_test))\n",
    "\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_test  = tokenizer.texts_to_sequences(x_test)\n",
    "x_train_lens = [len(i) for i in x_train]\n",
    "x_test_lens  = [len(i) for i in x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "327576"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_features = None\n",
    "max_features = max_features or len(tokenizer.word_index) + 1\n",
    "max_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../input/pickled-crawl300d2m-for-kernel-competitions/crawl-300d-2M.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-8cade583438c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcrawl_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munknown_words_crawl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCRAWL_EMBEDDING_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munknown_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'unknown'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'n unknown words (crawl): '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munknown_words_crawl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-bd79276c975a>\u001b[0m in \u001b[0;36mbuild_matrix\u001b[0;34m(word_index, emb_path, unknown_token)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbuild_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munknown_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'unknown'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0membedding_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# TODO: Build random token instead of using unknown\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/pickled-crawl300d2m-for-kernel-competitions/crawl-300d-2M.pkl'"
     ]
    }
   ],
   "source": [
    "crawl_matrix, unknown_words_crawl = build_matrix(tokenizer.word_index, emb_path=CRAWL_EMBEDDING_PATH, unknown_token='unknown')\n",
    "print('n unknown words (crawl): ', len(unknown_words_crawl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_matrix, unknown_words_glove = build_matrix(tokenizer.word_index, emb_path=GLOVE_EMBEDDING_PATH, unknown_token='unknown')\n",
    "print('n unknown words (glove): ', len(unknown_words_glove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.concatenate([crawl_matrix, glove_matrix], axis=-1)\n",
    "embedding_matrix.shape\n",
    "\n",
    "del crawl_matrix\n",
    "del glove_matrix\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(data.Dataset):\n",
    "    def __init__(self, text, lens, y=None):\n",
    "        self.text = text\n",
    "        self.lens = lens\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lens)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is None:\n",
    "            return self.text[idx], self.lens[idx]\n",
    "        return self.text[idx], self.lens[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Collator(object):\n",
    "    def __init__(self,test=False,percentile=100):\n",
    "        self.test = test\n",
    "        self.percentile = percentile\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        global MAX_LEN\n",
    "        \n",
    "        if self.test:\n",
    "            texts, lens = zip(*batch)\n",
    "        else:\n",
    "            texts, lens, target = zip(*batch)\n",
    "\n",
    "        lens = np.array(lens)\n",
    "        max_len = min(int(np.percentile(lens, self.percentile)), MAX_LEN)\n",
    "        texts = torch.tensor(sequence.pad_sequences(texts, maxlen=max_len), dtype=torch.long).cuda()\n",
    "        \n",
    "        if self.test:\n",
    "            return texts\n",
    "        \n",
    "        return texts, torch.tensor(target, dtype=torch.float32).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_y_train = np.hstack([y_train[:, np.newaxis], y_aux_train])\n",
    "\n",
    "train_collate = Collator(percentile=96)\n",
    "train_dataset = TextDataset(x_train, x_train_lens, final_y_train)\n",
    "train_loader  = torch.utils.data.DataLoader(train_dataset, batch_size=512, shuffle=True, collate_fn=train_collate)\n",
    "\n",
    "test_collate = Collator(test=True)\n",
    "test_dataset = TextDataset(x_test, x_test_lens)\n",
    "test_loader  = torch.utils.data.DataLoader(test_dataset, batch_size=512, shuffle=False , collate_fn=test_collate)\n",
    "\n",
    "# del y_train, y_aux_train; gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def train_model(model, train_loader, test_loader, loss_fn, output_dim, lr=0.001,\n",
    "                batch_size=512, n_epochs=4,\n",
    "                enable_checkpoint_ensemble=True):\n",
    "    param_lrs = [{'params': param, 'lr': lr} for param in model.parameters()]\n",
    "    optimizer = torch.optim.Adam(param_lrs, lr=lr)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda epoch: 0.6 ** epoch)\n",
    "    \n",
    "    all_test_preds = []\n",
    "    checkpoint_weights = [2 ** epoch for epoch in range(n_epochs)]\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        model.train()\n",
    "        avg_loss = 0.\n",
    "        \n",
    "        for step, (seq_batch, y_batch) in enumerate(tqdm(train_loader, disable=False)):\n",
    "            y_pred = model(seq_batch)            \n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item() #/ len(train_loader)\n",
    "            \n",
    "            if step > 0 and step % 100 == 0:\n",
    "                print(step, avg_loss / step)\n",
    "            \n",
    "        model.eval()\n",
    "        test_preds = np.zeros((len(test), output_dim))\n",
    "    \n",
    "        for step, seq_batch in enumerate(test_loader):\n",
    "            y_pred = sigmoid(model(seq_batch).detach().cpu().numpy())\n",
    "            test_preds[step * batch_size:step * batch_size + y_pred.shape[0], :] = y_pred[:,:1]\n",
    "\n",
    "        all_test_preds.append(test_preds)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print('Epoch {}/{} \\t loss={:.4f} \\t time={:.2f}s'.format(\n",
    "              epoch + 1, n_epochs, avg_loss / len(train_loader), elapsed_time))\n",
    "\n",
    "    if enable_checkpoint_ensemble:\n",
    "        test_preds = np.average(all_test_preds, weights=checkpoint_weights, axis=0)    \n",
    "    else:\n",
    "        test_preds = all_test_preds[-1]\n",
    "        \n",
    "    return test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_size = final_y_train.shape[-1] - 1  # targets\n",
    "\n",
    "all_test_preds = []\n",
    "for model_idx in range(NUM_MODELS):\n",
    "    print('Model ', model_idx)\n",
    "    seed_everything(1234 + model_idx)\n",
    "    \n",
    "    model = NeuralNet(embedding_matrix, aux_size)\n",
    "    model.cuda()\n",
    "    \n",
    "    test_preds = train_model(model, train_loader, test_loader, output_dim=1, loss_fn=nn.BCEWithLogitsLoss(reduction='mean'))\n",
    "    all_test_preds.append(test_preds)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame.from_dict({\n",
    "    'id': test['id'],\n",
    "    'prediction': np.mean(all_test_preds, axis=0)[:, 0]\n",
    "})\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the solution is not validated in this kernel. So for tuning anything, you should build a validation framework using e. g. KFold CV. If you just check what works best by submitting, you are very likely to overfit to the public LB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ways to improve this kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This kernel is just a simple baseline kernel, so there are many ways to improve it. Some ideas to get you started:\n",
    "- ~~Use bucketing to train faster and fit more networks into the two hours. The winning team of the quora competition successfully used bucketing to drastically reduce the time it took to train RNNs. An excerpt from their [solution summary](https://www.kaggle.com/c/quora-insincere-questions-classification/discussion/80568#latest-487092):~~\n",
    "- Add a contraction mapping. E. g. mapping \"is'nt\" to \"is not\" can help the network because \"not\" is explicitly mentioned. They were very popular in the recent quora competition, see for example [this kernel](https://www.kaggle.com/theoviel/improve-your-score-with-some-text-preprocessing).\n",
    "- Try to reduce the number of words that are not found in the embeddings. At the moment, around 170k words are not found. We can take some steps to decrease this amount, for example trying to find a vector for a processed (capitalized, stemmed, ...) version of the word when the vector for the regular word can not be found. See the [3rd place solution](https://www.kaggle.com/wowfattie/3rd-place) of the quora competition for an excellent implementation of this.\n",
    "- Try cyclic learning rate (CLR). I have found CLR to almost always improve my network recently compared to the default parameters for Adam. In this case, we are already using a learning rate scheduler, so this might not be the case. But it is still worth to try it out. See for example my [my other PyTorch kernel](https://www.kaggle.com/bminixhofer/deterministic-neural-networks-using-pytorch) for an implementation of CLR in PyTorch.\n",
    "\n",
    "> We aimed at combining as many models as possible. To do this, we needed to improve runtime and the most important thing to achieve this was the following. We do not pad sequences to the same length based on the whole data, but just on a batch level. That means we conduct padding and truncation on the data generator level for each batch separately, so that length of the sentences in a batch can vary in size. Additionally, we further improved this by not truncating based on the length of the longest sequence in the batch, but based on the 95% percentile of lengths within the sequence. This improved runtime heavily and kept accuracy quite robust on single model level, and improved it by being able to average more models.\n",
    "\n",
    "- Try a (weighted) average of embeddings instead of concatenating them. A 600d vector for each word is a lot, it might work better to average them instead. See [this paper](https://www.aclweb.org/anthology/N18-2031) for why this even works.\n",
    "- Limit the maximum number of words used to train the NN. At the moment, there is no limit set to the maximum number of words in the tokenizer, so we use every word that occurs in the training data, even if it is only mentioned once. This could lead to overfitting so it might be better to limit the maximum number of words to e. g. 100k.\n",
    "\n",
    "Thanks for reading. Good luck and have fun in this competition!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Happy Kaggling!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
