{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "lc = LancasterStemmer()\n",
    "from nltk.stem import SnowballStemmer\n",
    "sb = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 2.2707815170288086 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "train = pd.read_hdf('../input/train.h5')[:100]\n",
    "test = pd.read_hdf('../input/test.h5')[:100]\n",
    "train_text = train['comment_text']\n",
    "test_text = test['comment_text']\n",
    "text_list = pd.concat([train_text, test_text])\n",
    "y = train['target'].values\n",
    "num_train_data = y.shape[0]\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(train))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spacy NLP ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [00:00, 1364.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 7.462452411651611 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Third place solution\n",
    "# seems like the kind of tokenizer you use doesn't matter too much\n",
    "start_time = time.time()\n",
    "print(\"Spacy NLP ...\")\n",
    "nlp = spacy.load('en_core_web_lg', disable=['parser','ner','tagger'])\n",
    "nlp.vocab.add_flag(lambda s: s.lower() in spacy.lang.en.stop_words.STOP_WORDS, spacy.attrs.IS_STOP)\n",
    "word_dict = {}\n",
    "word_index = 1\n",
    "lemma_dict = {}\n",
    "docs = nlp.pipe(text_list, n_threads = 2)\n",
    "word_sequences = []\n",
    "for doc in tqdm(docs):\n",
    "    word_seq = []\n",
    "    for token in doc:\n",
    "        if (token.text not in word_dict) and (token.pos_ is not \"PUNCT\"):\n",
    "            word_dict[token.text] = word_index\n",
    "            word_index += 1\n",
    "            lemma_dict[token.text] = token.lemma_\n",
    "        if token.pos_ is not \"PUNCT\":\n",
    "            word_seq.append(word_dict[token.text])\n",
    "    word_sequences.append(word_seq)\n",
    "del docs\n",
    "gc.collect()\n",
    "# test and train_word_sequences are mapped numeric values for the words/characters\n",
    "train_word_sequences = word_sequences[:num_train_data]\n",
    "test_word_sequences = word_sequences[num_train_data:]\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'This': 'This',\n",
       " 'is': 'be',\n",
       " 'so': 'so',\n",
       " 'cool': 'cool',\n",
       " '.': '.',\n",
       " 'It': '-PRON-',\n",
       " \"'s\": 'have',\n",
       " 'like': 'like',\n",
       " ',': ',',\n",
       " \"'\": \"'\",\n",
       " 'would': 'would',\n",
       " 'you': 'you',\n",
       " 'want': 'want',\n",
       " 'your': 'your',\n",
       " 'mother': 'mother',\n",
       " 'to': 'to',\n",
       " 'read': 'read',\n",
       " 'this': 'this',\n",
       " '?': '?',\n",
       " 'Really': 'Really',\n",
       " 'great': 'great',\n",
       " 'idea': 'idea',\n",
       " 'well': 'good',\n",
       " 'done': 'do',\n",
       " '!': '!',\n",
       " 'Thank': 'Thank',\n",
       " 'make': 'make',\n",
       " 'my': 'my',\n",
       " 'life': 'life',\n",
       " 'a': 'a',\n",
       " 'lot': 'lot',\n",
       " 'less': 'little',\n",
       " 'anxiety': 'anxiety',\n",
       " '-': '-',\n",
       " 'inducing': 'induce',\n",
       " 'Keep': 'Keep',\n",
       " 'it': 'it',\n",
       " 'up': 'up',\n",
       " 'and': 'and',\n",
       " 'do': 'do',\n",
       " \"n't\": 'not',\n",
       " 'let': 'let',\n",
       " 'anyone': 'anyone',\n",
       " 'get': 'get',\n",
       " 'in': 'in',\n",
       " 'way': 'way',\n",
       " 'such': 'such',\n",
       " 'an': 'a',\n",
       " 'urgent': 'urgent',\n",
       " 'design': 'design',\n",
       " 'problem': 'problem',\n",
       " ';': ';',\n",
       " 'kudos': 'kudo',\n",
       " 'for': 'for',\n",
       " 'taking': 'take',\n",
       " 'on': 'on',\n",
       " 'Very': 'Very',\n",
       " 'impressive': 'impressive',\n",
       " 'Is': 'Is',\n",
       " 'something': 'something',\n",
       " 'I': '-PRON-',\n",
       " \"'ll\": 'will',\n",
       " 'be': 'be',\n",
       " 'able': 'able',\n",
       " 'install': 'install',\n",
       " 'site': 'site',\n",
       " 'When': 'When',\n",
       " 'will': 'will',\n",
       " 'releasing': 'release',\n",
       " 'haha': 'haha',\n",
       " 'guys': 'guy',\n",
       " 'are': 'be',\n",
       " 'bunch': 'bunch',\n",
       " 'of': 'of',\n",
       " 'losers': 'loser',\n",
       " 'ur': 'ur',\n",
       " 'sh*tty': 'sh*tty',\n",
       " 'comment': 'comment',\n",
       " 'hahahahahahahahhha': 'hahahahahahahahhha',\n",
       " 'suck': 'suck',\n",
       " 'FFFFUUUUUUUUUUUUUUU': 'FFFFUUUUUUUUUUUUUUU',\n",
       " 'The': 'The',\n",
       " 'ranchers': 'rancher',\n",
       " 'seem': 'seem',\n",
       " 'motivated': 'motivate',\n",
       " 'by': 'by',\n",
       " 'mostly': 'mostly',\n",
       " 'greed': 'greed',\n",
       " 'no': 'no',\n",
       " 'one': 'one',\n",
       " 'should': 'should',\n",
       " 'have': 'have',\n",
       " 'the': 'the',\n",
       " 'right': 'right',\n",
       " 'allow': 'allow',\n",
       " 'their': 'their',\n",
       " 'animals': 'animal',\n",
       " 'destroy': 'destroy',\n",
       " 'public': 'public',\n",
       " 'land': 'land',\n",
       " 'was': 'be',\n",
       " 'show': 'show',\n",
       " 'Not': 'Not',\n",
       " 'combo': 'combo',\n",
       " \"'d\": 'would',\n",
       " 'expected': 'expect',\n",
       " 'good': 'good',\n",
       " 'together': 'together',\n",
       " 'but': 'but',\n",
       " 'Wow': 'Wow',\n",
       " 'that': 'that',\n",
       " 'sounds': 'sound',\n",
       " 'story': 'story',\n",
       " 'Man': 'Man',\n",
       " 'wonder': 'wonder',\n",
       " 'if': 'if',\n",
       " 'person': 'person',\n",
       " 'who': 'who',\n",
       " 'yelled': 'yell',\n",
       " '\"': '\"',\n",
       " 'shut': 'shut',\n",
       " 'fuck': 'fuck',\n",
       " 'at': 'at',\n",
       " 'him': 'him',\n",
       " 'ever': 'ever',\n",
       " 'heard': 'hear',\n",
       " 'seems': 'seem',\n",
       " 'step': 'step',\n",
       " 'direction': 'direction',\n",
       " 'ridiculous': 'ridiculous',\n",
       " 'these': 'this',\n",
       " 'being': 'be',\n",
       " 'called': 'call',\n",
       " 'protesters': 'protester',\n",
       " 'Being': 'Being',\n",
       " 'armed': 'arm',\n",
       " 'threat': 'threat',\n",
       " 'violence': 'violence',\n",
       " 'which': 'which',\n",
       " 'makes': 'make',\n",
       " 'them': 'them',\n",
       " 'terrorists': 'terrorist',\n",
       " 'gets': 'get',\n",
       " 'more': 'much',\n",
       " 'hour': 'hour',\n",
       " 'And': 'And',\n",
       " 'love': 'love',\n",
       " 'people': 'people',\n",
       " 'sending': 'send',\n",
       " 'dildos': 'dildo',\n",
       " 'mail': 'mail',\n",
       " 'now': 'now',\n",
       " 'But': 'But',\n",
       " '…': '…',\n",
       " 'they': 'they',\n",
       " 'really': 'really',\n",
       " 'think': 'think',\n",
       " 'there': 'there',\n",
       " 'happy': 'happy',\n",
       " 'ending': 'end',\n",
       " 'any': 'any',\n",
       " \"'re\": 'be',\n",
       " 'even': 'even',\n",
       " 'deluded': 'delude',\n",
       " 'than': 'than',\n",
       " 'all': 'all',\n",
       " 'jokes': 'joke',\n",
       " 'about': 'about',\n",
       " 'assume': 'assume',\n",
       " 'agree': 'agree',\n",
       " 'grant': 'grant',\n",
       " 'legitimacy': 'legitimacy',\n",
       " 'protestors': 'protestor',\n",
       " 'They': '-PRON-',\n",
       " 'greedy': 'greedy',\n",
       " 'small': 'small',\n",
       " 'minded': 'mind',\n",
       " 'somehow': 'somehow',\n",
       " 'share': 'share',\n",
       " 'mass': 'mass',\n",
       " 'delusion': 'delusion',\n",
       " 'not': 'not',\n",
       " 'only': 'only',\n",
       " 'themselves': 'themselves',\n",
       " 'as': 'a',\n",
       " 'individuals': 'individual',\n",
       " 'thing': 'thing',\n",
       " 'large': 'large',\n",
       " 'Basically': 'Basically',\n",
       " ':': ':',\n",
       " 'take': 'take',\n",
       " 'currently': 'currently',\n",
       " 'belongs': 'belong',\n",
       " 'everyone': 'everyone',\n",
       " 'give': 'give',\n",
       " 'select': 'select',\n",
       " 'group': 'group',\n",
       " 'can': 'can',\n",
       " 'profit': 'profit',\n",
       " 'Interesting': 'Interesting',\n",
       " 'curious': 'curious',\n",
       " 'see': 'see',\n",
       " 'how': 'how',\n",
       " 'works': 'work',\n",
       " 'out': 'out',\n",
       " 'often': 'often',\n",
       " 'refrain': 'refrain',\n",
       " 'from': 'from',\n",
       " 'commenting': 'comment',\n",
       " 'because': 'because',\n",
       " 'time': 'time',\n",
       " 'or': 'or',\n",
       " 'desire': 'desire',\n",
       " 'engage': 'engage',\n",
       " 'with': 'with',\n",
       " 'couple': 'couple',\n",
       " 'resident': 'resident',\n",
       " 'trolls': 'troll',\n",
       " 'jump': 'jump',\n",
       " 'every': 'every',\n",
       " 'active': 'active',\n",
       " 'WW': 'WW',\n",
       " 'thread': 'thread',\n",
       " 'Awesome': 'Awesome',\n",
       " 'Civil': 'Civil',\n",
       " 'Comments': 'Comments',\n",
       " \"'m\": 'be',\n",
       " 'glad': 'glad',\n",
       " 'working': 'work',\n",
       " 'look': 'look',\n",
       " 'forward': 'forward',\n",
       " 'seeing': 'see',\n",
       " 'plays': 'play',\n",
       " '   ': '   ',\n",
       " 'comments': 'comment',\n",
       " 'sections': 'section',\n",
       " 'online': 'online',\n",
       " 'news': 'news',\n",
       " 'stories': 'story',\n",
       " 'potential': 'potential',\n",
       " 'tools': 'tool',\n",
       " 'community': 'community',\n",
       " 'interaction': 'interaction',\n",
       " 'current': 'current',\n",
       " 'events': 'event',\n",
       " 'Neo': 'Neo',\n",
       " 'Town': 'Town',\n",
       " 'Hall': 'Hall',\n",
       " 'sorts': 'sort',\n",
       " ' ': ' ',\n",
       " 'One': 'One',\n",
       " 'reasons': 'reason',\n",
       " 'rely': 'rely',\n",
       " 'Reddit': 'Reddit',\n",
       " 'platform': 'platform',\n",
       " 'local': 'local',\n",
       " 'discussions': 'discussion',\n",
       " 'sense': 'sense',\n",
       " 'lacking': 'lack',\n",
       " 'real': 'real',\n",
       " 'hectic': 'hectic',\n",
       " ' \\n\\n': ' \\n\\n',\n",
       " 'hopefully': 'hopefully',\n",
       " 'we': 'we',\n",
       " 'wo': 'will',\n",
       " 'tempted': 'tempt',\n",
       " 'silence': 'silence',\n",
       " 'those': 'that',\n",
       " 'unpopular': 'unpopular',\n",
       " 'stances': 'stance',\n",
       " 'Angry': 'Angry',\n",
       " 'misogynists': 'misogynist',\n",
       " 'Racists': 'Racists',\n",
       " 'oh': 'oh',\n",
       " 'does': 'do',\n",
       " '150': '150',\n",
       " 'IQ': 'IQ',\n",
       " 'slant': 'slant',\n",
       " 'here': 'here',\n",
       " 'Diversity': 'Diversity',\n",
       " 'diode': 'diode',\n",
       " 'work': 'work',\n",
       " 'yet': 'yet',\n",
       " 'again': 'again',\n",
       " 'We': 'We',\n",
       " 'say': 'say',\n",
       " 'anything': 'anything',\n",
       " 'You': 'You',\n",
       " 'other': 'other',\n",
       " 'hand': 'hand',\n",
       " 'must': 'must',\n",
       " 'what': 'what',\n",
       " 'From': 'From',\n",
       " 'winning': 'win',\n",
       " 'arguments': 'argument',\n",
       " 'against': 'against',\n",
       " 'member': 'member',\n",
       " 'diversity': 'diversity',\n",
       " 'considered': 'consider',\n",
       " 'offensive': 'offensive',\n",
       " 'language': 'language',\n",
       " 'facts': 'fact',\n",
       " 'cogent': 'cogent',\n",
       " 'linear': 'linear',\n",
       " 'posts': 'post',\n",
       " 'Math': 'Math',\n",
       " 'verboten': 'verboten',\n",
       " 'Nice': 'Nice',\n",
       " 'some': 'some',\n",
       " 'attempts': 'attempt',\n",
       " 'try': 'try',\n",
       " 'better': 'well',\n",
       " '—': '—',\n",
       " 'feels': 'feel',\n",
       " 'innovation': 'innovation',\n",
       " 'communities': 'community',\n",
       " 'ended': 'end',\n",
       " 'launch': 'launch',\n",
       " 'Disqus': 'Disqus',\n",
       " 'nearly': 'nearly',\n",
       " 'decade': 'decade',\n",
       " 'ago': 'ago',\n",
       " 'hope': 'hope',\n",
       " 'purpose': 'purpose',\n",
       " 'introducing': 'introduce',\n",
       " 'system': 'system',\n",
       " 'encourage': 'encourage',\n",
       " 'debate': 'debate',\n",
       " 'discussion': 'discussion',\n",
       " 'several': 'several',\n",
       " 'things': 'thing',\n",
       " 'limit': 'limit',\n",
       " 'flow': 'flow',\n",
       " 'Making': 'Making',\n",
       " 'personal': 'personal',\n",
       " 'attacks': 'attack',\n",
       " 'encouraging': 'encourage',\n",
       " 'witch': 'witch',\n",
       " 'hunts': 'hunt',\n",
       " 'true': 'true',\n",
       " 'spam': 'spam',\n",
       " '(': '(',\n",
       " 'has': 'have',\n",
       " 'been': 'be',\n",
       " 'section': 'section',\n",
       " 'experience': 'experience',\n",
       " ')': ')',\n",
       " 'chasing': 'chase',\n",
       " 'rabbits': 'rabbit',\n",
       " 'draw': 'draw',\n",
       " 'away': 'away',\n",
       " 'primary': 'primary',\n",
       " 'subject': 'subject',\n",
       " '\\n\\n': '\\n\\n',\n",
       " 'opinions': 'opinion',\n",
       " 'silenced': 'silence',\n",
       " 'Healthy': 'Healthy',\n",
       " 'important': 'important',\n",
       " 'component': 'component',\n",
       " 'civil': 'civil',\n",
       " 'society': 'society',\n",
       " 'when': 'when',\n",
       " 'degenerate': 'degenerate',\n",
       " 'into': 'into',\n",
       " 'insults': 'insult',\n",
       " 'distraction': 'distraction',\n",
       " 'actually': 'actually',\n",
       " 'limits': 'limit',\n",
       " 'ability': 'ability',\n",
       " 'talk': 'talk',\n",
       " 'each': 'each',\n",
       " 'believing': 'believe',\n",
       " 'intention': 'intention',\n",
       " 'newspaper': 'newspaper',\n",
       " 'movement': 'movement',\n",
       " 'results': 'result',\n",
       " 'commentary': 'commentary',\n",
       " 'need': 'need',\n",
       " 'randomly': 'randomly',\n",
       " 'chosen': 'choose',\n",
       " 'reviewed': 'review',\n",
       " 'bet': 'bet',\n",
       " 'thorough': 'thorough',\n",
       " 'answer': 'answer',\n",
       " 'question': 'question',\n",
       " 'She': 'She',\n",
       " 'major': 'major',\n",
       " 'improvement': 'improvement',\n",
       " 'city': 'city',\n",
       " 'council': 'council',\n",
       " 'she': 'she',\n",
       " 'long': 'long',\n",
       " 'history': 'history',\n",
       " 'giving': 'give',\n",
       " 'citizens': 'citizen',\n",
       " 'voice': 'voice',\n",
       " 'solver': 'solver',\n",
       " 'Portland': 'Portland',\n",
       " 'needs': 'need',\n",
       " 'much': 'much',\n",
       " 'places': 'place',\n",
       " 'especially': 'especially',\n",
       " 'publications': 'publication',\n",
       " 'already': 'already',\n",
       " 'serve': 'serve',\n",
       " 'amazing': 'amaze',\n",
       " 'then': 'then',\n",
       " 'biased': 'bias',\n",
       " \"I'm\": \"I'm\",\n",
       " 'co': 'co',\n",
       " 'founder': 'founder',\n",
       " \"'ve\": 'have',\n",
       " 'worked': 'work',\n",
       " 'hard': 'hard',\n",
       " 'sure': 'sure',\n",
       " 'become': 'become',\n",
       " 'echo': 'echo',\n",
       " 'chamber': 'chamber',\n",
       " 'though': 'though',\n",
       " 'combining': 'combine',\n",
       " 'clever': 'clever',\n",
       " 'algorithms': 'algorithm',\n",
       " 'backend': 'backend',\n",
       " 'peer': 'peer',\n",
       " 'reviews': 'review',\n",
       " 'That': 'that',\n",
       " 'also': 'also',\n",
       " 'why': 'why',\n",
       " 'two': 'two',\n",
       " 'separate': 'separate',\n",
       " 'questions': 'question',\n",
       " '*': '*',\n",
       " 'Great': 'Great',\n",
       " 'asked': 'ask',\n",
       " 'designed': 'design',\n",
       " 'assuming': 'assume',\n",
       " 'abuse': 'abuse',\n",
       " 'So': 'So',\n",
       " 'addition': 'addition',\n",
       " 'doing': 'do',\n",
       " 'meta': 'meta',\n",
       " 'analysis': 'analysis',\n",
       " '100': '100',\n",
       " '%': '%',\n",
       " 'perfect': 'perfect',\n",
       " 'know': 'know',\n",
       " 'months': 'month',\n",
       " 'beta': 'beta',\n",
       " 'testing': 'test',\n",
       " 'solid': 'solid',\n",
       " 'start': 'start',\n",
       " 'keep': 'keep',\n",
       " 'improve': 'improve',\n",
       " 'Thanks': 'Thanks',\n",
       " 'Christa': 'Christa',\n",
       " 'Will': 'Will',\n",
       " 'adding': 'add',\n",
       " 'features': 'feature',\n",
       " 'overall': 'overall',\n",
       " 'upvotes': 'upvotes',\n",
       " 'article': 'article',\n",
       " 'itself': 'itself',\n",
       " 'Also': 'Also',\n",
       " 'notification': 'notification',\n",
       " 'settings': 'setting',\n",
       " 'users': 'user',\n",
       " 'Our': 'Our',\n",
       " 'aim': 'aim',\n",
       " 'opposite': 'opposite',\n",
       " 'spirited': 'spirit',\n",
       " 'free': 'free',\n",
       " 'participate': 'participate',\n",
       " 'without': 'without',\n",
       " 'fear': 'fear',\n",
       " 'harassment': 'harassment',\n",
       " 'death': 'death',\n",
       " 'threats': 'threat',\n",
       " 'Right': 'Right',\n",
       " 'voices': 'voice',\n",
       " 'due': 'due',\n",
       " 'respectful': 'respectful',\n",
       " 'positive': 'positive',\n",
       " 'There': 'there',\n",
       " 'going': 'go',\n",
       " 'prevent': 'prevent',\n",
       " 'speak': 'speak',\n",
       " 'mind': 'mind',\n",
       " 'opinion': 'opinion',\n",
       " 'treat': 'treat',\n",
       " 'respect': 'respect',\n",
       " 'civility': 'civility',\n",
       " 'dynamic': 'dynamic',\n",
       " 'interesting': 'interest',\n",
       " 'applaud': 'applaud',\n",
       " 'efforts': 'effort',\n",
       " 'create': 'create',\n",
       " 'new': 'new',\n",
       " 'technology': 'technology',\n",
       " 'field': 'field',\n",
       " 'Hoping': 'Hoping',\n",
       " 'thoughtful': 'thoughtful',\n",
       " 'moving': 'move',\n",
       " 'Why': 'Why',\n",
       " 'bother': 'bother',\n",
       " 'writing': 'write',\n",
       " 'review': 'review',\n",
       " 'devoid': 'devoid',\n",
       " 'content': 'content',\n",
       " 'understand': 'understand',\n",
       " 'hardly': 'hardly',\n",
       " 'excuse': 'excuse',\n",
       " 'Fred': 'Fred',\n",
       " 'Armisen': 'Armisen',\n",
       " 'recorded': 'record',\n",
       " 'radio': 'radio',\n",
       " 'Grand': 'Grand',\n",
       " 'Theft': 'Theft',\n",
       " 'Auto': 'Auto',\n",
       " 'IV': 'IV',\n",
       " 'same': 'same',\n",
       " 'number': 'numb',\n",
       " 'Yet': 'Yet',\n",
       " 'call': 'call',\n",
       " 'Muslims': 'Muslim',\n",
       " 'acts': 'act',\n",
       " 'few': 'few',\n",
       " 'pilloried': 'pillory',\n",
       " '  ': '  ',\n",
       " 'okay': 'okay',\n",
       " 'smear': 'smear',\n",
       " 'entire': 'entire',\n",
       " 'religion': 'religion',\n",
       " 'over': 'over',\n",
       " 'idiots': 'idiot',\n",
       " 'Or': 'Or',\n",
       " 'bash': 'bash',\n",
       " 'Christian': 'Christian',\n",
       " 'sects': 'sect',\n",
       " 'upvoting': 'upvoting',\n",
       " 'articles': 'article',\n",
       " 'Publisher': 'Publisher',\n",
       " 'could': 'can',\n",
       " 'turn': 'turn',\n",
       " 'off': 'off',\n",
       " 'ton': 'ton',\n",
       " 'following': 'follow',\n",
       " 'blocking': 'block',\n",
       " 'bookmarking': 'bookmark',\n",
       " 'during': 'during',\n",
       " 'process': 'process',\n",
       " 'later': 'late',\n",
       " 'find': 'find',\n",
       " 'myself': 'myself',\n",
       " 'wanting': 'want',\n",
       " 'reviewing': 'review',\n",
       " 'conversations': 'conversation',\n",
       " 'enthusiasm': 'enthusiasm',\n",
       " 'Melinda': 'Melinda',\n",
       " 'patience': 'patience',\n",
       " 'brand': 'brand',\n",
       " 'approach': 'approach',\n",
       " 'continue': 'continue',\n",
       " 'welcome': 'welcome',\n",
       " 'suggestions': 'suggestion',\n",
       " 'feedback': 'feedback',\n",
       " 'Troll': 'Troll',\n",
       " 'since': 'since',\n",
       " '2016': '2016',\n",
       " 'bitch': 'bitch',\n",
       " 'nuts': 'nut',\n",
       " 'Who': 'Who',\n",
       " 'book': 'book',\n",
       " 'woman': 'woman',\n",
       " 'In': 'In',\n",
       " 'Training': 'Training',\n",
       " 'Commenting': 'Commenting',\n",
       " 'sake': 'sake',\n",
       " 'rate': 'rate',\n",
       " 'our': 'our',\n",
       " 'concept': 'concept',\n",
       " 'How': 'How',\n",
       " 'plan': 'plan',\n",
       " 'monetize': 'monetize',\n",
       " 'operation': 'operation',\n",
       " 'Pity': 'Pity',\n",
       " 'menu': 'menu',\n",
       " 'lost': 'lose',\n",
       " 'vegan': 'vegan',\n",
       " 'food': 'food',\n",
       " 'Mash': 'Mash',\n",
       " 'Tun': 'Tun',\n",
       " 'favorite': 'favorite',\n",
       " 'beer': 'beer',\n",
       " 'bar': 'bar',\n",
       " 'delicious': 'delicious',\n",
       " 'tempeh': 'tempeh',\n",
       " 'stuff': 'stuff',\n",
       " '\\n': '\\n',\n",
       " 'Excited': 'Excited',\n",
       " 'staff': 'staff',\n",
       " 'still': 'still',\n",
       " 'ca': 'can',\n",
       " 'wait': 'wait',\n",
       " 'beers': 'beer',\n",
       " 'dozens': 'dozen',\n",
       " 'just': 'just',\n",
       " 'vote': 'vote',\n",
       " 'type': 'type',\n",
       " 'voting': 'vote',\n",
       " 'completely': 'completely',\n",
       " 'insane-': 'insane-',\n",
       " 'extra': 'extra',\n",
       " 'multiple': 'multiple',\n",
       " 'non': 'non',\n",
       " 'related': 'relate',\n",
       " 'installed': 'install',\n",
       " 'oregonlive.com': 'oregonlive.com',\n",
       " 'enough': 'enough',\n",
       " 'visitors': 'visitor',\n",
       " 'established': 'establish',\n",
       " 'troll': 'troll',\n",
       " 'base': 'base',\n",
       " 'service': 'service',\n",
       " 'With': 'With',\n",
       " 'little': 'little',\n",
       " 'activity': 'activity',\n",
       " 'wweek': 'wweek',\n",
       " 'discourage': 'discourage',\n",
       " 'further': 'far',\n",
       " 'growth': 'growth',\n",
       " 'project': 'project',\n",
       " 'Signed': 'Signed',\n",
       " 'shot': 'shoot',\n",
       " '...': '...',\n",
       " 'luck': 'luck',\n",
       " 'enterprise': 'enterprise',\n",
       " 'disqus': 'disqus',\n",
       " 'functionality': 'functionality',\n",
       " 'obviously': 'obviously',\n",
       " 'huge': 'huge',\n",
       " 'increase': 'increase',\n",
       " 'visitor': 'visitor',\n",
       " 'count': 'count',\n",
       " 'significantly': 'significantly',\n",
       " 'moderator': 'moderator',\n",
       " 'feature': 'feature',\n",
       " 'either': 'either',\n",
       " 'never': 'never',\n",
       " 'wanted': 'want',\n",
       " 'pay': 'pay',\n",
       " 'undertake': 'undertake',\n",
       " 'developed': 'develope',\n",
       " 'trust': 'trust',\n",
       " 'behalf': 'behalf',\n",
       " 'volunteer': 'volunteer',\n",
       " 'basis': 'basis',\n",
       " 'YET': 'YET',\n",
       " 'ANOTHER': 'ANOTHER',\n",
       " 'BARACK': 'BARACK',\n",
       " 'OBAMA': 'OBAMA',\n",
       " 'LIBERAL': 'LIBERAL',\n",
       " 'MEDIA': 'MEDIA',\n",
       " 'CONSPIRACY': 'CONSPIRACY',\n",
       " 'BY': 'BY',\n",
       " 'THE': 'THE',\n",
       " 'THOUGHT': 'THOUGHT',\n",
       " 'CONTROL': 'CONTROL',\n",
       " 'POLICE': 'POLICE',\n",
       " 'DIDENT': 'DIDENT',\n",
       " 'SPEND': 'SPEND',\n",
       " '30': '30',\n",
       " 'YEARS': 'YEARS',\n",
       " 'MIXING': 'MIXING',\n",
       " 'CONCRETE': 'CONCRETE',\n",
       " 'TO': 'TO',\n",
       " 'LET': 'LET',\n",
       " 'AMERICA': 'AMERICA',\n",
       " 'FALL': 'FALL',\n",
       " 'COMMIES': 'COMMIES',\n",
       " 'TAXES': 'TAXES',\n",
       " 'HERE': 'HERE',\n",
       " 'ARE': 'ARE',\n",
       " 'SO': 'SO',\n",
       " 'HIGH': 'HIGH',\n",
       " 'CANT': 'CANT',\n",
       " 'EVEN': 'EVEN',\n",
       " 'AFFORD': 'AFFORD',\n",
       " 'A': 'A',\n",
       " 'KEYBOARD': 'KEYBOARD',\n",
       " 'WITH': 'WITH',\n",
       " 'WORKING': 'WORKING',\n",
       " 'CAPS': 'CAPS',\n",
       " 'LOCK': 'LOCK',\n",
       " 'PORTLAND': 'PORTLAND',\n",
       " 'HAS': 'HAS',\n",
       " 'BEEN': 'BEEN',\n",
       " 'GOING': 'GOING',\n",
       " 'DOWNHILL': 'DOWNHILL',\n",
       " 'FOR': 'FOR',\n",
       " 'NO': 'NO',\n",
       " 'WONDER': 'WONDER',\n",
       " 'TEA': 'TEA',\n",
       " 'PARTY': 'PARTY',\n",
       " 'IS': 'IS',\n",
       " 'MAKING': 'MAKING',\n",
       " 'COMEBACK': 'COMEBACK',\n",
       " 'WHATS': 'WHATS',\n",
       " 'NEXT': 'NEXT',\n",
       " 'FLOURAIDE': 'FLOURAIDE',\n",
       " 'IN': 'IN',\n",
       " 'WATER': 'WATER',\n",
       " 'SUPPLY': 'SUPPLY',\n",
       " 'seen': 'see',\n",
       " 'kind': 'kind',\n",
       " 'mentioning': 'mention',\n",
       " 'interested': 'interest',\n",
       " 'where': 'where',\n",
       " 'having': 'have',\n",
       " 'problems': 'problem',\n",
       " 'Do': 'Do',\n",
       " 'link': 'link',\n",
       " 'mentioned': 'mention',\n",
       " 'crazy': 'crazy',\n",
       " 'illustration': 'illustration',\n",
       " 'thought': 'think',\n",
       " 'pitch': 'pitch',\n",
       " 'everything': 'everything',\n",
       " 'yellow': 'yellow',\n",
       " 'orange': 'orange',\n",
       " 'wmcelha': 'wmcelha',\n",
       " 'exciting': 'excite',\n",
       " 'gluten': 'gluten',\n",
       " 'options': 'option',\n",
       " 'Buffalo': 'Buffalo',\n",
       " 'tostones': 'tostones',\n",
       " 'fun': 'fun',\n",
       " 'soy': 'soy',\n",
       " 'option': 'option',\n",
       " 'mention': 'mention',\n",
       " 'killer': 'killer',\n",
       " 'veggie': 'veggie',\n",
       " 'burger': 'burger',\n",
       " 'tacos': 'tacos',\n",
       " 'nice': 'nice',\n",
       " 'salads': 'salad',\n",
       " 'Hope': 'Hope',\n",
       " 'soon': 'soon',\n",
       " 'thank': 'thank',\n",
       " 'polluted': 'pollute',\n",
       " 'far': 'far',\n",
       " 'too': 'too',\n",
       " 'lack': 'lack',\n",
       " 'moderation': 'moderation',\n",
       " 'gave': 'give',\n",
       " 'loud': 'loud',\n",
       " 'megaphone': 'megaphone',\n",
       " 'wing': 'wing',\n",
       " 'Probably': 'Probably',\n",
       " 'consistently': 'consistently',\n",
       " 'waste': 'waste',\n",
       " 'funds': 'fund',\n",
       " 'trendy': 'trendy',\n",
       " 'projects': 'project',\n",
       " 'green': 'green',\n",
       " 'bike': 'bike',\n",
       " 'boxes': 'box',\n",
       " '--': '--',\n",
       " 'cost': 'cost',\n",
       " 'fortune': 'fortune',\n",
       " 'repainted': 'repaint',\n",
       " 'years': 'year',\n",
       " 'Oy': 'Oy',\n",
       " 'set': 'set',\n",
       " 'traffic': 'traffic',\n",
       " 'slow': 'slow',\n",
       " 'algorithm': 'algorithm',\n",
       " 'takes': 'take',\n",
       " 'until': 'until',\n",
       " 'picks': 'pick',\n",
       " 'Because': 'Because',\n",
       " 'drive': 'drive',\n",
       " 'cars': 'car',\n",
       " 'ones': 'one',\n",
       " 'cause': 'cause',\n",
       " 'wear': 'wear',\n",
       " 'tear': 'tear',\n",
       " 'roads': 'road',\n",
       " 'Pretty': 'Pretty',\n",
       " 'fair': 'fair',\n",
       " 'straightforward': 'straightforward',\n",
       " 'Affordable': 'Affordable',\n",
       " 'housing': 'house',\n",
       " 'built': 'build',\n",
       " 'pursuant': 'pursuant',\n",
       " 'tax': 'tax',\n",
       " '#': '#',\n",
       " '3': '3',\n",
       " 'immediately': 'immediately',\n",
       " 'becomes': 'become',\n",
       " 'affordable': 'affordable',\n",
       " '2': '2',\n",
       " 'Brilliant': 'Brilliant',\n",
       " 'Tried': 'Tried',\n",
       " 'another': 'another',\n",
       " 'post': 'post',\n",
       " 'Having': 'Having',\n",
       " 'three': 'three',\n",
       " 'own': 'own',\n",
       " 'lead': 'lead',\n",
       " 'designer': 'designer',\n",
       " 'big': 'big',\n",
       " 'Foucault': 'Foucault',\n",
       " 'fan': 'fan',\n",
       " 'Feels': 'Feels',\n",
       " 'panopticomments': 'panopticomments',\n",
       " 'suspect': 'suspect',\n",
       " 'go': 'go',\n",
       " 'back': 'back',\n",
       " 'prefer': 'prefer',\n",
       " 'ubiquity': 'ubiquity',\n",
       " 'notifications': 'notification',\n",
       " 'may': 'may',\n",
       " 'inadvertently': 'inadvertently',\n",
       " 'given': 'give',\n",
       " '2.0': '2.0',\n",
       " 'version': 'version',\n",
       " '1': '1',\n",
       " 'Did': 'Did',\n",
       " 'totally': 'totally',\n",
       " 'win': 'win',\n",
       " 'argument': 'argument',\n",
       " 'had': 'have',\n",
       " 'trouble': 'trouble',\n",
       " 'getting': 'get',\n",
       " 'signed': 'sign',\n",
       " 'after': 'after',\n",
       " 'brief': 'brief',\n",
       " 'email': 'email',\n",
       " 'exchange': 'exchange',\n",
       " 'folks': 'folk',\n",
       " 'appears': 'appear',\n",
       " 'If': 'If',\n",
       " 'successful': 'successful',\n",
       " 'goal': 'goal',\n",
       " 'extremely': 'extremely',\n",
       " 'pleased': 'please',\n",
       " 'Are': 'Are',\n",
       " 'taxed': 'tax',\n",
       " 'state': 'state',\n",
       " 'were': 'be',\n",
       " 'bought': 'buy',\n",
       " 'ticket': 'ticket',\n",
       " 'claim': 'claim',\n",
       " 'latter': 'latter',\n",
       " 'suppose': 'suppose',\n",
       " 'theoretical': 'theoretical',\n",
       " 'lucky': 'lucky',\n",
       " 'Oregonian': 'Oregonian',\n",
       " 'decide': 'decide',\n",
       " 'whether': 'whether',\n",
       " 'worth': 'worth',\n",
       " '$': '$',\n",
       " '90': '90',\n",
       " 'm': 'be',\n",
       " 'stuck': 'stick',\n",
       " 'hated': 'hate',\n",
       " 'Californian': 'Californian',\n",
       " 'moniker': 'moniker',\n",
       " 'letter': 'letter',\n",
       " 'campaign': 'campaign',\n",
       " 'livestock': 'livestock',\n",
       " 'Malheur': 'Malheur',\n",
       " 'Refuge': 'Refuge',\n",
       " 'started': 'start',\n",
       " 'late': 'late',\n",
       " '70': '70',\n",
       " 'phoned': 'phone',\n",
       " 'Nancy': 'Nancy',\n",
       " 'Denzel': 'Denzel',\n",
       " 'home': 'home',\n",
       " '1979': '1979',\n",
       " 'along': 'along',\n",
       " 'friends': 'friend',\n",
       " 'hired': 'hire',\n",
       " 'bodyguard': 'bodyguard',\n",
       " 'help': 'help',\n",
       " 'protect': 'protect',\n",
       " 'N&D': 'N&D',\n",
       " 'weekend': 'weekend',\n",
       " 'Hammond': 'Hammond',\n",
       " 'ejection': 'ejection',\n",
       " 'Ferguson': 'Ferguson',\n",
       " 'Diamond': 'Diamond',\n",
       " 'Dance': 'Dance',\n",
       " 'year': 'year',\n",
       " 'before': 'before',\n",
       " 'Jim': 'Jim',\n",
       " 'D.': 'D.',\n",
       " 'For': 'For',\n",
       " 'last': 'last',\n",
       " '10': '10',\n",
       " 'days': 'day',\n",
       " 'lots': 'lot',\n",
       " 'us': 'us',\n",
       " 'wishing': 'wish',\n",
       " 'hear': 'hear',\n",
       " 'witty': 'witty',\n",
       " 'quips': 'quip',\n",
       " 'Bundycon': 'Bundycon',\n",
       " 'occupation': 'occupation',\n",
       " 'refuge': 'refuge',\n",
       " 'hq': 'hq',\n",
       " 'Sacred': 'Sacred',\n",
       " 'Cows': 'Cows',\n",
       " 'trough': 'trough',\n",
       " 'easy': 'easy',\n",
       " 'Amazon': 'Amazon',\n",
       " 'Read': 'Read',\n",
       " 'learn': 'learn',\n",
       " 'western': 'western',\n",
       " 'abuses': 'abuse',\n",
       " 'Mormons': 'Mormon',\n",
       " 'complicated': 'complicate',\n",
       " 'relationship': 'relationship',\n",
       " 'federal': 'federal',\n",
       " 'law': 'law',\n",
       " 'Send': 'Send',\n",
       " 'STAT': 'STAT',\n",
       " 'loosely': 'loosely',\n",
       " 'announced': 'announce',\n",
       " 'tried': 'try',\n",
       " 'demonstration': 'demonstration',\n",
       " 'wondering': 'wonder',\n",
       " 'sign': 'sign',\n",
       " 'profiles': 'profile',\n",
       " 'uses': 'use',\n",
       " 'profile': 'profile',\n",
       " 'use': 'use',\n",
       " 'partner': 'partner',\n",
       " 'My': 'My',\n",
       " 'smoother': 'smooth',\n",
       " 'many': 'many',\n",
       " 'sites': 'site',\n",
       " 'overtaken': 'overtake',\n",
       " 'first': '\\ufeff1',\n",
       " 'its': 'its',\n",
       " 'intent': 'intent',\n",
       " 'town': 'town',\n",
       " 'consider': 'consider',\n",
       " 'engagement': 'engagement',\n",
       " 'rather': 'rather',\n",
       " 'driven': 'drive',\n",
       " 'click': 'click',\n",
       " 'driver': 'driver',\n",
       " 'intended': 'intend',\n",
       " 'boost': 'boost',\n",
       " 'ad': 'ad',\n",
       " 'rates': 'rate',\n",
       " 'progress': 'progress',\n",
       " 'helping': 'help',\n",
       " 'test': 'test',\n",
       " 'software': 'software',\n",
       " 'David': 'David',\n",
       " 'To': 'To',\n",
       " 'arching': 'arch',\n",
       " 'account': 'account',\n",
       " 'manage': 'manage',\n",
       " 'different': 'different',\n",
       " 'Of': 'Of',\n",
       " 'course': 'course',\n",
       " 'information': 'information',\n",
       " 'across': 'across',\n",
       " 'decided': 'decide',\n",
       " 'best': 'well',\n",
       " 'present': 'present',\n",
       " 'yourself': 'yourself',\n",
       " 'differently': 'differently',\n",
       " 'might': 'may',\n",
       " 'example': 'example',\n",
       " 'portland_hipster15': 'portland_hipster15',\n",
       " 'Willamette': 'Willamette',\n",
       " 'Week': 'Week',\n",
       " 'MrWhiskers': 'MrWhiskers',\n",
       " 'Cat': 'Cat',\n",
       " 'Enthusiast': 'Enthusiast',\n",
       " 'Digest': 'Digest',\n",
       " 'Enterprise': 'Enterprise',\n",
       " 'licenses': 'license',\n",
       " 'available': 'available',\n",
       " 'tiered': 'tier',\n",
       " 'business': 'business',\n",
       " 'plans': 'plan',\n",
       " 'very': 'very',\n",
       " 'certainly': 'certainly',\n",
       " 'journalism': 'journalism',\n",
       " 'stretch': 'stretch',\n",
       " 'imagination': 'imagination',\n",
       " 'words': 'word',\n",
       " 'metric': 'metric',\n",
       " 'chooses': 'choose',\n",
       " ...}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding matrix ...\n",
      "loading embedding file\n"
     ]
    },
    {
     "ename": "UnpicklingError",
     "evalue": "invalid load key, ','.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-08b625a0f5b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading embedding matrix ...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m \u001b[0membedding_matrix_glove\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_glove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlemma_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;31m#embedding_matrix_fasttext, nb_words = load_fasttext(word_dict, lemma_dict)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;31m#embedding_matrix = np.concatenate((embedding_matrix_glove, embedding_matrix_fasttext), axis=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-08b625a0f5b9>\u001b[0m in \u001b[0;36mload_glove\u001b[0;34m(word_dict, lemma_dict)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading embedding file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0membeddings_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_coefs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEMBEDDING_FILE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--- %s seconds ---\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0membed_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1328\u001b[0m         \"\"\"\n\u001b[1;32m   1329\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1330\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1332\u001b[0m             \u001b[0;31m# for backward compatibility for `max_final_vocab` feature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m         \"\"\"\n\u001b[0;32m-> 1244\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseWordEmbeddingsModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1245\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ns_exponent'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1246\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mns_exponent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.75\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname_or_handle, **kwargs)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m         \"\"\"\n\u001b[0;32m--> 603\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseAny2VecModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname, mmap)\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSaveLoad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adapt_by_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m         \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_specials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loaded %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36munpickle\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m   1382\u001b[0m         \u001b[0;31m# Because of loading from S3 load can't be used (missing readline in smart_open)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnpicklingError\u001b[0m: invalid load key, ','."
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "def load_glove(word_dict, lemma_dict):\n",
    "    EMBEDDING_FILE = '../../quora/input/embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    \n",
    "    print(\"loading embedding file\")\n",
    "    start_time = time.time()\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in Word2Vec.load(EMBEDDING_FILE))\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    embed_size = 300\n",
    "    nb_words = len(word_dict)+1\n",
    "    embedding_matrix = np.zeros((nb_words, embed_size), dtype=np.float32)\n",
    "    unknown_vector = np.zeros((embed_size,), dtype=np.float32) - 1.\n",
    "    print(unknown_vector[:5])\n",
    "    for key in tqdm(word_dict):\n",
    "        word = key\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = key.lower()\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = key.upper()\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = key.capitalize()\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = ps.stem(key)\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = lc.stem(key)\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = sb.stem(key)\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = lemma_dict[key]\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        if len(key) > 1:\n",
    "            #word = correction(key)\n",
    "            word = key\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[word_dict[key]] = embedding_vector\n",
    "                continue\n",
    "        embedding_matrix[word_dict[key]] = unknown_vector                    \n",
    "    return embedding_matrix, nb_words \n",
    "\n",
    "\n",
    "# takes 81 seconds with open\n",
    "start_time = time.time()\n",
    "print(\"Loading embedding matrix ...\")\n",
    "embedding_matrix_glove, nb_words = load_glove(word_dict, lemma_dict)\n",
    "#embedding_matrix_fasttext, nb_words = load_fasttext(word_dict, lemma_dict)\n",
    "#embedding_matrix = np.concatenate((embedding_matrix_glove, embedding_matrix_fasttext), axis=1)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.0007839202880859375 seconds ---\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: ','",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-34ebc9d41ef0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0membedding_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mload_word2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../../quora/input/embeddings/glove.840B.300d/glove.840B.300d.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-34ebc9d41ef0>\u001b[0m in \u001b[0;36mload_word2vec\u001b[0;34m(fname, encoding, unicode_errors, datatype, max_vocab, word_index)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--- %s seconds ---\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mbinary_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitemsize\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-34ebc9d41ef0>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--- %s seconds ---\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mbinary_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitemsize\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: ','"
     ]
    }
   ],
   "source": [
    "from gensim import utils\n",
    "\n",
    "def load_word2vec(fname, encoding='utf8', unicode_errors='strict',datatype=np.float32, max_vocab=3000000, word_index=None):\n",
    "    #emb_mean,emb_std = -0.0051106834, 0.18445626\n",
    "    #embedding_matrix = np.random.normal(emb_mean, emb_std, (max_features, 300))\n",
    "    embedding_index = {}\n",
    "    start_time = time.time()\n",
    "    with utils.smart_open(fname) as fin:\n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "        header = utils.to_unicode(fin.readline(), encoding=encoding)\n",
    "        vocab_size, vector_size = (int(x) for x in header.split())\n",
    "        binary_len = np.dtype(datatype).itemsize * vector_size\n",
    "        \n",
    "        for _ in tqdm(range(min(vocab_size,max_vocab))):\n",
    "            # mixed text and binary: read text first, then binary\n",
    "            word = []\n",
    "            while True:\n",
    "                ch = fin.read(1)\n",
    "                if ch == b' ':\n",
    "                    break\n",
    "                if ch == b'':\n",
    "                    raise EOFError(\"unexpected end of input\")\n",
    "                if ch != b'\\n':\n",
    "                    word.append(ch)\n",
    "            word = utils.to_unicode(b''.join(word), encoding=encoding, errors=unicode_errors)\n",
    "            weights = np.fromstring(fin.read(binary_len), dtype=datatype).astype(np.float16)\n",
    "            embedding_index[word] = weights\n",
    "    return embedding_index\n",
    "start_time = time.time()\n",
    "load_word2vec('../../quora/input/embeddings/glove.840B.300d/glove.840B.300d.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asd\n"
     ]
    }
   ],
   "source": [
    "print(\"asd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
