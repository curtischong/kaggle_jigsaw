{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "import random\n",
    "from tqdm._tqdm_notebook import tqdm_notebook as tqdm\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "lc = LancasterStemmer()\n",
    "from nltk.stem import SnowballStemmer\n",
    "sb = SnowballStemmer(\"english\")\n",
    "from keras.preprocessing import text, sequence\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from torch.nn import functional as F\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import spacy\n",
    "import gensim\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import apex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable progress bars when submitting\n",
    "def is_interactive():\n",
    "   return 'SHLVL' not in os.environ\n",
    "\n",
    "if not is_interactive():\n",
    "    def nop(it, *a, **k):\n",
    "        return it\n",
    "\n",
    "    tqdm = nop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CRAWL_EMBEDDING_PATH = '../input/crawl-300d-2M.vec'\n",
    "GLOVE_EMBEDDING_PATH = '../input/glove.840B.300d.txt'\n",
    "NUM_MODELS = 2\n",
    "LSTM_UNITS = 128\n",
    "DENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\n",
    "MAX_LEN = 220"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef build_matrix(word_dict, lemma_dict, path):\\n    embed_size = 300\\n    embeddings_index = load_embeddings(path)\\n    embedding_matrix = np.zeros((len(word_dict) + 1, embed_size), dtype=np.float32)\\n    unknown_words = []\\n    unknown_vector = np.zeros((embed_size,), dtype=np.float32) - 1\\n    \\n    for key in tqdm(word_dict):\\n        word = key\\n        embedding_vector = embeddings_index.get(word)\\n        if embedding_vector is not None:\\n            embedding_matrix[word_dict[key]] = embedding_vector\\n            continue\\n        word = key.lower()\\n        embedding_vector = embeddings_index.get(word)\\n        if embedding_vector is not None:\\n            embedding_matrix[word_dict[key]] = embedding_vector\\n            continue\\n        word = key.upper()\\n        embedding_vector = embeddings_index.get(word)\\n        if embedding_vector is not None:\\n            embedding_matrix[word_dict[key]] = embedding_vector\\n            continue\\n        word = key.capitalize()\\n        embedding_vector = embeddings_index.get(word)\\n        if embedding_vector is not None:\\n            embedding_matrix[word_dict[key]] = embedding_vector\\n            continue\\n        word = ps.stem(key)\\n        embedding_vector = embeddings_index.get(word)\\n        if embedding_vector is not None:\\n            embedding_matrix[word_dict[key]] = embedding_vector\\n            continue\\n        word = lc.stem(key)\\n        embedding_vector = embeddings_index.get(word)\\n        if embedding_vector is not None:\\n            embedding_matrix[word_dict[key]] = embedding_vector\\n            continue\\n        word = sb.stem(key)\\n        embedding_vector = embeddings_index.get(word)\\n        if embedding_vector is not None:\\n            embedding_matrix[word_dict[key]] = embedding_vector\\n            continue\\n        word = lemma_dict[key]\\n        embedding_vector = embeddings_index.get(word)\\n        if embedding_vector is not None:\\n            embedding_matrix[word_dict[key]] = embedding_vector\\n            continue\\n        if len(key) > 1:\\n            word = correction(key)\\n            embedding_vector = embeddings_index.get(word)\\n            if embedding_vector is not None:\\n                embedding_matrix[word_dict[key]] = embedding_vector\\n                continue\\n        \\n        #Unknown word, does not exist in dictionary\\n        embedding_matrix[word_dict[key]] = unknown_vector\\n        unknown_words.append(word)\\n    return embedding_matrix, unknown_words\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "def load_embeddings(path):\n",
    "    with open(path) as f:\n",
    "        return dict(get_coefs(*line.strip().split(' ')) for line in tqdm(f))\n",
    "    \n",
    "def build_matrix(word_index, path):\n",
    "    embedding_index = load_embeddings(path)\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "    unknown_words = []\n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        try:\n",
    "            embedding_matrix[i] = embedding_index[word]\n",
    "        except KeyError:\n",
    "            unknown_words.append(word)\n",
    "    return embedding_matrix, unknown_words\n",
    "\n",
    "'''\n",
    "def build_matrix(word_dict, lemma_dict, path):\n",
    "    embed_size = 300\n",
    "    embeddings_index = load_embeddings(path)\n",
    "    embedding_matrix = np.zeros((len(word_dict) + 1, embed_size), dtype=np.float32)\n",
    "    unknown_words = []\n",
    "    unknown_vector = np.zeros((embed_size,), dtype=np.float32) - 1\n",
    "    \n",
    "    for key in tqdm(word_dict):\n",
    "        word = key\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = key.lower()\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = key.upper()\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = key.capitalize()\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = ps.stem(key)\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = lc.stem(key)\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = sb.stem(key)\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = lemma_dict[key]\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        if len(key) > 1:\n",
    "            word = correction(key)\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[word_dict[key]] = embedding_vector\n",
    "                continue\n",
    "        \n",
    "        #Unknown word, does not exist in dictionary\n",
    "        embedding_matrix[word_dict[key]] = unknown_vector\n",
    "        unknown_words.append(word)\n",
    "    return embedding_matrix, unknown_words\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceBucketCollator():\n",
    "    def __init__(self, choose_length, sequence_index, length_index, label_index=None):\n",
    "        self.choose_length = choose_length\n",
    "        self.sequence_index = sequence_index\n",
    "        self.length_index = length_index\n",
    "        self.label_index = label_index\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        batch = [torch.stack(x) for x in list(zip(*batch))]\n",
    "        \n",
    "        sequences = batch[self.sequence_index]\n",
    "        lengths = batch[self.length_index]\n",
    "        \n",
    "        length = self.choose_length(lengths)\n",
    "        mask = torch.arange(start=maxlen, end=0, step=-1) < length\n",
    "        padded_sequences = sequences[:, mask]\n",
    "        \n",
    "        batch[self.sequence_index] = padded_sequences\n",
    "        \n",
    "        if self.label_index is not None:\n",
    "            return [x for i, x in enumerate(batch) if i != self.label_index], batch[self.label_index]\n",
    "    \n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "filepath = './model_files/checkpoint.pth'\n",
    "\n",
    "def train_model(model, train, test, loss_fn, output_dim, lr=0.001,\n",
    "                batch_size=512, n_epochs=6, n_epochs_embed=2,\n",
    "                enable_checkpoint_ensemble=True):\n",
    "    \n",
    "    train_collator = SequenceBucketCollator(lambda lengths: lengths.max(), \n",
    "                                            sequence_index=0, \n",
    "                                            length_index=1, \n",
    "                                            label_index=2)\n",
    "    \n",
    "    param_lrs = [{'params': param, 'lr': lr} for param in model.parameters()]\n",
    "    optimizer = torch.optim.Adam(param_lrs, lr=lr)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda epoch: 0.6 ** epoch)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True, collate_fn=train_collator)\n",
    "    val_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False, collate_fn=train_collator)\n",
    "    all_test_preds = []\n",
    "    checkpoint_weights = [2 ** epoch for epoch in range(n_epochs)]\n",
    "    \n",
    "    best_loss = 1\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        model.train() #set model to train mode\n",
    "        avg_loss = 0.\n",
    "        \n",
    "        for data in tqdm(train_loader, disable=False):\n",
    "            \n",
    "            #training loop\n",
    "            x_batch = data[:-1]\n",
    "            #print(\"First: \", x_batch[0][0])\n",
    "            #print(\"Second: \", x_batch[0][1])\n",
    "            first = x_batch[0][0]\n",
    "            second = x_batch[0][1]\n",
    "            \n",
    "            y_batch = data[-1]\n",
    "\n",
    "            y_pred = model(first, second)  #feed data into model          \n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            \n",
    "            #calculate error and adjust model params\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item() / len(train_loader)\n",
    "        \n",
    "        #Check if loss is better than current best loss, if so, save the model\n",
    "        is_best = (avg_loss < best_loss)\n",
    "        \n",
    "        if is_best:\n",
    "            print (\"=> Saving a new best\")\n",
    "            best_loss = avg_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_loss': best_loss\n",
    "            }, filepath)  # save checkpoint\n",
    "        else:\n",
    "            print (\"=> Model Accuracy did not improve\")\n",
    "            \n",
    "        \n",
    "        model.eval() #set model to eval mode for test data\n",
    "        test_preds = np.zeros((len(test), output_dim))\n",
    "        \n",
    "    \n",
    "        for i, x_batch in enumerate(val_loader):\n",
    "            #print(\"X_Batch: \", x_batch)\n",
    "            data_param = x_batch[0][0]\n",
    "            lengths_param = x_batch[0][1]\n",
    "            y_pred = sigmoid(model(data_param, lengths_param).detach().cpu().numpy()) #feed data into model\n",
    "\n",
    "            test_preds[i * batch_size:(i+1) * batch_size, :] = y_pred #get test predictions\n",
    "        \n",
    "        #test_preds has the predictions for the entire test set now\n",
    "        all_test_preds.append(test_preds) #append predictions to the record of all past predictions\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print('Epoch {}/{} \\t loss={:.4f} \\t time={:.2f}s'.format(\n",
    "              epoch + 1, n_epochs, avg_loss, elapsed_time))\n",
    "        \n",
    "    #Make embeddings layer only layer unfreezed, train again (literally run through the n_epochs)\n",
    "    #maybe define a n_epochs_embedding\n",
    "    \n",
    "    #parameters = model.parameters()\n",
    "    #for param in parameters:\n",
    "    #        param.requires_grad = False\n",
    "    #parameters[0].requires_grad = True\n",
    "    \n",
    "    '''\n",
    "    ct = 0\n",
    "    for child in model.children():\n",
    "        if ct == 0:\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = True\n",
    "        else:\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = False\n",
    "        ct += 1\n",
    "    \n",
    "    for epoch in range(n_epochs_embed):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        model.train() #set model to train mode\n",
    "        avg_loss = 0.\n",
    "        \n",
    "        for data in tqdm(train_loader, disable=False):\n",
    "            \n",
    "            #training loop\n",
    "            x_batch = data[:-1]\n",
    "            y_batch = data[-1]\n",
    "\n",
    "            y_pred = model(*x_batch)  #feed data into model          \n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            \n",
    "            #calculate error and adjust model params\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item() / len(train_loader) #gets the loss per epoch\n",
    "        \n",
    "            \n",
    "        model.eval() #set model to eval mode for test data\n",
    "        test_preds = np.zeros((len(test), output_dim))\n",
    "    \n",
    "        for i, x_batch in enumerate(test_loader):\n",
    "            y_pred = sigmoid(model(*x_batch).detach().cpu().numpy()) #feed data into model\n",
    "\n",
    "            test_preds[i * batch_size:(i+1) * batch_size, :] = y_pred #get test predictions\n",
    "        \n",
    "        #test_preds has the predictions for the entire test set now\n",
    "        #all_test_preds.append(test_preds) #append predictions to the record of all past predictions\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print('[EMBEDDING TRAINING] Epoch {}/{} \\t loss={:.4f} \\t time={:.2f}s '.format(\n",
    "              epoch + 1, n_epochs_embed, avg_loss, elapsed_time))\n",
    "    '''\n",
    "    \n",
    "    #PREDICTION CODE\n",
    "    '''\n",
    "    if enable_checkpoint_ensemble:\n",
    "        #if our approach is an ensemble then we average it amongst all the historical predictions\n",
    "        test_preds = np.average(all_test_preds, weights=checkpoint_weights, axis=0)    \n",
    "    else:\n",
    "        #if our approach is not an ensemble then we just take the last set of predictions\n",
    "        test_preds = all_test_preds[-1]\n",
    "        \n",
    "    return test_preds\n",
    "    '''\n",
    "    \n",
    "    #return trained model\n",
    "    return model\n",
    "\n",
    "def predict(model, test, output_dim, batch_size=512, pred_type=\"val\"):\n",
    "    checkpoint = torch.load(filepath)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    #optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    if pred_type == \"test\":\n",
    "        test_collator = SequenceBucketCollator(lambda lengths: lengths.max(), sequence_index=0, length_index=1)\n",
    "        test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False, collate_fn=test_collator)\n",
    "\n",
    "        model.eval() #set model to eval mode for test data\n",
    "        test_preds = np.zeros((len(test), output_dim))\n",
    "\n",
    "        for i, x_batch in enumerate(test_loader):\n",
    "            #print(x_batch[0])\n",
    "            data_param = x_batch[0]\n",
    "            lengths_param = x_batch[1]\n",
    "            y_pred = sigmoid(model(data_param, lengths_param).detach().cpu().numpy()) #feed data into model\n",
    "            test_preds[i * batch_size:(i+1) * batch_size, :] = y_pred #get test predictions\n",
    "\n",
    "        return test_preds\n",
    "    else:\n",
    "        test_collator = SequenceBucketCollator(lambda lengths: lengths.max(), sequence_index=0, length_index=1, label_index=2)\n",
    "        test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False, collate_fn=test_collator)\n",
    "\n",
    "        model.eval() #set model to eval mode for test data\n",
    "        test_preds = np.zeros((len(test), output_dim))\n",
    "\n",
    "        for i, x_batch in enumerate(test_loader):\n",
    "            #print(x_batch)\n",
    "            data_param = x_batch[0][0]\n",
    "            lengths_param = x_batch[0][1]\n",
    "            y_pred = sigmoid(model(data_param, lengths_param).detach().cpu().numpy()) #feed data into model\n",
    "            test_preds[i * batch_size:(i+1) * batch_size, :] = y_pred #get test predictions\n",
    "\n",
    "        return test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialDropout(nn.Dropout2d):\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(2)    # (N, T, 1, K)\n",
    "        x = x.permute(0, 3, 2, 1)  # (N, K, 1, T)\n",
    "        #call the forward method in Dropout2d (super function specifies the subclass and instance)\n",
    "        x = super(SpatialDropout, self).forward(x)  # (N, K, 1, T), some features are masked\n",
    "        x = x.permute(0, 3, 2, 1)  # (N, T, 1, K)\n",
    "        x = x.squeeze(2)  # (N, T, K)\n",
    "        return x\n",
    "    \n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, embedding_matrix, num_aux_targets):\n",
    "        #call the init mthod in Module (super function specifies the subclass and instance)\n",
    "        super(NeuralNet, self).__init__() \n",
    "        embed_size = embedding_matrix.shape[1]\n",
    "        \n",
    "        self.embedding = nn.Embedding(max_features, embed_size)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.embedding_dropout = SpatialDropout(0.3)\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(embed_size, LSTM_UNITS, bidirectional=True, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(LSTM_UNITS * 2, LSTM_UNITS, bidirectional=True, batch_first=True)\n",
    "    \n",
    "        self.linear1 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n",
    "        self.linear2 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n",
    "        \n",
    "        self.linear_out = nn.Linear(DENSE_HIDDEN_UNITS, 1)\n",
    "        self.linear_aux_out = nn.Linear(DENSE_HIDDEN_UNITS, num_aux_targets)\n",
    "        \n",
    "    def forward(self, x, lengths=None):\n",
    "        h_embedding = self.embedding(x)\n",
    "        h_embedding = self.embedding_dropout(h_embedding)\n",
    "        \n",
    "        #first variable h_(lstm #) holds the output, _ is the (hidden state, cell state)\n",
    "        h_lstm1, _ = self.lstm1(h_embedding) \n",
    "        h_lstm2, _ = self.lstm2(h_lstm1)\n",
    "        \n",
    "        # global average pooling\n",
    "        avg_pool = torch.mean(h_lstm2, 1) #get the mean value of the first dimension in h_lstm2\n",
    "        # global max pooling\n",
    "        max_pool, _ = torch.max(h_lstm2, 1) #get the max value of the first dimension in h_lstm2\n",
    "        \n",
    "        h_conc = torch.cat((max_pool, avg_pool), 1)\n",
    "        h_conc_linear1  = F.relu(self.linear1(h_conc))\n",
    "        h_conc_linear2  = F.relu(self.linear2(h_conc))\n",
    "        \n",
    "        hidden = h_conc + h_conc_linear1 + h_conc_linear2\n",
    "        \n",
    "        result = self.linear_out(hidden)\n",
    "        aux_result = self.linear_aux_out(hidden)\n",
    "        out = torch.cat([result, aux_result], 1)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def preprocess(data):\n",
    "\n",
    "    punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~`\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n",
    "    def clean_special_chars(text, punct):\n",
    "        for p in punct:\n",
    "            text = text.replace(p, ' ')\n",
    "        return text\n",
    "\n",
    "    data = data.astype(str).apply(lambda x: clean_special_chars(x, punct))\n",
    "    return data\n",
    "'''\n",
    "symbols_to_isolate = '.,?!-;*\"…:—()%#$&_/@＼・ω+=”“[]^–>\\\\°<~•≠™ˈʊɒ∞§{}·τα❤☺ɡ|¢→̶`❥━┣┫┗Ｏ►★©―ɪ✔®\\x96\\x92●£♥➤´¹☕≈÷♡◐║▬′ɔː€۩۞†μ✒➥═☆ˌ◄½ʻπδηλσερνʃ✬ＳＵＰＥＲＩＴ☻±♍µº¾✓◾؟．⬅℅»Вав❣⋅¿¬♫ＣＭβ█▓▒░⇒⭐›¡₂₃❧▰▔◞▀▂▃▄▅▆▇↙γ̄″☹➡«φ⅓„✋：¥̲̅́∙‛◇✏▷❓❗¶˚˙）сиʿ✨。ɑ\\x80◕！％¯−ﬂﬁ₁²ʌ¼⁴⁄₄⌠♭✘╪▶☭✭♪☔☠♂☃☎✈✌✰❆☙○‣⚓年∎ℒ▪▙☏⅛ｃａｓǀ℮¸ｗ‚∼‖ℳ❄←☼⋆ʒ⊂、⅔¨͡๏⚾⚽Φ×θ￦？（℃⏩☮⚠月✊❌⭕▸■⇌☐☑⚡☄ǫ╭∩╮，例＞ʕɐ̣Δ₀✞┈╱╲▏▕┃╰▊▋╯┳┊≥☒↑☝ɹ✅☛♩☞ＡＪＢ◔◡↓♀⬆̱ℏ\\x91⠀ˤ╚↺⇤∏✾◦♬³の｜／∵∴√Ω¤☜▲↳▫‿⬇✧ｏｖｍ－２０８＇‰≤∕ˆ⚜☁'\n",
    "symbols_to_delete = '\\n🍕\\r🐵😑\\xa0\\ue014\\t\\uf818\\uf04a\\xad😢🐶️\\uf0e0😜😎👊\\u200b\\u200e😁عدويهصقأناخلىبمغر😍💖💵Е👎😀😂\\u202a\\u202c🔥😄🏻💥ᴍʏʀᴇɴᴅᴏᴀᴋʜᴜʟᴛᴄᴘʙғᴊᴡɢ😋👏שלוםבי😱‼\\x81エンジ故障\\u2009🚌ᴵ͞🌟😊😳😧🙀😐😕\\u200f👍😮😃😘אעכח💩💯⛽🚄🏼ஜ😖ᴠ🚲‐😟😈💪🙏🎯🌹😇💔😡\\x7f👌ἐὶήιὲκἀίῃἴξ🙄Ｈ😠\\ufeff\\u2028😉😤⛺🙂\\u3000تحكسة👮💙فزط😏🍾🎉😞\\u2008🏾😅😭👻😥😔😓🏽🎆🍻🍽🎶🌺🤔😪\\x08‑🐰🐇🐱🙆😨🙃💕𝘊𝘦𝘳𝘢𝘵𝘰𝘤𝘺𝘴𝘪𝘧𝘮𝘣💗💚地獄谷улкнПоАН🐾🐕😆ה🔗🚽歌舞伎🙈😴🏿🤗🇺🇸мυтѕ⤵🏆🎃😩\\u200a🌠🐟💫💰💎эпрд\\x95🖐🙅⛲🍰🤐👆🙌\\u2002💛🙁👀🙊🙉\\u2004ˢᵒʳʸᴼᴷᴺʷᵗʰᵉᵘ\\x13🚬🤓\\ue602😵άοόςέὸתמדףנרךצט😒͝🆕👅👥👄🔄🔤👉👤👶👲🔛🎓\\uf0b7\\uf04c\\x9f\\x10成都😣⏺😌🤑🌏😯ех😲Ἰᾶὁ💞🚓🔔📚🏀👐\\u202d💤🍇\\ue613小土豆🏡❔⁉\\u202f👠》कर्मा🇹🇼🌸蔡英文🌞🎲レクサス😛外国人关系Сб💋💀🎄💜🤢َِьыгя不是\\x9c\\x9d🗑\\u2005💃📣👿༼つ༽😰ḷЗз▱ц￼🤣卖温哥华议会下降你失去所有的钱加拿大坏税骗子🐝ツ🎅\\x85🍺آإشء🎵🌎͟ἔ油别克🤡🤥😬🤧й\\u2003🚀🤴ʲшчИОРФДЯМюж😝🖑ὐύύ特殊作戦群щ💨圆明园קℐ🏈😺🌍⏏ệ🍔🐮🍁🍆🍑🌮🌯🤦\\u200d𝓒𝓲𝓿𝓵안영하세요ЖљКћ🍀😫🤤ῦ我出生在了可以说普通话汉语好极🎼🕺🍸🥂🗽🎇🎊🆘🤠👩🖒🚪天一家⚲\\u2006⚭⚆⬭⬯⏖新✀╌🇫🇷🇩🇪🇮🇬🇧😷🇨🇦ХШ🌐\\x1f杀鸡给猴看ʁ𝗪𝗵𝗲𝗻𝘆𝗼𝘂𝗿𝗮𝗹𝗶𝘇𝗯𝘁𝗰𝘀𝘅𝗽𝘄𝗱📺ϖ\\u2000үսᴦᎥһͺ\\u2007հ\\u2001ɩｙｅ൦ｌƽｈ𝐓𝐡𝐞𝐫𝐮𝐝𝐚𝐃𝐜𝐩𝐭𝐢𝐨𝐧Ƅᴨןᑯ໐ΤᏧ௦Іᴑ܁𝐬𝐰𝐲𝐛𝐦𝐯𝐑𝐙𝐣𝐇𝐂𝐘𝟎ԜТᗞ౦〔Ꭻ𝐳𝐔𝐱𝟔𝟓𝐅🐋ﬃ💘💓ё𝘥𝘯𝘶💐🌋🌄🌅𝙬𝙖𝙨𝙤𝙣𝙡𝙮𝙘𝙠𝙚𝙙𝙜𝙧𝙥𝙩𝙪𝙗𝙞𝙝𝙛👺🐷ℋ𝐀𝐥𝐪🚶𝙢Ἱ🤘ͦ💸ج패티Ｗ𝙇ᵻ👂👃ɜ🎫\\uf0a7БУі🚢🚂ગુજરાતીῆ🏃𝓬𝓻𝓴𝓮𝓽𝓼☘﴾̯﴿₽\\ue807𝑻𝒆𝒍𝒕𝒉𝒓𝒖𝒂𝒏𝒅𝒔𝒎𝒗𝒊👽😙\\u200cЛ‒🎾👹⎌🏒⛸公寓养宠物吗🏄🐀🚑🤷操美𝒑𝒚𝒐𝑴🤙🐒欢迎来到阿拉斯ספ𝙫🐈𝒌𝙊𝙭𝙆𝙋𝙍𝘼𝙅ﷻ🦄巨收赢得白鬼愤怒要买额ẽ🚗🐳𝟏𝐟𝟖𝟑𝟕𝒄𝟗𝐠𝙄𝙃👇锟斤拷𝗢𝟳𝟱𝟬⦁マルハニチロ株式社⛷한국어ㄸㅓ니͜ʖ𝘿𝙔₵𝒩ℯ𝒾𝓁𝒶𝓉𝓇𝓊𝓃𝓈𝓅ℴ𝒻𝒽𝓀𝓌𝒸𝓎𝙏ζ𝙟𝘃𝗺𝟮𝟭𝟯𝟲👋🦊多伦🐽🎻🎹⛓🏹🍷🦆为和中友谊祝贺与其想象对法如直接问用自己猜本传教士没积唯认识基督徒曾经让相信耶稣复活死怪他但当们聊些政治题时候战胜因圣把全堂结婚孩恐惧且栗谓这样还♾🎸🤕🤒⛑🎁批判检讨🏝🦁🙋😶쥐스탱트뤼도석유가격인상이경제황을렵게만들지않록잘관리해야합다캐나에서대마초와화약금의품런성분갈때는반드시허된사용🔫👁凸ὰ💲🗯𝙈Ἄ𝒇𝒈𝒘𝒃𝑬𝑶𝕾𝖙𝖗𝖆𝖎𝖌𝖍𝖕𝖊𝖔𝖑𝖉𝖓𝖐𝖜𝖞𝖚𝖇𝕿𝖘𝖄𝖛𝖒𝖋𝖂𝕴𝖟𝖈𝕸👑🚿💡知彼百\\uf005𝙀𝒛𝑲𝑳𝑾𝒋𝟒😦𝙒𝘾𝘽🏐𝘩𝘨ὼṑ𝑱𝑹𝑫𝑵𝑪🇰🇵👾ᓇᒧᔭᐃᐧᐦᑳᐨᓃᓂᑲᐸᑭᑎᓀᐣ🐄🎈🔨🐎🤞🐸💟🎰🌝🛳点击查版🍭𝑥𝑦𝑧ＮＧ👣\\uf020っ🏉ф💭🎥Ξ🐴👨🤳🦍\\x0b🍩𝑯𝒒😗𝟐🏂👳🍗🕉🐲چی𝑮𝗕𝗴🍒ꜥⲣⲏ🐑⏰鉄リ事件ї💊「」\\uf203\\uf09a\\uf222\\ue608\\uf202\\uf099\\uf469\\ue607\\uf410\\ue600燻製シ虚偽屁理屈Г𝑩𝑰𝒀𝑺🌤𝗳𝗜𝗙𝗦𝗧🍊ὺἈἡχῖΛ⤏🇳𝒙ψՁմեռայինրւդձ冬至ὀ𝒁🔹🤚🍎𝑷🐂💅𝘬𝘱𝘸𝘷𝘐𝘭𝘓𝘖𝘹𝘲𝘫کΒώ💢ΜΟΝΑΕ🇱♲𝝈↴💒⊘Ȼ🚴🖕🖤🥘📍👈➕🚫🎨🌑🐻𝐎𝐍𝐊𝑭🤖🎎😼🕷ｇｒｎｔｉｄｕｆｂｋ𝟰🇴🇭🇻🇲𝗞𝗭𝗘𝗤👼📉🍟🍦🌈🔭《🐊🐍\\uf10aლڡ🐦\\U0001f92f\\U0001f92a🐡💳ἱ🙇𝗸𝗟𝗠𝗷🥜さようなら🔼'\n",
    "\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "nltk_tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "\n",
    "isolate_dict = {ord(c):f' {c} ' for c in symbols_to_isolate}\n",
    "remove_dict = {ord(c):f'' for c in symbols_to_delete}\n",
    "\n",
    "\n",
    "def handle_punctuation(x):\n",
    "    x = x.translate(remove_dict)\n",
    "    x = x.translate(isolate_dict)\n",
    "    return x\n",
    "\n",
    "def handle_contractions(x):\n",
    "    x = nltk_tokenizer.tokenize(x)\n",
    "    return x\n",
    "\n",
    "def fix_quote(x):\n",
    "    x = [x_[1:] if x_.startswith(\"'\") else x_ for x_ in x]\n",
    "    x = ' '.join(x)\n",
    "    return x\n",
    "\n",
    "def preprocess(x):\n",
    "    x = handle_punctuation(x)\n",
    "    x = handle_contractions(x)\n",
    "    x = fix_quote(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 2.178173303604126 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/curtis/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:20: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "train = pd.read_hdf('../input/train.h5')\n",
    "test = pd.read_hdf('../input/test.h5')\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "#pd.read_csv(\"P00000001-ALL.csv\", nrows=20)\n",
    "#train = pd.read_hdf('../input/train.h5')\n",
    "#test = pd.read_hdf('../input/test.h5')\n",
    "#tqdm.pandas()\n",
    "\n",
    "identity_columns = [\n",
    "    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n",
    "    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n",
    "\n",
    "x_train = train['comment_text'].apply(lambda x:preprocess(x))\n",
    "y_train = np.where(train['target'] >= 0.5, 1, 0)\n",
    "num_train_data = y_train.shape[0]\n",
    "y_train_identity = np.where(train[identity_columns] >= 0.5, 1, 0)\n",
    "y_aux_train = train[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']]\n",
    "x_test = test['comment_text'].apply(lambda x:preprocess(x))\n",
    "y_aux_train = y_aux_train.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                       int64\n",
       "target                                 float64\n",
       "comment_text                            object\n",
       "severe_toxicity                        float64\n",
       "obscene                                float64\n",
       "identity_attack                        float64\n",
       "insult                                 float64\n",
       "threat                                 float64\n",
       "asian                                  float64\n",
       "atheist                                float64\n",
       "bisexual                               float64\n",
       "black                                  float64\n",
       "buddhist                               float64\n",
       "christian                              float64\n",
       "female                                 float64\n",
       "heterosexual                           float64\n",
       "hindu                                  float64\n",
       "homosexual_gay_or_lesbian              float64\n",
       "intellectual_or_learning_disability    float64\n",
       "jewish                                 float64\n",
       "latino                                 float64\n",
       "male                                   float64\n",
       "muslim                                 float64\n",
       "other_disability                       float64\n",
       "other_gender                           float64\n",
       "other_race_or_ethnicity                float64\n",
       "other_religion                         float64\n",
       "other_sexual_orientation               float64\n",
       "physical_disability                    float64\n",
       "psychiatric_or_mental_illness          float64\n",
       "transgender                            float64\n",
       "white                                  float64\n",
       "created_date                            object\n",
       "publication_id                           int64\n",
       "parent_id                              float64\n",
       "article_id                               int64\n",
       "rating                                  object\n",
       "funny                                    int64\n",
       "wow                                      int64\n",
       "sad                                      int64\n",
       "likes                                    int64\n",
       "disagree                                 int64\n",
       "sexual_explicit                        float64\n",
       "identity_annotator_count                 int64\n",
       "toxicity_annotator_count                 int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmax_features = None\\n\\n#Create the dictionary of all words that exist in our data\\nnlp = spacy.load(\"en_core_web_lg\", disable=[\\'parser\\',\\'ner\\',\\'tagger\\'])\\ntext_list = pd.concat([x_train, x_test])\\nnlp.vocab.add_flag(lambda s: s.lower() in spacy.lang.en.stop_words.STOP_WORDS, spacy.attrs.IS_STOP)\\nword_dict = {}\\nlemma_dict = {}\\nword_index = 1\\ndocs = nlp.pipe(text_list, n_threads = 2)\\nword_sequences = []\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "max_features = None\n",
    "\n",
    "#Create the dictionary of all words that exist in our data\n",
    "nlp = spacy.load(\"en_core_web_lg\", disable=['parser','ner','tagger'])\n",
    "text_list = pd.concat([x_train, x_test])\n",
    "nlp.vocab.add_flag(lambda s: s.lower() in spacy.lang.en.stop_words.STOP_WORDS, spacy.attrs.IS_STOP)\n",
    "word_dict = {}\n",
    "lemma_dict = {}\n",
    "word_index = 1\n",
    "docs = nlp.pipe(text_list, n_threads = 2)\n",
    "word_sequences = []\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nstart_time = time.time()\\nfor doc in tqdm(docs): #one doc is one comment(row)\\n    #print(count)\\n    word_seq = []\\n    for token in doc:\\n        if (token.text not in word_dict) and (token.pos_ is not \"PUNCT\"):\\n            word_dict[token.text] = word_index\\n            word_index += 1\\n            lemma_dict[token.text] = token.lemma_\\n        if token.pos_ is not \"PUNCT\":\\n            word_seq.append(word_dict[token.text])\\n    word_sequences.append(word_seq)\\n    #count+= 1\\n\\nprint(\"--- %s seconds ---\" % (time.time() - start_time))\\ndel docs\\ndel text_list\\ngc.collect()\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create dictionary of word mapping to integers as wel as lemma dictionary\n",
    "#count = 1\n",
    "'''\n",
    "start_time = time.time()\n",
    "for doc in tqdm(docs): #one doc is one comment(row)\n",
    "    #print(count)\n",
    "    word_seq = []\n",
    "    for token in doc:\n",
    "        if (token.text not in word_dict) and (token.pos_ is not \"PUNCT\"):\n",
    "            word_dict[token.text] = word_index\n",
    "            word_index += 1\n",
    "            lemma_dict[token.text] = token.lemma_\n",
    "        if token.pos_ is not \"PUNCT\":\n",
    "            word_seq.append(word_dict[token.text])\n",
    "    word_sequences.append(word_seq)\n",
    "    #count+= 1\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "del docs\n",
    "del text_list\n",
    "gc.collect()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 400000\n",
    "tokenizer = text.Tokenizer(num_words = max_features, filters='',lower=False)\n",
    "tokenizer.fit_on_texts(list(x_train) + list(x_test))\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_test = tokenizer.texts_to_sequences(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#max_features = max_features or len(word_dict) + 1\n",
    "max_features = max_features or len(tokenizer.word_index) + 1\n",
    "max_features #number of unique words there are in the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n unknown words (crawl):  222943\n",
      "--- 80.3553524017334 seconds ---\n",
      "Size:  (487814, 300)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "#crawl_matrix, unknown_words_crawl = build_matrix(word_dict, lemma_dict, CRAWL_EMBEDDING_PATH)\n",
    "crawl_matrix, unknown_words_crawl = build_matrix(tokenizer.word_index, CRAWL_EMBEDDING_PATH)\n",
    "print('n unknown words (crawl): ', len(unknown_words_crawl))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print('Size: ', crawl_matrix.shape)\n",
    "del unknown_words_crawl\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n unknown words (glove):  225246\n",
      "--- 90.67239451408386 seconds ---\n",
      "Size:  (487814, 300)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "#glove_matrix, unknown_words_glove = build_matrix(word_dict, lemma_dict, GLOVE_EMBEDDING_PATH)\n",
    "glove_matrix, unknown_words_glove = build_matrix(tokenizer.word_index, GLOVE_EMBEDDING_PATH)\n",
    "print('n unknown words (glove): ', len(unknown_words_glove))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print('Size: ', glove_matrix.shape)\n",
    "del unknown_words_glove\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = np.concatenate([crawl_matrix, glove_matrix], axis=-1)\n",
    "\n",
    "del crawl_matrix\n",
    "del glove_matrix\n",
    "#del word_dict\n",
    "#del lemma_dict\n",
    "#del WORDS\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JigsawEvaluator:\n",
    "\n",
    "    def __init__(self, y_true, y_identity, power=-5, overall_model_weight=0.25):\n",
    "        self.y = y_true\n",
    "        self.y_i = y_identity\n",
    "        self.n_subgroups = self.y_i.shape[1]\n",
    "        self.power = power\n",
    "        self.overall_model_weight = overall_model_weight\n",
    "\n",
    "    @staticmethod\n",
    "    def _compute_auc(y_true, y_pred):\n",
    "        #print(\"Here: \", y_true)\n",
    "        #print(y_pred)\n",
    "        try:\n",
    "            return roc_auc_score(y_true, y_pred)\n",
    "        except ValueError:\n",
    "            return np.nan\n",
    "\n",
    "    def _compute_subgroup_auc(self, i, y_pred):\n",
    "        mask = self.y_i[:, i] == 1\n",
    "        #print(self.y)\n",
    "        return self._compute_auc(self.y[mask], y_pred[mask])\n",
    "\n",
    "    def _compute_bpsn_auc(self, i, y_pred):\n",
    "        mask = self.y_i[:, i] + self.y == 1\n",
    "        return self._compute_auc(self.y[mask], y_pred[mask])\n",
    "\n",
    "    def _compute_bnsp_auc(self, i, y_pred):\n",
    "        mask = self.y_i[:, i] + self.y != 1\n",
    "        return self._compute_auc(self.y[mask], y_pred[mask])\n",
    "\n",
    "    def compute_bias_metrics_for_model(self, y_pred):\n",
    "        #print(y_pred)\n",
    "        records = np.zeros((3, self.n_subgroups))\n",
    "        for i in range(self.n_subgroups):\n",
    "            #print(y_pred)\n",
    "            records[0, i] = self._compute_subgroup_auc(i, y_pred)\n",
    "            records[1, i] = self._compute_bpsn_auc(i, y_pred)\n",
    "            records[2, i] = self._compute_bnsp_auc(i, y_pred)\n",
    "        return records\n",
    "\n",
    "    def _calculate_overall_auc(self, y_pred):\n",
    "        return roc_auc_score(self.y, y_pred)\n",
    "\n",
    "    def _power_mean(self, array):\n",
    "        total = sum(np.power(array, self.power))\n",
    "        return np.power(total / len(array), 1 / self.power)\n",
    "\n",
    "    def get_final_metric(self, y_pred):\n",
    "        bias_metrics = self.compute_bias_metrics_for_model(y_pred)\n",
    "        bias_score = np.average([\n",
    "            self._power_mean(bias_metrics[0]),\n",
    "            self._power_mean(bias_metrics[1]),\n",
    "            self._power_mean(bias_metrics[2])\n",
    "        ])\n",
    "        overall_score = self.overall_model_weight * self._calculate_overall_auc(y_pred)\n",
    "        bias_score = (1 - self.overall_model_weight) * bias_score\n",
    "        return overall_score + bias_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1804874\n",
      "torch.Size([1804874])\n",
      "188\n",
      "tensor(713)\n"
     ]
    }
   ],
   "source": [
    "#x_train = word_sequences[:num_train_data]\n",
    "#x_test = word_sequences[num_train_data:]\n",
    "lengths = torch.from_numpy(np.array([len(x) for x in x_train]))\n",
    "test_lengths = torch.from_numpy(np.array([len(x) for x in x_test]))\n",
    "print(len(x_train))\n",
    "print(lengths.shape)\n",
    "maxlen = int(np.percentile(lengths, 95)) \n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "print(maxlen)\n",
    "print(lengths.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1804874, 188)\n",
      "(1804874,)\n",
      "1804874\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(num_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: PRELIM FILTER\n",
      "=> Saving a new best\n",
      "Epoch 1/6 \t loss=0.1110 \t time=652.35s\n",
      "=> Saving a new best\n",
      "Epoch 2/6 \t loss=0.1040 \t time=654.59s\n",
      "=> Saving a new best\n",
      "Epoch 3/6 \t loss=0.1027 \t time=655.05s\n",
      "=> Saving a new best\n",
      "Epoch 4/6 \t loss=0.1018 \t time=655.46s\n",
      "=> Saving a new best\n",
      "Epoch 5/6 \t loss=0.1013 \t time=655.24s\n",
      "=> Saving a new best\n",
      "Epoch 6/6 \t loss=0.1010 \t time=654.92s\n",
      "====================== END OF FILTER =============================\n"
     ]
    }
   ],
   "source": [
    "#Here, we use one model that filters out all the easy predictions\n",
    "x_train_torch = torch.tensor(x_train, dtype=torch.long).cuda()\n",
    "y_train_torch = torch.tensor(np.hstack([y_train[:, np.newaxis], y_aux_train]), dtype=torch.float32).cuda()\n",
    "train_dataset = data.TensorDataset(x_train_torch, lengths ,y_train_torch)\n",
    "\n",
    "print('Model: PRELIM FILTER')\n",
    "seed_everything(1234)\n",
    "\n",
    "model = NeuralNet(embedding_matrix, y_aux_train.shape[-1])\n",
    "model.cuda()\n",
    "\n",
    "#training using training and validation set\n",
    "model = train_model(model, train_dataset, train_dataset, output_dim=y_train_torch.shape[-1], \n",
    "                         loss_fn=nn.BCEWithLogitsLoss(reduction='mean'))\n",
    "\n",
    "\n",
    "#prediction on entire train set (actual predictions to be submitted)\n",
    "pred = predict(model, train_dataset, output_dim=y_train_torch.shape[-1])[:,0]\n",
    "\n",
    "        \n",
    "print('====================== END OF FILTER =============================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CLEAR \n",
    "del x_train_torch\n",
    "del y_train_torch\n",
    "del train_dataset\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAEICAYAAACavRnhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAGLdJREFUeJzt3XuwnVd93vHvEwlzCRhfpLhEMpETlDSK20zMqXGGNiFRxpYNRZ4pJfaEWBAFTYJJUqAFcUlMIUlN0+LGU3DqYtUyTTGOm9ZqsFEUY8ZJJzI+huAbIT41vkj4IixjLg4Xw69/7CXYHPa56KwjbY71/czs2e/7W+t913p1ZD/nveytVBWSJPX4vnFPQJK09BkmkqRuhokkqZthIknqZphIkroZJpKkboaJNIskf5TktxdpX89N8qUky9r6R5P86mLsu+3vuiSbFmt/0sFYPu4JSOOU5B7gBOAJ4BvAncAVwKVV9c2q+rWD2M+vVtVfzNSnqu4Dntk75zbe24HnVdUrhvZ/5mLsW1oIz0wk+OdV9Szgh4ALgTcBly3mAEn8xU1PaoaJ1FTVY1W1A/hFYFOSk5NcnuR3AZKsSPJnST6fZH+Sv0zyfUneDzwX+D/tMtYbk6xJUkk2J7kP+MhQbThYfiTJx5J8Ick1SY5rY70oyZ7h+SW5J8kvJNkAvAX4xTbeJ1v7ty6btXm9Lcm9SR5OckWSZ7e2A/PYlOS+JJ9L8tZD+6erJzvDRJqmqj4G7AH+2bSmN7T6SgaXxt4y6F6/DNzH4AznmVX174e2+Vngx4EzZhjuPOBXgOcwuNR28Tzm92Hg94EPtvF+ckS3V7bXzwE/zODy2n+e1uefAj8GrAd+J8mPzzW2NBPDRBrts8Bx02pfZ/A//R+qqq9X1V/W3F9u9/aq+nJV/f0M7e+vqtur6svAbwMvP3CDvtMvAe+uqrur6kvAm4Fzpp0V/duq+vuq+iTwSWBUKEnzYphIo60C9k+r/QEwBfx5kruTbJ3Hfu4/iPZ7gacAK+Y9y5n9YNvf8L6XMzijOuDBoeXHWaSHA3RkMkykaZL8EwZh8lfD9ar6YlW9oap+GHgp8Pok6w80z7C7uc5cThxafi6Ds5/PAV8GnjE0p2UMLq/Nd7+fZfBAwfC+nwAemmM7aUEME6lJcnSSlwBXAv+9qm6b1v6SJM9LEuAxBo8Sf7M1P8Tg3sTBekWSdUmeAbwDuLqqvgH8HfC0JC9O8hTgbcBTh7Z7CFiTZKb/hj8AvC7JSUmeybfvsTyxgDlKczJMpMFTWF9kcMnprcC7gVeN6LcW+AvgS8BfA++tqhta278D3tae9PrXBzH2+4HLGVxyehrwmzB4sgx4DfA+YC+DM5Xhp7v+pL0/kuTjI/a7re37RuAzwFeA3ziIeUkHJf7jWJKkXp6ZSJK6GSaSpG6GiSSpm2EiSep2xHz53IoVK2rNmjXjnoYkLSm33HLL56pq5Vz9jpgwWbNmDZOTk+OehiQtKUnunbuXl7kkSYvAMJEkdTNMJEndDBNJUjfDRJLUzTCRJHUzTCRJ3QwTSVI3w0SS1O2I+QR8jzVbPzS2se+58MVjG1uS5sszE0lSN8NEktTNMJEkdTNMJEndDBNJUjfDRJLUzTCRJHUzTCRJ3QwTSVK3OcMkybYkDye5fUTbG5JUkhVtPUkuTjKV5NYkpwz13ZTkrvbaNFR/fpLb2jYXJ0mrH5dkV+u/K8mxc40hSRqP+ZyZXA5smF5MciJwOnDfUPlMYG17bQEuaX2PAy4AXgCcClxwIBxan1cPbXdgrK3A9VW1Fri+rc84hiRpfOYMk6q6Edg/ouki4I1ADdU2AlfUwG7gmCTPAc4AdlXV/qp6FNgFbGhtR1fV7qoq4Arg7KF9bW/L26fVR40hSRqTBd0zSbIR2FtVn5zWtAq4f2h9T6vNVt8zog5wQlU90JYfBE6YY4xR89ySZDLJ5L59++ZzaJKkBTjoMEnyDOAtwO8s/nRGa2ctNWfH797u0qqaqKqJlStXHoKZSZJgYWcmPwKcBHwyyT3AauDjSf4BsBc4cajv6labrb56RB3goQOXr9r7w60+074kSWNy0GFSVbdV1Q9U1ZqqWsPgMtMpVfUgsAM4rz1xdRrwWLtUtRM4Pcmx7cb76cDO1vaFJKe1p7jOA65pQ+0ADjz1tWlafdQYkqQxmfMfx0ryAeBFwIoke4ALquqyGbpfC5wFTAGPA68CqKr9Sd4J3Nz6vaOqDtzUfw2DJ8aeDlzXXgAXAlcl2QzcC7x8tjEkSeMzZ5hU1blztK8ZWi7g/Bn6bQO2jahPAiePqD8CrB9Rn3EMSdJ4+Al4SVI3w0SS1M0wkSR1M0wkSd0ME0lSN8NEktTNMJEkdTNMJEndDBNJUjfDRJLUzTCRJHUzTCRJ3QwTSVI3w0SS1M0wkSR1M0wkSd0ME0lSN8NEktRtzjBJsi3Jw0luH6r9QZK/TXJrkv+V5JihtjcnmUry6SRnDNU3tNpUkq1D9ZOS3NTqH0xyVKs/ta1PtfY1c40hSRqP+ZyZXA5smFbbBZxcVf8Y+DvgzQBJ1gHnAD/RtnlvkmVJlgHvAc4E1gHntr4A7wIuqqrnAY8Cm1t9M/Boq1/U+s04xkEetyRpEc0ZJlV1I7B/Wu3Pq+qJtrobWN2WNwJXVtVXq+ozwBRwantNVdXdVfU14EpgY5IAPw9c3bbfDpw9tK/tbflqYH3rP9MYkqQxWYx7Jr8CXNeWVwH3D7XtabWZ6scDnx8KpgP179hXa3+s9Z9pX5KkMekKkyRvBZ4A/nhxprO4kmxJMplkct++feOejiQ9aS04TJK8EngJ8EtVVa28FzhxqNvqVpup/ghwTJLl0+rfsa/W/uzWf6Z9fZequrSqJqpqYuXKlQs4SknSfCwoTJJsAN4IvLSqHh9q2gGc057EOglYC3wMuBlY257cOorBDfQdLYRuAF7Wtt8EXDO0r01t+WXAR1r/mcaQJI3J8rk6JPkA8CJgRZI9wAUMnt56KrBrcE+c3VX1a1V1R5KrgDsZXP46v6q+0fbzWmAnsAzYVlV3tCHeBFyZ5HeBTwCXtfplwPuTTDF4AOAcgNnGkCSNR759herJbWJioiYnJxe07ZqtH1rk2czfPRe+eGxjS1KSW6pqYq5+fgJektTNMJEkdTNMJEndDBNJUjfDRJLUzTCRJHUzTCRJ3QwTSVI3w0SS1M0wkSR1M0wkSd0ME0lSN8NEktTNMJEkdTNMJEndDBNJUjfDRJLUzTCRJHUzTCRJ3eYMkyTbkjyc5Pah2nFJdiW5q70f2+pJcnGSqSS3JjllaJtNrf9dSTYN1Z+f5La2zcVJstAxJEnjMZ8zk8uBDdNqW4Hrq2otcH1bBzgTWNteW4BLYBAMwAXAC4BTgQsOhEPr8+qh7TYsZAxJ0vjMGSZVdSOwf1p5I7C9LW8Hzh6qX1EDu4FjkjwHOAPYVVX7q+pRYBewobUdXVW7q6qAK6bt62DGkCSNyULvmZxQVQ+05QeBE9ryKuD+oX57Wm22+p4R9YWM8V2SbEkymWRy37598zw0SdLB6r4B384oahHmsuhjVNWlVTVRVRMrV648BDOTJMHCw+ShA5eW2vvDrb4XOHGo3+pWm62+ekR9IWNIksZkoWGyAzjwRNYm4Jqh+nntiavTgMfapaqdwOlJjm033k8Hdra2LyQ5rT3Fdd60fR3MGJKkMVk+V4ckHwBeBKxIsofBU1kXAlcl2QzcC7y8db8WOAuYAh4HXgVQVfuTvBO4ufV7R1UduKn/GgZPjD0duK69ONgxJEnjM2eYVNW5MzStH9G3gPNn2M82YNuI+iRw8oj6Iwc7hiRpPPwEvCSpm2EiSepmmEiSuhkmkqRuhokkqZthIknqZphIkroZJpKkboaJJKmbYSJJ6maYSJK6GSaSpG6GiSSpm2EiSepmmEiSuhkmkqRuhokkqZthIknqZphIkrp1hUmS1yW5I8ntST6Q5GlJTkpyU5KpJB9MclTr+9S2PtXa1wzt582t/ukkZwzVN7TaVJKtQ/WRY0iSxmPBYZJkFfCbwERVnQwsA84B3gVcVFXPAx4FNrdNNgOPtvpFrR9J1rXtfgLYALw3ybIky4D3AGcC64BzW19mGUOSNAa9l7mWA09Pshx4BvAA8PPA1a19O3B2W97Y1mnt65Ok1a+sqq9W1WeAKeDU9pqqqrur6mvAlcDGts1MY0iSxmDBYVJVe4H/ANzHIEQeA24BPl9VT7Rue4BVbXkVcH/b9onW//jh+rRtZqofP8sY3yHJliSTSSb37du30EOVJM2h5zLXsQzOKk4CfhD4fgaXqb5nVNWlVTVRVRMrV64c93Qk6Umr5zLXLwCfqap9VfV14E+BFwLHtMteAKuBvW15L3AiQGt/NvDIcH3aNjPVH5llDEnSGPSEyX3AaUme0e5jrAfuBG4AXtb6bAKuacs72jqt/SNVVa1+Tnva6yRgLfAx4GZgbXty6ygGN+l3tG1mGkOSNAY990xuYnAT/OPAbW1flwJvAl6fZIrB/Y3L2iaXAce3+uuBrW0/dwBXMQiiDwPnV9U32j2R1wI7gU8BV7W+zDKGJGkMMvhF/8lvYmKiJicnF7Ttmq0fWuTZzN89F754bGNLUpJbqmpirn5+Al6S1M0wkSR1M0wkSd0ME0lSN8NEktTNMJEkdTNMJEndDBNJUjfDRJLUzTCRJHUzTCRJ3QwTSVI3w0SS1M0wkSR1M0wkSd0ME0lSN8NEktTNMJEkdTNMJEndusIkyTFJrk7yt0k+leSnkxyXZFeSu9r7sa1vklycZCrJrUlOGdrPptb/riSbhurPT3Jb2+biJGn1kWNIksaj98zkD4EPV9U/BH4S+BSwFbi+qtYC17d1gDOBte21BbgEBsEAXAC8ADgVuGAoHC4BXj203YZWn2kMSdIYLDhMkjwb+BngMoCq+lpVfR7YCGxv3bYDZ7fljcAVNbAbOCbJc4AzgF1Vtb+qHgV2ARta29FVtbuqCrhi2r5GjSFJGoOeM5OTgH3Af0vyiSTvS/L9wAlV9UDr8yBwQlteBdw/tP2eVputvmdEnVnG+A5JtiSZTDK5b9++hRyjJGkeesJkOXAKcElV/RTwZaZdbmpnFNUxxpxmG6OqLq2qiaqaWLly5aGchiQd0XrCZA+wp6puautXMwiXh9olKtr7w619L3Di0ParW222+uoRdWYZQ5I0BgsOk6p6ELg/yY+10nrgTmAHcOCJrE3ANW15B3Bee6rrNOCxdqlqJ3B6kmPbjffTgZ2t7QtJTmtPcZ03bV+jxpAkjcHyzu1/A/jjJEcBdwOvYhBQVyXZDNwLvLz1vRY4C5gCHm99qar9Sd4J3Nz6vaOq9rfl1wCXA08HrmsvgAtnGEOSNAZdYVJVfwNMjGhaP6JvAefPsJ9twLYR9Ung5BH1R0aNIUkaDz8BL0nqZphIkroZJpKkboaJJKmbYSJJ6maYSJK6GSaSpG6GiSSpm2EiSepmmEiSuhkmkqRuhokkqZthIknqZphIkroZJpKkboaJJKmbYSJJ6maYSJK6dYdJkmVJPpHkz9r6SUluSjKV5IPt34cnyVPb+lRrXzO0jze3+qeTnDFU39BqU0m2DtVHjiFJGo/FODP5LeBTQ+vvAi6qqucBjwKbW30z8GirX9T6kWQdcA7wE8AG4L0toJYB7wHOBNYB57a+s40hSRqDrjBJshp4MfC+th7g54GrW5ftwNlteWNbp7Wvb/03AldW1Ver6jPAFHBqe01V1d1V9TXgSmDjHGNIksag98zkPwFvBL7Z1o8HPl9VT7T1PcCqtrwKuB+gtT/W+n+rPm2bmeqzjfEdkmxJMplkct++fQs9RknSHBYcJkleAjxcVbcs4nwWVVVdWlUTVTWxcuXKcU9Hkp60lnds+0LgpUnOAp4GHA38IXBMkuXtzGE1sLf13wucCOxJshx4NvDIUP2A4W1G1R+ZZQxJ0hgs+Mykqt5cVaurag2DG+gfqapfAm4AXta6bQKuacs72jqt/SNVVa1+Tnva6yRgLfAx4GZgbXty66g2xo62zUxjSJLG4FB8zuRNwOuTTDG4v3FZq18GHN/qrwe2AlTVHcBVwJ3Ah4Hzq+ob7azjtcBOBk+LXdX6zjaGJGkMei5zfUtVfRT4aFu+m8GTWNP7fAX4lzNs/3vA742oXwtcO6I+cgxJ0nj4CXhJUjfDRJLUzTCRJHUzTCRJ3QwTSVI3w0SS1M0wkSR1M0wkSd0ME0lSN8NEktTNMJEkdTNMJEndDBNJUjfDRJLUzTCRJHUzTCRJ3QwTSVI3w0SS1M0wkSR1W3CYJDkxyQ1J7kxyR5LfavXjkuxKcld7P7bVk+TiJFNJbk1yytC+NrX+dyXZNFR/fpLb2jYXJ8lsY0iSxqPnzOQJ4A1VtQ44DTg/yTpgK3B9Va0Frm/rAGcCa9trC3AJDIIBuAB4AXAqcMFQOFwCvHpouw2tPtMYkqQxWHCYVNUDVfXxtvxF4FPAKmAjsL112w6c3ZY3AlfUwG7gmCTPAc4AdlXV/qp6FNgFbGhtR1fV7qoq4Ipp+xo1hiRpDBblnkmSNcBPATcBJ1TVA63pQeCEtrwKuH9osz2tNlt9z4g6s4wxfV5bkkwmmdy3b9/BH5gkaV66wyTJM4H/CfyrqvrCcFs7o6jeMWYz2xhVdWlVTVTVxMqVKw/lNCTpiNYVJkmewiBI/riq/rSVH2qXqGjvD7f6XuDEoc1Xt9ps9dUj6rONIUkag56nuQJcBnyqqt491LQDOPBE1ibgmqH6ee2prtOAx9qlqp3A6UmObTfeTwd2trYvJDmtjXXetH2NGkOSNAbLO7Z9IfDLwG1J/qbV3gJcCFyVZDNwL/Dy1nYtcBYwBTwOvAqgqvYneSdwc+v3jqra35ZfA1wOPB24rr2YZQxJ0hgsOEyq6q+AzNC8fkT/As6fYV/bgG0j6pPAySPqj4waQ5I0Hn4CXpLUzTCRJHUzTCRJ3QwTSVI3w0SS1M0wkSR1M0wkSd0ME0lSN8NEktTNMJEkdTNMJEnder7oUYfBmq0fGsu491z44rGMK2lp8sxEktTNMJEkdfMylyQdBuO6ZA2H57K1ZyaSpG6emWikJ/tvUZIWl2Gi7zk+wSYtPYaJ1Hg2Ji3ckg6TJBuAPwSWAe+rqgvHPCVpQcYZZNJiWLI34JMsA94DnAmsA85Nsm68s5KkI9OSDRPgVGCqqu6uqq8BVwIbxzwnSToiLeXLXKuA+4fW9wAvGO6QZAuwpa1+KcmnFzjWCuBzC9x2qfKYjwwe8xEg7+o65h+aT6elHCZzqqpLgUt795NksqomFmFKS4bHfGTwmI8Mh+OYl/Jlrr3AiUPrq1tNknSYLeUwuRlYm+SkJEcB5wA7xjwnSToiLdnLXFX1RJLXAjsZPBq8raruOETDdV8qW4I85iODx3xkOOTHnKo61GNIkp7klvJlLknS9wjDRJLUzTAZkmRDkk8nmUqydUT7U5N8sLXflGTN4Z/l4prHMb8+yZ1Jbk1yfZJ5PXP+vWyuYx7q9y+SVJIl/xjpfI45ycvbz/qOJP/jcM9xsc3j7/Zzk9yQ5BPt7/dZ45jnYkmyLcnDSW6foT1JLm5/HrcmOWVRJ1BVvgb3jZYB/w/4YeAo4JPAuml9XgP8UVs+B/jguOd9GI7554BntOVfPxKOufV7FnAjsBuYGPe8D8PPeS3wCeDYtv4D4573YTjmS4Ffb8vrgHvGPe/OY/4Z4BTg9hnazwKuAwKcBty0mON7ZvJt8/l6lo3A9rZ8NbA+SQ7jHBfbnMdcVTdU1eNtdTeDz/MsZfP9Gp53Au8CvnI4J3eIzOeYXw28p6oeBaiqhw/zHBfbfI65gKPb8rOBzx7G+S26qroR2D9Ll43AFTWwGzgmyXMWa3zD5NtGfT3Lqpn6VNUTwGPA8YdldofGfI552GYGv9ksZXMeczv9P7Gqnixf5Tufn/OPAj+a5P8m2d2+kXspm88xvx14RZI9wLXAbxyeqY3Nwf73flCW7OdMdHgleQUwAfzsuOdyKCX5PuDdwCvHPJXDbTmDS10vYnD2eWOSf1RVnx/rrA6tc4HLq+o/Jvlp4P1JTq6qb457YkuRZybfNp+vZ/lWnyTLGZwaP3JYZndozOsraZL8AvBW4KVV9dXDNLdDZa5jfhZwMvDRJPcwuLa8Y4nfhJ/Pz3kPsKOqvl5VnwH+jkG4LFXzOebNwFUAVfXXwNMYfAnkk9Uh/Qoqw+Tb5vP1LDuATW35ZcBHqt3ZWqLmPOYkPwX8FwZBstSvo8Mcx1xVj1XViqpaU1VrGNwnemlVTY5nuotiPn+3/zeDsxKSrGBw2evuwznJRTafY74PWA+Q5McZhMm+wzrLw2sHcF57qus04LGqemCxdu5lrqZm+HqWJO8AJqtqB3AZg1PhKQY3us4Z34z7zfOY/wB4JvAn7VmD+6rqpWObdKd5HvOTyjyPeSdwepI7gW8A/6aqluxZ9zyP+Q3Af03yOgY341+5lH85TPIBBr8QrGj3gS4AngJQVX/E4L7QWcAU8DjwqkUdfwn/2UmSvkd4mUuS1M0wkSR1M0wkSd0ME0lSN8NEktTNMJEkdTNMJEnd/j8rcLxYpR5HxgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#AND GET DISTRIBUTIONS OF PREDICTIONS\n",
    "plt.hist(pred)\n",
    "plt.title(\"Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1% of comments have a score of less than 3.337556288897759e-05\n",
      "2% of comments have a score of less than 4.199579620035365e-05\n",
      "3% of comments have a score of less than 4.931899733492173e-05\n",
      "4% of comments have a score of less than 5.6134143669623875e-05\n",
      "5% of comments have a score of less than 6.281218884396367e-05\n",
      "6% of comments have a score of less than 6.945829692995176e-05\n",
      "7% of comments have a score of less than 7.616755683557131e-05\n",
      "8% of comments have a score of less than 8.302498172270134e-05\n",
      "9% of comments have a score of less than 9.01532027637586e-05\n",
      "10% of comments have a score of less than 9.745782881509513e-05\n",
      "11% of comments have a score of less than 0.00010503281540877652\n",
      "12% of comments have a score of less than 0.00011289473506622015\n",
      "13% of comments have a score of less than 0.00012119509367039427\n",
      "14% of comments have a score of less than 0.000129816260014195\n",
      "15% of comments have a score of less than 0.00013898321994929575\n",
      "16% of comments have a score of less than 0.00014843305805698035\n",
      "17% of comments have a score of less than 0.00015830218180781232\n",
      "18% of comments have a score of less than 0.00016869838436832653\n",
      "19% of comments have a score of less than 0.0001797901655663736\n",
      "20% of comments have a score of less than 0.00019136882037855684\n",
      "21% of comments have a score of less than 0.00020364543757750653\n",
      "22% of comments have a score of less than 0.00021688510052626954\n",
      "23% of comments have a score of less than 0.00023077148696756925\n",
      "24% of comments have a score of less than 0.0002456313173752278\n",
      "25% of comments have a score of less than 0.00026124185387743637\n",
      "26% of comments have a score of less than 0.00027816378336865456\n",
      "27% of comments have a score of less than 0.00029609446588438007\n",
      "28% of comments have a score of less than 0.00031524597550742334\n",
      "29% of comments have a score of less than 0.000335820855689235\n",
      "30% of comments have a score of less than 0.0003576844756025821\n",
      "31% of comments have a score of less than 0.00038118807773571464\n",
      "32% of comments have a score of less than 0.00040673262439668175\n",
      "33% of comments have a score of less than 0.00043424226285424086\n",
      "34% of comments have a score of less than 0.0004636009287787605\n",
      "35% of comments have a score of less than 0.0004953260271577163\n",
      "36% of comments have a score of less than 0.0005290697165764868\n",
      "37% of comments have a score of less than 0.0005662781768478453\n",
      "38% of comments have a score of less than 0.0006071355368476361\n",
      "39% of comments have a score of less than 0.000650989524438046\n",
      "40% of comments have a score of less than 0.0006986698950640861\n",
      "41% of comments have a score of less than 0.0007506113027920946\n",
      "42% of comments have a score of less than 0.0008079618343617767\n",
      "43% of comments have a score of less than 0.0008698991301935167\n",
      "44% of comments have a score of less than 0.00093832200858742\n",
      "45% of comments have a score of less than 0.0010133484669495373\n",
      "46% of comments have a score of less than 0.0010961663094349206\n",
      "47% of comments have a score of less than 0.0011870523157995192\n",
      "48% of comments have a score of less than 0.001287220912054181\n",
      "49% of comments have a score of less than 0.0013969161291606724\n",
      "50% of comments have a score of less than 0.0015207676333375275\n",
      "51% of comments have a score of less than 0.0016554303781595079\n",
      "52% of comments have a score of less than 0.0018052565120160599\n",
      "53% of comments have a score of less than 0.001973765352740885\n",
      "54% of comments have a score of less than 0.0021600405871868133\n",
      "55% of comments have a score of less than 0.0023713659029453993\n",
      "56% of comments have a score of less than 0.002611720450222494\n",
      "57% of comments have a score of less than 0.002880919578019526\n",
      "58% of comments have a score of less than 0.003186239572241902\n",
      "59% of comments have a score of less than 0.003527613640762852\n",
      "60% of comments have a score of less than 0.00391936376690865\n",
      "61% of comments have a score of less than 0.004363956525921822\n",
      "62% of comments have a score of less than 0.004876531586050988\n",
      "63% of comments have a score of less than 0.005473214695230126\n",
      "64% of comments have a score of less than 0.006159290671348572\n",
      "65% of comments have a score of less than 0.006937866914086043\n",
      "66% of comments have a score of less than 0.007842073366045954\n",
      "67% of comments have a score of less than 0.008876413106918335\n",
      "68% of comments have a score of less than 0.01008838940411807\n",
      "69% of comments have a score of less than 0.011530974945053445\n",
      "70% of comments have a score of less than 0.013219328690320196\n",
      "71% of comments have a score of less than 0.01519688793458043\n",
      "72% of comments have a score of less than 0.017535351291298882\n",
      "73% of comments have a score of less than 0.020323971882462502\n",
      "74% of comments have a score of less than 0.02359050258994103\n",
      "75% of comments have a score of less than 0.02752486662939191\n",
      "76% of comments have a score of less than 0.032231080532073964\n",
      "77% of comments have a score of less than 0.03775924619287252\n",
      "78% of comments have a score of less than 0.04442781560122966\n",
      "79% of comments have a score of less than 0.05237089507281782\n",
      "80% of comments have a score of less than 0.06180671602487566\n",
      "81% of comments have a score of less than 0.07307182282209397\n",
      "82% of comments have a score of less than 0.0864164972305296\n",
      "83% of comments have a score of less than 0.10217970736324758\n",
      "84% of comments have a score of less than 0.1204836279153823\n",
      "85% of comments have a score of less than 0.14252778440713884\n",
      "86% of comments have a score of less than 0.16891480416059498\n",
      "87% of comments have a score of less than 0.19938909336924554\n",
      "88% of comments have a score of less than 0.23511009633541108\n",
      "89% of comments have a score of less than 0.27660134643316264\n",
      "90% of comments have a score of less than 0.3248975664377212\n",
      "91% of comments have a score of less than 0.3805968904495241\n",
      "92% of comments have a score of less than 0.4444474744796755\n",
      "93% of comments have a score of less than 0.518796628117562\n",
      "94% of comments have a score of less than 0.6031810271739957\n",
      "95% of comments have a score of less than 0.6980702638626073\n",
      "96% of comments have a score of less than 0.7990966200828549\n",
      "97% of comments have a score of less than 0.8934608978033068\n",
      "98% of comments have a score of less than 0.9580717134475707\n",
      "99% of comments have a score of less than 0.9900480794906616\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 100):\n",
    "    thres = np.percentile(pred, i)\n",
    "    print('{}% of comments have a score of less than {}'.format(i, thres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.006159290671348572\n",
      "[False False False ... False  True False]\n"
     ]
    }
   ],
   "source": [
    "#FILTER OUT ALL THE EASY COMMENTS (64th percentile)\n",
    "threshold = np.percentile(pred, 64)\n",
    "print(threshold)\n",
    "keep_index = (pred > threshold)\n",
    "print(keep_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train[np.where(keep_index)]\n",
    "y_train = y_train[np.where(keep_index)]\n",
    "y_train_identity = y_train_identity[np.where(keep_index)]\n",
    "y_aux_train = y_aux_train[np.where(keep_index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(649754, 188)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2732"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del pred\n",
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model  0\n",
      "=> Saving a new best\n",
      "Epoch 1/6 \t loss=0.2486 \t time=154.56s\n",
      "=> Saving a new best\n",
      "Epoch 2/6 \t loss=0.2304 \t time=154.48s\n",
      "=> Saving a new best\n",
      "Epoch 3/6 \t loss=0.2273 \t time=153.99s\n",
      "=> Saving a new best\n",
      "Epoch 4/6 \t loss=0.2256 \t time=154.70s\n",
      "=> Saving a new best\n",
      "Epoch 5/6 \t loss=0.2246 \t time=155.08s\n",
      "=> Saving a new best\n",
      "Epoch 6/6 \t loss=0.2239 \t time=154.63s\n",
      "\n",
      "Model  1\n",
      "=> Saving a new best\n",
      "Epoch 1/6 \t loss=0.2487 \t time=154.10s\n",
      "=> Saving a new best\n",
      "Epoch 2/6 \t loss=0.2305 \t time=154.77s\n",
      "=> Saving a new best\n",
      "Epoch 3/6 \t loss=0.2273 \t time=154.67s\n",
      "=> Saving a new best\n",
      "Epoch 4/6 \t loss=0.2256 \t time=154.55s\n",
      "=> Saving a new best\n",
      "Epoch 5/6 \t loss=0.2246 \t time=154.15s\n",
      "=> Saving a new best\n",
      "Epoch 6/6 \t loss=0.2239 \t time=154.66s\n",
      "\n",
      "Kaggle Score:  0.8834263304540407\n",
      "ROC score:  0.9027343674849325\n",
      "=============End-of-Fold================\n",
      "Model  0\n",
      "=> Saving a new best\n",
      "Epoch 1/6 \t loss=0.2483 \t time=154.50s\n",
      "=> Saving a new best\n",
      "Epoch 2/6 \t loss=0.2303 \t time=154.51s\n",
      "=> Saving a new best\n",
      "Epoch 3/6 \t loss=0.2271 \t time=154.18s\n",
      "=> Saving a new best\n",
      "Epoch 4/6 \t loss=0.2255 \t time=154.60s\n",
      "=> Saving a new best\n",
      "Epoch 5/6 \t loss=0.2244 \t time=154.55s\n",
      "=> Saving a new best\n",
      "Epoch 6/6 \t loss=0.2238 \t time=154.48s\n",
      "\n",
      "Model  1\n",
      "=> Saving a new best\n",
      "Epoch 1/6 \t loss=0.2486 \t time=154.07s\n",
      "=> Saving a new best\n",
      "Epoch 2/6 \t loss=0.2304 \t time=154.60s\n",
      "=> Saving a new best\n",
      "Epoch 3/6 \t loss=0.2273 \t time=154.72s\n",
      "=> Saving a new best\n",
      "Epoch 4/6 \t loss=0.2256 \t time=154.22s\n",
      "=> Saving a new best\n",
      "Epoch 5/6 \t loss=0.2245 \t time=154.32s\n",
      "=> Saving a new best\n",
      "Epoch 6/6 \t loss=0.2238 \t time=154.72s\n",
      "\n",
      "Kaggle Score:  0.8830907120400693\n",
      "ROC score:  0.9021457969706173\n",
      "=============End-of-Fold================\n",
      "Model  0\n",
      "=> Saving a new best\n",
      "Epoch 1/6 \t loss=0.2482 \t time=154.51s\n",
      "=> Saving a new best\n",
      "Epoch 2/6 \t loss=0.2304 \t time=154.56s\n",
      "=> Saving a new best\n",
      "Epoch 3/6 \t loss=0.2273 \t time=154.37s\n",
      "=> Saving a new best\n",
      "Epoch 4/6 \t loss=0.2255 \t time=154.80s\n",
      "=> Saving a new best\n",
      "Epoch 5/6 \t loss=0.2246 \t time=154.79s\n",
      "=> Saving a new best\n",
      "Epoch 6/6 \t loss=0.2238 \t time=154.62s\n",
      "\n",
      "Model  1\n",
      "=> Saving a new best\n",
      "Epoch 1/6 \t loss=0.2489 \t time=154.00s\n",
      "=> Saving a new best\n",
      "Epoch 2/6 \t loss=0.2304 \t time=154.68s\n",
      "=> Saving a new best\n",
      "Epoch 3/6 \t loss=0.2271 \t time=154.74s\n",
      "=> Saving a new best\n",
      "Epoch 4/6 \t loss=0.2255 \t time=154.62s\n",
      "=> Saving a new best\n",
      "Epoch 5/6 \t loss=0.2245 \t time=154.30s\n",
      "=> Saving a new best\n",
      "Epoch 6/6 \t loss=0.2238 \t time=154.77s\n",
      "\n",
      "Kaggle Score:  0.8826232776677687\n",
      "ROC score:  0.9015850315231676\n",
      "=============End-of-Fold================\n",
      "Model  0\n",
      "=> Saving a new best\n",
      "Epoch 1/6 \t loss=0.2486 \t time=154.62s\n",
      "=> Saving a new best\n",
      "Epoch 2/6 \t loss=0.2307 \t time=154.59s\n",
      "=> Saving a new best\n",
      "Epoch 3/6 \t loss=0.2275 \t time=154.31s\n",
      "=> Saving a new best\n",
      "Epoch 4/6 \t loss=0.2258 \t time=154.69s\n",
      "=> Saving a new best\n",
      "Epoch 5/6 \t loss=0.2247 \t time=154.73s\n",
      "=> Saving a new best\n",
      "Epoch 6/6 \t loss=0.2241 \t time=154.57s\n",
      "\n",
      "Model  1\n",
      "=> Saving a new best\n",
      "Epoch 1/6 \t loss=0.2488 \t time=153.95s\n",
      "=> Saving a new best\n",
      "Epoch 2/6 \t loss=0.2305 \t time=154.59s\n",
      "=> Saving a new best\n",
      "Epoch 3/6 \t loss=0.2274 \t time=154.14s\n",
      "=> Saving a new best\n",
      "Epoch 4/6 \t loss=0.2257 \t time=154.40s\n",
      "=> Saving a new best\n",
      "Epoch 5/6 \t loss=0.2246 \t time=154.23s\n",
      "=> Saving a new best\n",
      "Epoch 6/6 \t loss=0.2238 \t time=154.51s\n",
      "\n",
      "Kaggle Score:  0.8820354748891639\n",
      "ROC score:  0.9027634988634995\n",
      "=============End-of-Fold================\n",
      "Model  0\n",
      "=> Saving a new best\n",
      "Epoch 1/6 \t loss=0.2483 \t time=154.40s\n",
      "=> Saving a new best\n",
      "Epoch 2/6 \t loss=0.2305 \t time=154.41s\n",
      "=> Saving a new best\n",
      "Epoch 3/6 \t loss=0.2274 \t time=154.10s\n",
      "=> Saving a new best\n",
      "Epoch 4/6 \t loss=0.2257 \t time=154.67s\n",
      "=> Saving a new best\n",
      "Epoch 5/6 \t loss=0.2246 \t time=154.61s\n",
      "=> Saving a new best\n",
      "Epoch 6/6 \t loss=0.2240 \t time=154.54s\n",
      "\n",
      "Model  1\n",
      "=> Saving a new best\n",
      "Epoch 1/6 \t loss=0.2486 \t time=153.91s\n",
      "=> Saving a new best\n",
      "Epoch 2/6 \t loss=0.2306 \t time=154.38s\n",
      "=> Saving a new best\n",
      "Epoch 3/6 \t loss=0.2273 \t time=154.53s\n",
      "=> Saving a new best\n",
      "Epoch 4/6 \t loss=0.2256 \t time=154.44s\n",
      "=> Saving a new best\n",
      "Epoch 5/6 \t loss=0.2246 \t time=154.19s\n",
      "=> Saving a new best\n",
      "Epoch 6/6 \t loss=0.2240 \t time=154.57s\n",
      "\n",
      "Kaggle Score:  0.8829231106298552\n",
      "ROC score:  0.9029899907603467\n",
      "=============End-of-Fold================\n",
      "Time:  9504.520491838455\n",
      "Final Kaggle Score:  0.8826374502680023\n",
      "Final ROC score:  0.9023287111848801\n"
     ]
    }
   ],
   "source": [
    "\n",
    "all_val_preds = []\n",
    "all_test_preds = []\n",
    "num_splits = 5\n",
    "\n",
    "#Add in K fold \n",
    "random_state = 2019\n",
    "\n",
    "#K fold splits\n",
    "splits = list(StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=random_state).split(x_train,y_train))\n",
    "\n",
    "#final validation predictions\n",
    "final_val_preds = np.zeros((x_train.shape[0]))\n",
    "\n",
    "#final test predictions to be stored in this var\n",
    "final_test_preds = np.zeros((x_test.shape[0]))\n",
    "\n",
    "start_time = time.time()\n",
    "for fold in range(num_splits):\n",
    "    tr_ind, val_ind = splits[fold]\n",
    "    all_val_preds = []\n",
    "    all_test_preds = []\n",
    "    #print('Training set size: ', len(tr_ind))\n",
    "    #print('Val set size: ', len(val_ind))\n",
    "    x_training = x_train[tr_ind]\n",
    "    y_training = y_train[tr_ind]\n",
    "    y_aux_training = y_aux_train[tr_ind]\n",
    "    \n",
    "    x_val = x_train[val_ind]\n",
    "    y_val = y_train[val_ind]\n",
    "    y_aux_val = y_aux_train[val_ind]\n",
    "    \n",
    "    \n",
    "    \n",
    "    x_train_torch = torch.tensor(x_training, dtype=torch.long).cuda()\n",
    "    x_val_torch = torch.tensor(x_val, dtype=torch.long).cuda()\n",
    "    y_train_torch = torch.tensor(np.hstack([y_training[:, np.newaxis], y_aux_training]), dtype=torch.float32).cuda()\n",
    "    y_val_torch = torch.tensor(np.hstack([y_val[:, np.newaxis], y_aux_val]), dtype=torch.float32).cuda()\n",
    "    \n",
    "    x_test_torch = torch.tensor(x_test, dtype=torch.long).cuda()\n",
    "    \n",
    "    ###\n",
    "    \n",
    "    #test_dataset = data.TensorDataset(x_test_torch, test_lengths)\n",
    "    #train_dataset = data.TensorDataset(x_train_torch, lengths, y_train_torch)\n",
    "    #val_dataset = data.TensorDataset(x_val_torch)\n",
    "\n",
    "    #train_collator = SequenceBucketCollator(lambda lengths: lengths.max(), sequence_index=0, length_index=1, label_index=2)\n",
    "    #test_collator = SequenceBucketCollator(lambda lengths: lengths.max(), sequence_index=0, length_index=1)\n",
    "    \n",
    "    ####\n",
    "    train_dataset = data.TensorDataset(x_train_torch, lengths[tr_ind], y_train_torch)\n",
    "    val_dataset = data.TensorDataset(x_val_torch, lengths[val_ind], y_val_torch)\n",
    "    test_dataset = data.TensorDataset(x_test_torch, test_lengths)\n",
    "    \n",
    "    #temp_dataset = data.Subset(train_dataset, indices=[0, 1])\n",
    "\n",
    "    for model_idx in range(NUM_MODELS):\n",
    "        print('Model ', model_idx)\n",
    "        seed_everything(1234 + model_idx)\n",
    "\n",
    "        model = NeuralNet(embedding_matrix, y_aux_train.shape[-1])\n",
    "        model.cuda()\n",
    "\n",
    "        #training using training and validation set\n",
    "        model = train_model(model, train_dataset, val_dataset, output_dim=y_train_torch.shape[-1], loss_fn=nn.BCEWithLogitsLoss(reduction='mean'))\n",
    "        \n",
    "        #prediction on validation set (used for score measurement)\n",
    "        val_pred = predict(model, val_dataset, output_dim=y_train_torch.shape[-1], pred_type=\"val\") #val preds on the val split\n",
    "        all_val_preds.append(val_pred)\n",
    "        #print(len(val_pred))\n",
    "        \n",
    "        #prediction on entire test set (actual predictions to be submitted)\n",
    "        test_pred = predict(model, test_dataset, output_dim=y_train_torch.shape[-1], pred_type=\"test\")\n",
    "        all_test_preds.append(test_pred)\n",
    "        \n",
    "        print()\n",
    "        \n",
    "    #average validation prediction amongst all models\n",
    "    avg_val = np.mean(all_val_preds, axis=0)[:, 0] #will be printed out per split\n",
    "    final_val_preds[val_ind] += avg_val\n",
    "    \n",
    "    avg_test = np.mean(all_test_preds, axis=0)[:, 0]\n",
    "    \n",
    "    final_test_preds += avg_test\n",
    "\n",
    "    y_true = y_train[val_ind] #true scores for this validation set\n",
    "    y_identity = y_train_identity[val_ind] #true scores for the identity groups for this validation set\n",
    "    evaluator = JigsawEvaluator(y_true, y_identity)\n",
    "    auc_score = evaluator.get_final_metric(avg_val)\n",
    "\n",
    "    roc_score = roc_auc_score(y_train[val_ind], avg_val)\n",
    "    print('Kaggle Score: ', auc_score)\n",
    "    print('ROC score: ', roc_score)\n",
    "    \n",
    "    del x_train_torch\n",
    "    del x_val_torch\n",
    "    del y_train_torch\n",
    "    del y_val_torch\n",
    "    del x_test_torch\n",
    "    del train_dataset\n",
    "    del val_dataset\n",
    "    del test_dataset\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print('=============End-of-Fold================')\n",
    "    \n",
    "end_time = time.time()\n",
    "print('Time: ', end_time - start_time)\n",
    "\n",
    "#Final combined score\n",
    "y_true = y_train\n",
    "y_identity = y_train_identity\n",
    "evaluator = JigsawEvaluator(y_true, y_identity)\n",
    "auc_score = evaluator.get_final_metric(final_val_preds)\n",
    "print('Final Kaggle Score: ', auc_score)\n",
    "print('Final ROC score: ', roc_auc_score(y_train, final_val_preds))\n",
    "\n",
    "#average test predictions AGAIN this time by number of splits\n",
    "final_test_preds /= num_splits\n",
    "#print(final_test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7000000</td>\n",
       "      <td>0.018381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7000001</td>\n",
       "      <td>0.004410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7000002</td>\n",
       "      <td>0.014784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7000003</td>\n",
       "      <td>0.009222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7000004</td>\n",
       "      <td>0.977801</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  prediction\n",
       "0  7000000    0.018381\n",
       "1  7000001    0.004410\n",
       "2  7000002    0.014784\n",
       "3  7000003    0.009222\n",
       "4  7000004    0.977801"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.DataFrame.from_dict({\n",
    "    'id': test['id'],\n",
    "    'prediction': final_test_preds\n",
    "})\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
