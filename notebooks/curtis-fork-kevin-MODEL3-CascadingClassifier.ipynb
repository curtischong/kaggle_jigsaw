{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "import random\n",
    "from tqdm._tqdm_notebook import tqdm_notebook as tqdm\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "lc = LancasterStemmer()\n",
    "from nltk.stem import SnowballStemmer\n",
    "sb = SnowballStemmer(\"english\")\n",
    "from keras.preprocessing import text, sequence\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from torch.nn import functional as F\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import spacy\n",
    "import gensim\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import apex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable progress bars when submitting\n",
    "def is_interactive():\n",
    "   return 'SHLVL' not in os.environ\n",
    "\n",
    "if not is_interactive():\n",
    "    def nop(it, *a, **k):\n",
    "        return it\n",
    "\n",
    "    tqdm = nop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CRAWL_EMBEDDING_PATH = '../input/crawl-300d-2M.vec'\n",
    "GLOVE_EMBEDDING_PATH = '../input/glove.840B.300d.txt'\n",
    "NUM_MODELS = 2\n",
    "LSTM_UNITS = 128\n",
    "DENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\n",
    "MAX_LEN = 220"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef build_matrix(word_dict, lemma_dict, path):\\n    embed_size = 300\\n    embeddings_index = load_embeddings(path)\\n    embedding_matrix = np.zeros((len(word_dict) + 1, embed_size), dtype=np.float32)\\n    unknown_words = []\\n    unknown_vector = np.zeros((embed_size,), dtype=np.float32) - 1\\n    \\n    for key in tqdm(word_dict):\\n        word = key\\n        embedding_vector = embeddings_index.get(word)\\n        if embedding_vector is not None:\\n            embedding_matrix[word_dict[key]] = embedding_vector\\n            continue\\n        word = key.lower()\\n        embedding_vector = embeddings_index.get(word)\\n        if embedding_vector is not None:\\n            embedding_matrix[word_dict[key]] = embedding_vector\\n            continue\\n        word = key.upper()\\n        embedding_vector = embeddings_index.get(word)\\n        if embedding_vector is not None:\\n            embedding_matrix[word_dict[key]] = embedding_vector\\n            continue\\n        word = key.capitalize()\\n        embedding_vector = embeddings_index.get(word)\\n        if embedding_vector is not None:\\n            embedding_matrix[word_dict[key]] = embedding_vector\\n            continue\\n        word = ps.stem(key)\\n        embedding_vector = embeddings_index.get(word)\\n        if embedding_vector is not None:\\n            embedding_matrix[word_dict[key]] = embedding_vector\\n            continue\\n        word = lc.stem(key)\\n        embedding_vector = embeddings_index.get(word)\\n        if embedding_vector is not None:\\n            embedding_matrix[word_dict[key]] = embedding_vector\\n            continue\\n        word = sb.stem(key)\\n        embedding_vector = embeddings_index.get(word)\\n        if embedding_vector is not None:\\n            embedding_matrix[word_dict[key]] = embedding_vector\\n            continue\\n        word = lemma_dict[key]\\n        embedding_vector = embeddings_index.get(word)\\n        if embedding_vector is not None:\\n            embedding_matrix[word_dict[key]] = embedding_vector\\n            continue\\n        if len(key) > 1:\\n            word = correction(key)\\n            embedding_vector = embeddings_index.get(word)\\n            if embedding_vector is not None:\\n                embedding_matrix[word_dict[key]] = embedding_vector\\n                continue\\n        \\n        #Unknown word, does not exist in dictionary\\n        embedding_matrix[word_dict[key]] = unknown_vector\\n        unknown_words.append(word)\\n    return embedding_matrix, unknown_words\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "def load_embeddings(path):\n",
    "    with open(path) as f:\n",
    "        return dict(get_coefs(*line.strip().split(' ')) for line in tqdm(f))\n",
    "    \n",
    "def build_matrix(word_index, path):\n",
    "    embedding_index = load_embeddings(path)\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "    unknown_words = []\n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        try:\n",
    "            embedding_matrix[i] = embedding_index[word]\n",
    "        except KeyError:\n",
    "            unknown_words.append(word)\n",
    "    return embedding_matrix, unknown_words\n",
    "\n",
    "'''\n",
    "def build_matrix(word_dict, lemma_dict, path):\n",
    "    embed_size = 300\n",
    "    embeddings_index = load_embeddings(path)\n",
    "    embedding_matrix = np.zeros((len(word_dict) + 1, embed_size), dtype=np.float32)\n",
    "    unknown_words = []\n",
    "    unknown_vector = np.zeros((embed_size,), dtype=np.float32) - 1\n",
    "    \n",
    "    for key in tqdm(word_dict):\n",
    "        word = key\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = key.lower()\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = key.upper()\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = key.capitalize()\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = ps.stem(key)\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = lc.stem(key)\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = sb.stem(key)\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = lemma_dict[key]\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        if len(key) > 1:\n",
    "            word = correction(key)\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[word_dict[key]] = embedding_vector\n",
    "                continue\n",
    "        \n",
    "        #Unknown word, does not exist in dictionary\n",
    "        embedding_matrix[word_dict[key]] = unknown_vector\n",
    "        unknown_words.append(word)\n",
    "    return embedding_matrix, unknown_words\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceBucketCollator():\n",
    "    def __init__(self, choose_length, sequence_index, length_index, label_index=None):\n",
    "        self.choose_length = choose_length\n",
    "        self.sequence_index = sequence_index\n",
    "        self.length_index = length_index\n",
    "        self.label_index = label_index\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        batch = [torch.stack(x) for x in list(zip(*batch))]\n",
    "        \n",
    "        sequences = batch[self.sequence_index]\n",
    "        lengths = batch[self.length_index]\n",
    "        \n",
    "        length = self.choose_length(lengths)\n",
    "        mask = torch.arange(start=maxlen, end=0, step=-1) < length\n",
    "        padded_sequences = sequences[:, mask]\n",
    "        \n",
    "        batch[self.sequence_index] = padded_sequences\n",
    "        \n",
    "        if self.label_index is not None:\n",
    "            return [x for i, x in enumerate(batch) if i != self.label_index], batch[self.label_index]\n",
    "    \n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "filepath = './model_files/checkpoint.pth'\n",
    "\n",
    "def train_model(model, train, test, loss_fn, output_dim, lr=0.001,\n",
    "                batch_size=512, n_epochs=6, n_epochs_embed=2,\n",
    "                enable_checkpoint_ensemble=True):\n",
    "    \n",
    "    train_collator = SequenceBucketCollator(lambda lengths: lengths.max(), \n",
    "                                            sequence_index=0, \n",
    "                                            length_index=1, \n",
    "                                            label_index=2)\n",
    "    \n",
    "    param_lrs = [{'params': param, 'lr': lr} for param in model.parameters()]\n",
    "    optimizer = torch.optim.Adam(param_lrs, lr=lr)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda epoch: 0.6 ** epoch)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True, collate_fn=train_collator)\n",
    "    val_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False, collate_fn=train_collator)\n",
    "    all_test_preds = []\n",
    "    checkpoint_weights = [2 ** epoch for epoch in range(n_epochs)]\n",
    "    \n",
    "    best_loss = 1\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        model.train() #set model to train mode\n",
    "        avg_loss = 0.\n",
    "        \n",
    "        for data in tqdm(train_loader, disable=False):\n",
    "            \n",
    "            #training loop\n",
    "            x_batch = data[:-1]\n",
    "            #print(\"First: \", x_batch[0][0])\n",
    "            #print(\"Second: \", x_batch[0][1])\n",
    "            first = x_batch[0][0]\n",
    "            second = x_batch[0][1]\n",
    "            \n",
    "            y_batch = data[-1]\n",
    "\n",
    "            y_pred = model(first, second)  #feed data into model          \n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            \n",
    "            #calculate error and adjust model params\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item() / len(train_loader)\n",
    "        \n",
    "        #Check if loss is better than current best loss, if so, save the model\n",
    "        is_best = (avg_loss < best_loss)\n",
    "        \n",
    "        if is_best:\n",
    "            print (\"=> Saving a new best\")\n",
    "            best_loss = avg_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_loss': best_loss\n",
    "            }, filepath)  # save checkpoint\n",
    "        else:\n",
    "            print (\"=> Model Accuracy did not improve\")\n",
    "            \n",
    "        \n",
    "        model.eval() #set model to eval mode for test data\n",
    "        test_preds = np.zeros((len(test), output_dim))\n",
    "        \n",
    "    \n",
    "        for i, x_batch in enumerate(val_loader):\n",
    "            #print(\"X_Batch: \", x_batch)\n",
    "            data_param = x_batch[0][0]\n",
    "            lengths_param = x_batch[0][1]\n",
    "            y_pred = sigmoid(model(data_param, lengths_param).detach().cpu().numpy()) #feed data into model\n",
    "\n",
    "            test_preds[i * batch_size:(i+1) * batch_size, :] = y_pred #get test predictions\n",
    "        \n",
    "        #test_preds has the predictions for the entire test set now\n",
    "        all_test_preds.append(test_preds) #append predictions to the record of all past predictions\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print('Epoch {}/{} \\t loss={:.4f} \\t time={:.2f}s'.format(\n",
    "              epoch + 1, n_epochs, avg_loss, elapsed_time))\n",
    "        \n",
    "    #Make embeddings layer only layer unfreezed, train again (literally run through the n_epochs)\n",
    "    #maybe define a n_epochs_embedding\n",
    "    \n",
    "    #parameters = model.parameters()\n",
    "    #for param in parameters:\n",
    "    #        param.requires_grad = False\n",
    "    #parameters[0].requires_grad = True\n",
    "    \n",
    "    '''\n",
    "    ct = 0\n",
    "    for child in model.children():\n",
    "        if ct == 0:\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = True\n",
    "        else:\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = False\n",
    "        ct += 1\n",
    "    \n",
    "    for epoch in range(n_epochs_embed):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        model.train() #set model to train mode\n",
    "        avg_loss = 0.\n",
    "        \n",
    "        for data in tqdm(train_loader, disable=False):\n",
    "            \n",
    "            #training loop\n",
    "            x_batch = data[:-1]\n",
    "            y_batch = data[-1]\n",
    "\n",
    "            y_pred = model(*x_batch)  #feed data into model          \n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            \n",
    "            #calculate error and adjust model params\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item() / len(train_loader) #gets the loss per epoch\n",
    "        \n",
    "            \n",
    "        model.eval() #set model to eval mode for test data\n",
    "        test_preds = np.zeros((len(test), output_dim))\n",
    "    \n",
    "        for i, x_batch in enumerate(test_loader):\n",
    "            y_pred = sigmoid(model(*x_batch).detach().cpu().numpy()) #feed data into model\n",
    "\n",
    "            test_preds[i * batch_size:(i+1) * batch_size, :] = y_pred #get test predictions\n",
    "        \n",
    "        #test_preds has the predictions for the entire test set now\n",
    "        #all_test_preds.append(test_preds) #append predictions to the record of all past predictions\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print('[EMBEDDING TRAINING] Epoch {}/{} \\t loss={:.4f} \\t time={:.2f}s '.format(\n",
    "              epoch + 1, n_epochs_embed, avg_loss, elapsed_time))\n",
    "    '''\n",
    "    \n",
    "    #PREDICTION CODE\n",
    "    '''\n",
    "    if enable_checkpoint_ensemble:\n",
    "        #if our approach is an ensemble then we average it amongst all the historical predictions\n",
    "        test_preds = np.average(all_test_preds, weights=checkpoint_weights, axis=0)    \n",
    "    else:\n",
    "        #if our approach is not an ensemble then we just take the last set of predictions\n",
    "        test_preds = all_test_preds[-1]\n",
    "        \n",
    "    return test_preds\n",
    "    '''\n",
    "    \n",
    "    #return trained model\n",
    "    return model\n",
    "\n",
    "def predict(model, test, output_dim, batch_size=512, pred_type=\"val\"):\n",
    "    checkpoint = torch.load(filepath)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    #optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    if pred_type == \"test\":\n",
    "        test_collator = SequenceBucketCollator(lambda lengths: lengths.max(), sequence_index=0, length_index=1)\n",
    "        test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False, collate_fn=test_collator)\n",
    "\n",
    "        model.eval() #set model to eval mode for test data\n",
    "        test_preds = np.zeros((len(test), output_dim))\n",
    "\n",
    "        for i, x_batch in enumerate(test_loader):\n",
    "            #print(x_batch[0])\n",
    "            data_param = x_batch[0]\n",
    "            lengths_param = x_batch[1]\n",
    "            y_pred = sigmoid(model(data_param, lengths_param).detach().cpu().numpy()) #feed data into model\n",
    "            test_preds[i * batch_size:(i+1) * batch_size, :] = y_pred #get test predictions\n",
    "\n",
    "        return test_preds\n",
    "    else:\n",
    "        test_collator = SequenceBucketCollator(lambda lengths: lengths.max(), sequence_index=0, length_index=1, label_index=2)\n",
    "        test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False, collate_fn=test_collator)\n",
    "\n",
    "        model.eval() #set model to eval mode for test data\n",
    "        test_preds = np.zeros((len(test), output_dim))\n",
    "\n",
    "        for i, x_batch in enumerate(test_loader):\n",
    "            #print(x_batch)\n",
    "            data_param = x_batch[0][0]\n",
    "            lengths_param = x_batch[0][1]\n",
    "            y_pred = sigmoid(model(data_param, lengths_param).detach().cpu().numpy()) #feed data into model\n",
    "            test_preds[i * batch_size:(i+1) * batch_size, :] = y_pred #get test predictions\n",
    "\n",
    "        return test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialDropout(nn.Dropout2d):\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(2)    # (N, T, 1, K)\n",
    "        x = x.permute(0, 3, 2, 1)  # (N, K, 1, T)\n",
    "        #call the forward method in Dropout2d (super function specifies the subclass and instance)\n",
    "        x = super(SpatialDropout, self).forward(x)  # (N, K, 1, T), some features are masked\n",
    "        x = x.permute(0, 3, 2, 1)  # (N, T, 1, K)\n",
    "        x = x.squeeze(2)  # (N, T, K)\n",
    "        return x\n",
    "    \n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, embedding_matrix, num_aux_targets):\n",
    "        #call the init mthod in Module (super function specifies the subclass and instance)\n",
    "        super(NeuralNet, self).__init__() \n",
    "        embed_size = embedding_matrix.shape[1]\n",
    "        \n",
    "        self.embedding = nn.Embedding(max_features, embed_size)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.embedding_dropout = SpatialDropout(0.3)\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(embed_size, LSTM_UNITS, bidirectional=True, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(LSTM_UNITS * 2, LSTM_UNITS, bidirectional=True, batch_first=True)\n",
    "    \n",
    "        self.linear1 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n",
    "        self.linear2 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n",
    "        \n",
    "        self.linear_out = nn.Linear(DENSE_HIDDEN_UNITS, 1)\n",
    "        self.linear_aux_out = nn.Linear(DENSE_HIDDEN_UNITS, num_aux_targets)\n",
    "        \n",
    "    def forward(self, x, lengths=None):\n",
    "        h_embedding = self.embedding(x)\n",
    "        h_embedding = self.embedding_dropout(h_embedding)\n",
    "        \n",
    "        #first variable h_(lstm #) holds the output, _ is the (hidden state, cell state)\n",
    "        h_lstm1, _ = self.lstm1(h_embedding) \n",
    "        h_lstm2, _ = self.lstm2(h_lstm1)\n",
    "        \n",
    "        # global average pooling\n",
    "        avg_pool = torch.mean(h_lstm2, 1) #get the mean value of the first dimension in h_lstm2\n",
    "        # global max pooling\n",
    "        max_pool, _ = torch.max(h_lstm2, 1) #get the max value of the first dimension in h_lstm2\n",
    "        \n",
    "        h_conc = torch.cat((max_pool, avg_pool), 1)\n",
    "        h_conc_linear1  = F.relu(self.linear1(h_conc))\n",
    "        h_conc_linear2  = F.relu(self.linear2(h_conc))\n",
    "        \n",
    "        hidden = h_conc + h_conc_linear1 + h_conc_linear2\n",
    "        \n",
    "        result = self.linear_out(hidden)\n",
    "        aux_result = self.linear_aux_out(hidden)\n",
    "        out = torch.cat([result, aux_result], 1)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def preprocess(data):\n",
    "\n",
    "    punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~`\" + '\"\"â€œâ€â€™' + 'âˆžÎ¸Ã·Î±â€¢Ã âˆ’Î²âˆ…Â³Ï€â€˜â‚¹Â´Â°Â£â‚¬\\Ã—â„¢âˆšÂ²â€”â€“&'\n",
    "    def clean_special_chars(text, punct):\n",
    "        for p in punct:\n",
    "            text = text.replace(p, ' ')\n",
    "        return text\n",
    "\n",
    "    data = data.astype(str).apply(lambda x: clean_special_chars(x, punct))\n",
    "    return data\n",
    "'''\n",
    "symbols_to_isolate = '.,?!-;*\"â€¦:â€”()%#$&_/@ï¼¼ãƒ»Ï‰+=â€â€œ[]^â€“>\\\\Â°<~â€¢â‰ â„¢ËˆÊŠÉ’âˆžÂ§{}Â·Ï„Î±â¤â˜ºÉ¡|Â¢â†’Ì¶`â¥â”â”£â”«â”—ï¼¯â–ºâ˜…Â©â€•Éªâœ”Â®\\x96\\x92â—Â£â™¥âž¤Â´Â¹â˜•â‰ˆÃ·â™¡â—â•‘â–¬â€²É”Ëâ‚¬Û©Ûžâ€ Î¼âœ’âž¥â•â˜†ËŒâ—„Â½Ê»Ï€Î´Î·Î»ÏƒÎµÏÎ½Êƒâœ¬ï¼³ï¼µï¼°ï¼¥ï¼²ï¼©ï¼´â˜»Â±â™ÂµÂºÂ¾âœ“â—¾ØŸï¼Žâ¬…â„…Â»Ð’Ð°Ð²â£â‹…Â¿Â¬â™«ï¼£ï¼­Î²â–ˆâ–“â–’â–‘â‡’â­â€ºÂ¡â‚‚â‚ƒâ§â–°â–”â—žâ–€â–‚â–ƒâ–„â–…â–†â–‡â†™Î³Ì„â€³â˜¹âž¡Â«Ï†â…“â€žâœ‹ï¼šÂ¥Ì²Ì…Ìâˆ™â€›â—‡âœâ–·â“â—Â¶ËšË™ï¼‰ÑÐ¸Ê¿âœ¨ã€‚É‘\\x80â—•ï¼ï¼…Â¯âˆ’ï¬‚ï¬â‚Â²ÊŒÂ¼â´â„â‚„âŒ â™­âœ˜â•ªâ–¶â˜­âœ­â™ªâ˜”â˜ â™‚â˜ƒâ˜ŽâœˆâœŒâœ°â†â˜™â—‹â€£âš“å¹´âˆŽâ„’â–ªâ–™â˜â…›ï½ƒï½ï½“Ç€â„®Â¸ï½—â€šâˆ¼â€–â„³â„â†â˜¼â‹†Ê’âŠ‚ã€â…”Â¨Í¡à¹âš¾âš½Î¦Ã—Î¸ï¿¦ï¼Ÿï¼ˆâ„ƒâ©â˜®âš æœˆâœŠâŒâ­•â–¸â– â‡Œâ˜â˜‘âš¡â˜„Ç«â•­âˆ©â•®ï¼Œä¾‹ï¼žÊ•ÉÌ£Î”â‚€âœžâ”ˆâ•±â•²â–â–•â”ƒâ•°â–Šâ–‹â•¯â”³â”Šâ‰¥â˜’â†‘â˜É¹âœ…â˜›â™©â˜žï¼¡ï¼ªï¼¢â—”â—¡â†“â™€â¬†Ì±â„\\x91â €Ë¤â•šâ†ºâ‡¤âˆâœ¾â—¦â™¬Â³ã®ï½œï¼âˆµâˆ´âˆšÎ©Â¤â˜œâ–²â†³â–«â€¿â¬‡âœ§ï½ï½–ï½ï¼ï¼’ï¼ï¼˜ï¼‡â€°â‰¤âˆ•Ë†âšœâ˜'\n",
    "symbols_to_delete = '\\nðŸ•\\rðŸµðŸ˜‘\\xa0\\ue014\\t\\uf818\\uf04a\\xadðŸ˜¢ðŸ¶ï¸\\uf0e0ðŸ˜œðŸ˜ŽðŸ‘Š\\u200b\\u200eðŸ˜Ø¹Ø¯ÙˆÙŠÙ‡ØµÙ‚Ø£Ù†Ø§Ø®Ù„Ù‰Ø¨Ù…ØºØ±ðŸ˜ðŸ’–ðŸ’µÐ•ðŸ‘ŽðŸ˜€ðŸ˜‚\\u202a\\u202cðŸ”¥ðŸ˜„ðŸ»ðŸ’¥á´ÊÊ€á´‡É´á´…á´á´€á´‹Êœá´œÊŸá´›á´„á´˜Ê™Ò“á´Šá´¡É¢ðŸ˜‹ðŸ‘×©×œ×•××‘×™ðŸ˜±â€¼\\x81ã‚¨ãƒ³ã‚¸æ•…éšœ\\u2009ðŸšŒá´µÍžðŸŒŸðŸ˜ŠðŸ˜³ðŸ˜§ðŸ™€ðŸ˜ðŸ˜•\\u200fðŸ‘ðŸ˜®ðŸ˜ƒðŸ˜˜××¢×›×—ðŸ’©ðŸ’¯â›½ðŸš„ðŸ¼à®œðŸ˜–á´ ðŸš²â€ðŸ˜ŸðŸ˜ˆðŸ’ªðŸ™ðŸŽ¯ðŸŒ¹ðŸ˜‡ðŸ’”ðŸ˜¡\\x7fðŸ‘Œá¼á½¶Î®Î¹á½²Îºá¼€Î¯á¿ƒá¼´Î¾ðŸ™„ï¼¨ðŸ˜ \\ufeff\\u2028ðŸ˜‰ðŸ˜¤â›ºðŸ™‚\\u3000ØªØ­ÙƒØ³Ø©ðŸ‘®ðŸ’™ÙØ²Ø·ðŸ˜ðŸ¾ðŸŽ‰ðŸ˜ž\\u2008ðŸ¾ðŸ˜…ðŸ˜­ðŸ‘»ðŸ˜¥ðŸ˜”ðŸ˜“ðŸ½ðŸŽ†ðŸ»ðŸ½ðŸŽ¶ðŸŒºðŸ¤”ðŸ˜ª\\x08â€‘ðŸ°ðŸ‡ðŸ±ðŸ™†ðŸ˜¨ðŸ™ƒðŸ’•ð˜Šð˜¦ð˜³ð˜¢ð˜µð˜°ð˜¤ð˜ºð˜´ð˜ªð˜§ð˜®ð˜£ðŸ’—ðŸ’šåœ°ç„è°·ÑƒÐ»ÐºÐ½ÐŸÐ¾ÐÐðŸ¾ðŸ•ðŸ˜†×”ðŸ”—ðŸš½æ­Œèˆžä¼ŽðŸ™ˆðŸ˜´ðŸ¿ðŸ¤—ðŸ‡ºðŸ‡¸Ð¼Ï…Ñ‚Ñ•â¤µðŸ†ðŸŽƒðŸ˜©\\u200aðŸŒ ðŸŸðŸ’«ðŸ’°ðŸ’ŽÑÐ¿Ñ€Ð´\\x95ðŸ–ðŸ™…â›²ðŸ°ðŸ¤ðŸ‘†ðŸ™Œ\\u2002ðŸ’›ðŸ™ðŸ‘€ðŸ™ŠðŸ™‰\\u2004Ë¢áµ’Ê³Ê¸á´¼á´·á´ºÊ·áµ—Ê°áµ‰áµ˜\\x13ðŸš¬ðŸ¤“\\ue602ðŸ˜µÎ¬Î¿ÏŒÏ‚Î­á½¸×ª×ž×“×£× ×¨×š×¦×˜ðŸ˜’ÍðŸ†•ðŸ‘…ðŸ‘¥ðŸ‘„ðŸ”„ðŸ”¤ðŸ‘‰ðŸ‘¤ðŸ‘¶ðŸ‘²ðŸ”›ðŸŽ“\\uf0b7\\uf04c\\x9f\\x10æˆéƒ½ðŸ˜£âºðŸ˜ŒðŸ¤‘ðŸŒðŸ˜¯ÐµÑ…ðŸ˜²á¼¸á¾¶á½ðŸ’žðŸš“ðŸ””ðŸ“šðŸ€ðŸ‘\\u202dðŸ’¤ðŸ‡\\ue613å°åœŸè±†ðŸ¡â”â‰\\u202fðŸ‘ ã€‹à¤•à¤°à¥à¤®à¤¾ðŸ‡¹ðŸ‡¼ðŸŒ¸è”¡è‹±æ–‡ðŸŒžðŸŽ²ãƒ¬ã‚¯ã‚µã‚¹ðŸ˜›å¤–å›½äººå…³ç³»Ð¡Ð±ðŸ’‹ðŸ’€ðŸŽ„ðŸ’œðŸ¤¢ÙÙŽÑŒÑ‹Ð³Ñä¸æ˜¯\\x9c\\x9dðŸ—‘\\u2005ðŸ’ƒðŸ“£ðŸ‘¿à¼¼ã¤à¼½ðŸ˜°á¸·Ð—Ð·â–±Ñ†ï¿¼ðŸ¤£å–æ¸©å“¥åŽè®®ä¼šä¸‹é™ä½ å¤±åŽ»æ‰€æœ‰çš„é’±åŠ æ‹¿å¤§åç¨Žéª—å­ðŸãƒ„ðŸŽ…\\x85ðŸºØ¢Ø¥Ø´Ø¡ðŸŽµðŸŒŽÍŸá¼”æ²¹åˆ«å…‹ðŸ¤¡ðŸ¤¥ðŸ˜¬ðŸ¤§Ð¹\\u2003ðŸš€ðŸ¤´Ê²ÑˆÑ‡Ð˜ÐžÐ Ð¤Ð”Ð¯ÐœÑŽÐ¶ðŸ˜ðŸ–‘á½á½»Ïç‰¹æ®Šä½œæˆ¦ç¾¤Ñ‰ðŸ’¨åœ†æ˜Žå›­×§â„ðŸˆðŸ˜ºðŸŒâá»‡ðŸ”ðŸ®ðŸðŸ†ðŸ‘ðŸŒ®ðŸŒ¯ðŸ¤¦\\u200dð“’ð“²ð“¿ð“µì•ˆì˜í•˜ì„¸ìš”Ð–Ñ™ÐšÑ›ðŸ€ðŸ˜«ðŸ¤¤á¿¦æˆ‘å‡ºç”Ÿåœ¨äº†å¯ä»¥è¯´æ™®é€šè¯æ±‰è¯­å¥½æžðŸŽ¼ðŸ•ºðŸ¸ðŸ¥‚ðŸ—½ðŸŽ‡ðŸŽŠðŸ†˜ðŸ¤ ðŸ‘©ðŸ–’ðŸšªå¤©ä¸€å®¶âš²\\u2006âš­âš†â¬­â¬¯â–æ–°âœ€â•ŒðŸ‡«ðŸ‡·ðŸ‡©ðŸ‡ªðŸ‡®ðŸ‡¬ðŸ‡§ðŸ˜·ðŸ‡¨ðŸ‡¦Ð¥Ð¨ðŸŒ\\x1fæ€é¸¡ç»™çŒ´çœ‹Êð—ªð—µð—²ð—»ð˜†ð—¼ð˜‚ð—¿ð—®ð—¹ð—¶ð˜‡ð—¯ð˜ð—°ð˜€ð˜…ð—½ð˜„ð—±ðŸ“ºÏ–\\u2000Ò¯Õ½á´¦áŽ¥Ò»Íº\\u2007Õ°\\u2001É©ï½™ï½…àµ¦ï½ŒÆ½ï½ˆð“ð¡ðžð«ð®ððšðƒðœð©ð­ð¢ð¨ð§Æ„á´¨×Ÿá‘¯à»Î¤á§à¯¦Ð†á´‘Üð¬ð°ð²ð›ð¦ð¯ð‘ð™ð£ð‡ð‚ð˜ðŸŽÔœÐ¢á—žà±¦ã€”áŽ«ð³ð”ð±ðŸ”ðŸ“ð…ðŸ‹ï¬ƒðŸ’˜ðŸ’“Ñ‘ð˜¥ð˜¯ð˜¶ðŸ’ðŸŒ‹ðŸŒ„ðŸŒ…ð™¬ð™–ð™¨ð™¤ð™£ð™¡ð™®ð™˜ð™ ð™šð™™ð™œð™§ð™¥ð™©ð™ªð™—ð™žð™ð™›ðŸ‘ºðŸ·â„‹ð€ð¥ðªðŸš¶ð™¢á¼¹ðŸ¤˜Í¦ðŸ’¸Ø¬íŒ¨í‹°ï¼·ð™‡áµ»ðŸ‘‚ðŸ‘ƒÉœðŸŽ«\\uf0a7Ð‘Ð£Ñ–ðŸš¢ðŸš‚àª—à«àªœàª°àª¾àª¤à«€á¿†ðŸƒð“¬ð“»ð“´ð“®ð“½ð“¼â˜˜ï´¾Ì¯ï´¿â‚½\\ue807ð‘»ð’†ð’ð’•ð’‰ð’“ð’–ð’‚ð’ð’…ð’”ð’Žð’—ð’ŠðŸ‘½ðŸ˜™\\u200cÐ›â€’ðŸŽ¾ðŸ‘¹âŽŒðŸ’â›¸å…¬å¯“å…»å® ç‰©å—ðŸ„ðŸ€ðŸš‘ðŸ¤·æ“ç¾Žð’‘ð’šð’ð‘´ðŸ¤™ðŸ’æ¬¢è¿Žæ¥åˆ°é˜¿æ‹‰æ–¯×¡×¤ð™«ðŸˆð’Œð™Šð™­ð™†ð™‹ð™ð˜¼ð™…ï·»ðŸ¦„å·¨æ”¶èµ¢å¾—ç™½é¬¼æ„¤æ€’è¦ä¹°é¢áº½ðŸš—ðŸ³ðŸðŸðŸ–ðŸ‘ðŸ•ð’„ðŸ—ð ð™„ð™ƒðŸ‘‡é”Ÿæ–¤æ‹·ð—¢ðŸ³ðŸ±ðŸ¬â¦ãƒžãƒ«ãƒãƒ‹ãƒãƒ­æ ªå¼ç¤¾â›·í•œêµ­ì–´ã„¸ã…“ë‹ˆÍœÊ–ð˜¿ð™”â‚µð’©â„¯ð’¾ð“ð’¶ð“‰ð“‡ð“Šð“ƒð“ˆð“…â„´ð’»ð’½ð“€ð“Œð’¸ð“Žð™Î¶ð™Ÿð˜ƒð—ºðŸ®ðŸ­ðŸ¯ðŸ²ðŸ‘‹ðŸ¦Šå¤šä¼¦ðŸ½ðŸŽ»ðŸŽ¹â›“ðŸ¹ðŸ·ðŸ¦†ä¸ºå’Œä¸­å‹è°Šç¥è´ºä¸Žå…¶æƒ³è±¡å¯¹æ³•å¦‚ç›´æŽ¥é—®ç”¨è‡ªå·±çŒœæœ¬ä¼ æ•™å£«æ²¡ç§¯å”¯è®¤è¯†åŸºç£å¾’æ›¾ç»è®©ç›¸ä¿¡è€¶ç¨£å¤æ´»æ­»æ€ªä»–ä½†å½“ä»¬èŠäº›æ”¿æ²»é¢˜æ—¶å€™æˆ˜èƒœå› åœ£æŠŠå…¨å ‚ç»“å©šå­©ææƒ§ä¸”æ —è°“è¿™æ ·è¿˜â™¾ðŸŽ¸ðŸ¤•ðŸ¤’â›‘ðŸŽæ‰¹åˆ¤æ£€è®¨ðŸðŸ¦ðŸ™‹ðŸ˜¶ì¥ìŠ¤íƒ±íŠ¸ë¤¼ë„ì„ìœ ê°€ê²©ì¸ìƒì´ê²½ì œí™©ì„ë µê²Œë§Œë“¤ì§€ì•Šë¡ìž˜ê´€ë¦¬í•´ì•¼í•©ë‹¤ìºë‚˜ì—ì„œëŒ€ë§ˆì´ˆì™€í™”ì•½ê¸ˆì˜í’ˆëŸ°ì„±ë¶„ê°ˆë•ŒëŠ”ë°˜ë“œì‹œí—ˆëœì‚¬ìš©ðŸ”«ðŸ‘å‡¸á½°ðŸ’²ðŸ—¯ð™ˆá¼Œð’‡ð’ˆð’˜ð’ƒð‘¬ð‘¶ð•¾ð–™ð–—ð–†ð–Žð–Œð–ð–•ð–Šð–”ð–‘ð–‰ð–“ð–ð–œð–žð–šð–‡ð•¿ð–˜ð–„ð–›ð–’ð–‹ð–‚ð•´ð–Ÿð–ˆð•¸ðŸ‘‘ðŸš¿ðŸ’¡çŸ¥å½¼ç™¾\\uf005ð™€ð’›ð‘²ð‘³ð‘¾ð’‹ðŸ’ðŸ˜¦ð™’ð˜¾ð˜½ðŸð˜©ð˜¨á½¼á¹‘ð‘±ð‘¹ð‘«ð‘µð‘ªðŸ‡°ðŸ‡µðŸ‘¾á“‡á’§á”­áƒá§á¦á‘³á¨á“ƒá“‚á‘²á¸á‘­á‘Žá“€á£ðŸ„ðŸŽˆðŸ”¨ðŸŽðŸ¤žðŸ¸ðŸ’ŸðŸŽ°ðŸŒðŸ›³ç‚¹å‡»æŸ¥ç‰ˆðŸ­ð‘¥ð‘¦ð‘§ï¼®ï¼§ðŸ‘£\\uf020ã£ðŸ‰Ñ„ðŸ’­ðŸŽ¥ÎžðŸ´ðŸ‘¨ðŸ¤³ðŸ¦\\x0bðŸ©ð‘¯ð’’ðŸ˜—ðŸðŸ‚ðŸ‘³ðŸ—ðŸ•‰ðŸ²Ú†ÛŒð‘®ð—•ð—´ðŸ’êœ¥â²£â²ðŸ‘â°é‰„ãƒªäº‹ä»¶Ñ—ðŸ’Šã€Œã€\\uf203\\uf09a\\uf222\\ue608\\uf202\\uf099\\uf469\\ue607\\uf410\\ue600ç‡»è£½ã‚·è™šå½å±ç†å±ˆÐ“ð‘©ð‘°ð’€ð‘ºðŸŒ¤ð—³ð—œð—™ð—¦ð—§ðŸŠá½ºá¼ˆá¼¡Ï‡á¿–Î›â¤ðŸ‡³ð’™ÏˆÕÕ´Õ¥Õ¼Õ¡ÕµÕ«Õ¶Ö€Ö‚Õ¤Õ±å†¬è‡³á½€ð’ðŸ”¹ðŸ¤šðŸŽð‘·ðŸ‚ðŸ’…ð˜¬ð˜±ð˜¸ð˜·ð˜ð˜­ð˜“ð˜–ð˜¹ð˜²ð˜«Ú©Î’ÏŽðŸ’¢ÎœÎŸÎÎ‘Î•ðŸ‡±â™²ðˆâ†´ðŸ’’âŠ˜È»ðŸš´ðŸ–•ðŸ–¤ðŸ¥˜ðŸ“ðŸ‘ˆâž•ðŸš«ðŸŽ¨ðŸŒ‘ðŸ»ðŽððŠð‘­ðŸ¤–ðŸŽŽðŸ˜¼ðŸ•·ï½‡ï½’ï½Žï½”ï½‰ï½„ï½•ï½†ï½‚ï½‹ðŸ°ðŸ‡´ðŸ‡­ðŸ‡»ðŸ‡²ð—žð—­ð—˜ð—¤ðŸ‘¼ðŸ“‰ðŸŸðŸ¦ðŸŒˆðŸ”­ã€ŠðŸŠðŸ\\uf10aáƒšÚ¡ðŸ¦\\U0001f92f\\U0001f92aðŸ¡ðŸ’³á¼±ðŸ™‡ð—¸ð—Ÿð— ð—·ðŸ¥œã•ã‚ˆã†ãªã‚‰ðŸ”¼'\n",
    "\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "nltk_tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "\n",
    "isolate_dict = {ord(c):f' {c} ' for c in symbols_to_isolate}\n",
    "remove_dict = {ord(c):f'' for c in symbols_to_delete}\n",
    "\n",
    "\n",
    "def handle_punctuation(x):\n",
    "    x = x.translate(remove_dict)\n",
    "    x = x.translate(isolate_dict)\n",
    "    return x\n",
    "\n",
    "def handle_contractions(x):\n",
    "    x = nltk_tokenizer.tokenize(x)\n",
    "    return x\n",
    "\n",
    "def fix_quote(x):\n",
    "    x = [x_[1:] if x_.startswith(\"'\") else x_ for x_ in x]\n",
    "    x = ' '.join(x)\n",
    "    return x\n",
    "\n",
    "def preprocess(x):\n",
    "    x = handle_punctuation(x)\n",
    "    x = handle_contractions(x)\n",
    "    x = fix_quote(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 2.178173303604126 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/curtis/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:20: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "train = pd.read_hdf('../input/train.h5')\n",
    "test = pd.read_hdf('../input/test.h5')\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "#pd.read_csv(\"P00000001-ALL.csv\", nrows=20)\n",
    "#train = pd.read_hdf('../input/train.h5')\n",
    "#test = pd.read_hdf('../input/test.h5')\n",
    "#tqdm.pandas()\n",
    "\n",
    "identity_columns = [\n",
    "    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n",
    "    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n",
    "\n",
    "x_train = train['comment_text'].apply(lambda x:preprocess(x))\n",
    "y_train = np.where(train['target'] >= 0.5, 1, 0)\n",
    "num_train_data = y_train.shape[0]\n",
    "y_train_identity = np.where(train[identity_columns] >= 0.5, 1, 0)\n",
    "y_aux_train = train[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']]\n",
    "x_test = test['comment_text'].apply(lambda x:preprocess(x))\n",
    "y_aux_train = y_aux_train.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                       int64\n",
       "target                                 float64\n",
       "comment_text                            object\n",
       "severe_toxicity                        float64\n",
       "obscene                                float64\n",
       "identity_attack                        float64\n",
       "insult                                 float64\n",
       "threat                                 float64\n",
       "asian                                  float64\n",
       "atheist                                float64\n",
       "bisexual                               float64\n",
       "black                                  float64\n",
       "buddhist                               float64\n",
       "christian                              float64\n",
       "female                                 float64\n",
       "heterosexual                           float64\n",
       "hindu                                  float64\n",
       "homosexual_gay_or_lesbian              float64\n",
       "intellectual_or_learning_disability    float64\n",
       "jewish                                 float64\n",
       "latino                                 float64\n",
       "male                                   float64\n",
       "muslim                                 float64\n",
       "other_disability                       float64\n",
       "other_gender                           float64\n",
       "other_race_or_ethnicity                float64\n",
       "other_religion                         float64\n",
       "other_sexual_orientation               float64\n",
       "physical_disability                    float64\n",
       "psychiatric_or_mental_illness          float64\n",
       "transgender                            float64\n",
       "white                                  float64\n",
       "created_date                            object\n",
       "publication_id                           int64\n",
       "parent_id                              float64\n",
       "article_id                               int64\n",
       "rating                                  object\n",
       "funny                                    int64\n",
       "wow                                      int64\n",
       "sad                                      int64\n",
       "likes                                    int64\n",
       "disagree                                 int64\n",
       "sexual_explicit                        float64\n",
       "identity_annotator_count                 int64\n",
       "toxicity_annotator_count                 int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmax_features = None\\n\\n#Create the dictionary of all words that exist in our data\\nnlp = spacy.load(\"en_core_web_lg\", disable=[\\'parser\\',\\'ner\\',\\'tagger\\'])\\ntext_list = pd.concat([x_train, x_test])\\nnlp.vocab.add_flag(lambda s: s.lower() in spacy.lang.en.stop_words.STOP_WORDS, spacy.attrs.IS_STOP)\\nword_dict = {}\\nlemma_dict = {}\\nword_index = 1\\ndocs = nlp.pipe(text_list, n_threads = 2)\\nword_sequences = []\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "max_features = None\n",
    "\n",
    "#Create the dictionary of all words that exist in our data\n",
    "nlp = spacy.load(\"en_core_web_lg\", disable=['parser','ner','tagger'])\n",
    "text_list = pd.concat([x_train, x_test])\n",
    "nlp.vocab.add_flag(lambda s: s.lower() in spacy.lang.en.stop_words.STOP_WORDS, spacy.attrs.IS_STOP)\n",
    "word_dict = {}\n",
    "lemma_dict = {}\n",
    "word_index = 1\n",
    "docs = nlp.pipe(text_list, n_threads = 2)\n",
    "word_sequences = []\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nstart_time = time.time()\\nfor doc in tqdm(docs): #one doc is one comment(row)\\n    #print(count)\\n    word_seq = []\\n    for token in doc:\\n        if (token.text not in word_dict) and (token.pos_ is not \"PUNCT\"):\\n            word_dict[token.text] = word_index\\n            word_index += 1\\n            lemma_dict[token.text] = token.lemma_\\n        if token.pos_ is not \"PUNCT\":\\n            word_seq.append(word_dict[token.text])\\n    word_sequences.append(word_seq)\\n    #count+= 1\\n\\nprint(\"--- %s seconds ---\" % (time.time() - start_time))\\ndel docs\\ndel text_list\\ngc.collect()\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create dictionary of word mapping to integers as wel as lemma dictionary\n",
    "#count = 1\n",
    "'''\n",
    "start_time = time.time()\n",
    "for doc in tqdm(docs): #one doc is one comment(row)\n",
    "    #print(count)\n",
    "    word_seq = []\n",
    "    for token in doc:\n",
    "        if (token.text not in word_dict) and (token.pos_ is not \"PUNCT\"):\n",
    "            word_dict[token.text] = word_index\n",
    "            word_index += 1\n",
    "            lemma_dict[token.text] = token.lemma_\n",
    "        if token.pos_ is not \"PUNCT\":\n",
    "            word_seq.append(word_dict[token.text])\n",
    "    word_sequences.append(word_seq)\n",
    "    #count+= 1\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "del docs\n",
    "del text_list\n",
    "gc.collect()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 400000\n",
    "tokenizer = text.Tokenizer(num_words = max_features, filters='',lower=False)\n",
    "tokenizer.fit_on_texts(list(x_train) + list(x_test))\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_test = tokenizer.texts_to_sequences(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#max_features = max_features or len(word_dict) + 1\n",
    "max_features = max_features or len(tokenizer.word_index) + 1\n",
    "max_features #number of unique words there are in the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n unknown words (crawl):  222943\n",
      "--- 80.3553524017334 seconds ---\n",
      "Size:  (487814, 300)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "#crawl_matrix, unknown_words_crawl = build_matrix(word_dict, lemma_dict, CRAWL_EMBEDDING_PATH)\n",
    "crawl_matrix, unknown_words_crawl = build_matrix(tokenizer.word_index, CRAWL_EMBEDDING_PATH)\n",
    "print('n unknown words (crawl): ', len(unknown_words_crawl))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print('Size: ', crawl_matrix.shape)\n",
    "del unknown_words_crawl\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n unknown words (glove):  225246\n",
      "--- 90.67239451408386 seconds ---\n",
      "Size:  (487814, 300)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "#glove_matrix, unknown_words_glove = build_matrix(word_dict, lemma_dict, GLOVE_EMBEDDING_PATH)\n",
    "glove_matrix, unknown_words_glove = build_matrix(tokenizer.word_index, GLOVE_EMBEDDING_PATH)\n",
    "print('n unknown words (glove): ', len(unknown_words_glove))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print('Size: ', glove_matrix.shape)\n",
    "del unknown_words_glove\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = np.concatenate([crawl_matrix, glove_matrix], axis=-1)\n",
    "\n",
    "del crawl_matrix\n",
    "del glove_matrix\n",
    "#del word_dict\n",
    "#del lemma_dict\n",
    "#del WORDS\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JigsawEvaluator:\n",
    "\n",
    "    def __init__(self, y_true, y_identity, power=-5, overall_model_weight=0.25):\n",
    "        self.y = y_true\n",
    "        self.y_i = y_identity\n",
    "        self.n_subgroups = self.y_i.shape[1]\n",
    "        self.power = power\n",
    "        self.overall_model_weight = overall_model_weight\n",
    "\n",
    "    @staticmethod\n",
    "    def _compute_auc(y_true, y_pred):\n",
    "        #print(\"Here: \", y_true)\n",
    "        #print(y_pred)\n",
    "        try:\n",
    "            return roc_auc_score(y_true, y_pred)\n",
    "        except ValueError:\n",
    "            return np.nan\n",
    "\n",
    "    def _compute_subgroup_auc(self, i, y_pred):\n",
    "        mask = self.y_i[:, i] == 1\n",
    "        #print(self.y)\n",
    "        return self._compute_auc(self.y[mask], y_pred[mask])\n",
    "\n",
    "    def _compute_bpsn_auc(self, i, y_pred):\n",
    "        mask = self.y_i[:, i] + self.y == 1\n",
    "        return self._compute_auc(self.y[mask], y_pred[mask])\n",
    "\n",
    "    def _compute_bnsp_auc(self, i, y_pred):\n",
    "        mask = self.y_i[:, i] + self.y != 1\n",
    "        return self._compute_auc(self.y[mask], y_pred[mask])\n",
    "\n",
    "    def compute_bias_metrics_for_model(self, y_pred):\n",
    "        #print(y_pred)\n",
    "        records = np.zeros((3, self.n_subgroups))\n",
    "        for i in range(self.n_subgroups):\n",
    "            #print(y_pred)\n",
    "            records[0, i] = self._compute_subgroup_auc(i, y_pred)\n",
    "            records[1, i] = self._compute_bpsn_auc(i, y_pred)\n",
    "            records[2, i] = self._compute_bnsp_auc(i, y_pred)\n",
    "        return records\n",
    "\n",
    "    def _calculate_overall_auc(self, y_pred):\n",
    "        return roc_auc_score(self.y, y_pred)\n",
    "\n",
    "    def _power_mean(self, array):\n",
    "        total = sum(np.power(array, self.power))\n",
    "        return np.power(total / len(array), 1 / self.power)\n",
    "\n",
    "    def get_final_metric(self, y_pred):\n",
    "        bias_metrics = self.compute_bias_metrics_for_model(y_pred)\n",
    "        bias_score = np.average([\n",
    "            self._power_mean(bias_metrics[0]),\n",
    "            self._power_mean(bias_metrics[1]),\n",
    "            self._power_mean(bias_metrics[2])\n",
    "        ])\n",
    "        overall_score = self.overall_model_weight * self._calculate_overall_auc(y_pred)\n",
    "        bias_score = (1 - self.overall_model_weight) * bias_score\n",
    "        return overall_score + bias_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1804874\n",
      "torch.Size([1804874])\n",
      "188\n",
      "tensor(713)\n"
     ]
    }
   ],
   "source": [
    "#x_train = word_sequences[:num_train_data]\n",
    "#x_test = word_sequences[num_train_data:]\n",
    "lengths = torch.from_numpy(np.array([len(x) for x in x_train]))\n",
    "test_lengths = torch.from_numpy(np.array([len(x) for x in x_test]))\n",
    "print(len(x_train))\n",
    "print(lengths.shape)\n",
    "maxlen = int(np.percentile(lengths, 95)) \n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "print(maxlen)\n",
    "print(lengths.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1804874, 188)\n",
      "(1804874,)\n",
      "1804874\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(num_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: PRELIM FILTER\n",
      "=> Saving a new best\n",
      "Epoch 1/6 \t loss=0.1110 \t time=652.35s\n",
      "=> Saving a new best\n",
      "Epoch 2/6 \t loss=0.1040 \t time=654.59s\n",
      "=> Saving a new best\n",
      "Epoch 3/6 \t loss=0.1027 \t time=655.05s\n",
      "=> Saving a new best\n",
      "Epoch 4/6 \t loss=0.1018 \t time=655.46s\n",
      "=> Saving a new best\n",
      "Epoch 5/6 \t loss=0.1013 \t time=655.24s\n",
      "=> Saving a new best\n",
      "Epoch 6/6 \t loss=0.1010 \t time=654.92s\n",
      "====================== END OF FILTER =============================\n"
     ]
    }
   ],
   "source": [
    "#Here, we use one model that filters out all the easy predictions\n",
    "x_train_torch = torch.tensor(x_train, dtype=torch.long).cuda()\n",
    "y_train_torch = torch.tensor(np.hstack([y_train[:, np.newaxis], y_aux_train]), dtype=torch.float32).cuda()\n",
    "train_dataset = data.TensorDataset(x_train_torch, lengths ,y_train_torch)\n",
    "\n",
    "print('Model: PRELIM FILTER')\n",
    "seed_everything(1234)\n",
    "\n",
    "model = NeuralNet(embedding_matrix, y_aux_train.shape[-1])\n",
    "model.cuda()\n",
    "\n",
    "#training using training and validation set\n",
    "model = train_model(model, train_dataset, train_dataset, output_dim=y_train_torch.shape[-1], \n",
    "                         loss_fn=nn.BCEWithLogitsLoss(reduction='mean'))\n",
    "\n",
    "\n",
    "#prediction on entire train set (actual predictions to be submitted)\n",
    "pred = predict(model, train_dataset, output_dim=y_train_torch.shape[-1])[:,0]\n",
    "\n",
    "        \n",
    "print('====================== END OF FILTER =============================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CLEAR \n",
    "del x_train_torch\n",
    "del y_train_torch\n",
    "del train_dataset\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#AND GET DISTRIBUTIONS OF PREDICTIONS\n",
    "plt.hist(pred)\n",
    "plt.title(\"Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1% of comments have a score of less than 3.337556288897759e-05\n",
      "2% of comments have a score of less than 4.199579620035365e-05\n",
      "3% of comments have a score of less than 4.931899733492173e-05\n",
      "4% of comments have a score of less than 5.6134143669623875e-05\n",
      "5% of comments have a score of less than 6.281218884396367e-05\n",
      "6% of comments have a score of less than 6.945829692995176e-05\n",
      "7% of comments have a score of less than 7.616755683557131e-05\n",
      "8% of comments have a score of less than 8.302498172270134e-05\n",
      "9% of comments have a score of less than 9.01532027637586e-05\n",
      "10% of comments have a score of less than 9.745782881509513e-05\n",
      "11% of comments have a score of less than 0.00010503281540877652\n",
      "12% of comments have a score of less than 0.00011289473506622015\n",
      "13% of comments have a score of less than 0.00012119509367039427\n",
      "14% of comments have a score of less than 0.000129816260014195\n",
      "15% of comments have a score of less than 0.00013898321994929575\n",
      "16% of comments have a score of less than 0.00014843305805698035\n",
      "17% of comments have a score of less than 0.00015830218180781232\n",
      "18% of comments have a score of less than 0.00016869838436832653\n",
      "19% of comments have a score of less than 0.0001797901655663736\n",
      "20% of comments have a score of less than 0.00019136882037855684\n",
      "21% of comments have a score of less than 0.00020364543757750653\n",
      "22% of comments have a score of less than 0.00021688510052626954\n",
      "23% of comments have a score of less than 0.00023077148696756925\n",
      "24% of comments have a score of less than 0.0002456313173752278\n",
      "25% of comments have a score of less than 0.00026124185387743637\n",
      "26% of comments have a score of less than 0.00027816378336865456\n",
      "27% of comments have a score of less than 0.00029609446588438007\n",
      "28% of comments have a score of less than 0.00031524597550742334\n",
      "29% of comments have a score of less than 0.000335820855689235\n",
      "30% of comments have a score of less than 0.0003576844756025821\n",
      "31% of comments have a score of less than 0.00038118807773571464\n",
      "32% of comments have a score of less than 0.00040673262439668175\n",
      "33% of comments have a score of less than 0.00043424226285424086\n",
      "34% of comments have a score of less than 0.0004636009287787605\n",
      "35% of comments have a score of less than 0.0004953260271577163\n",
      "36% of comments have a score of less than 0.0005290697165764868\n",
      "37% of comments have a score of less than 0.0005662781768478453\n",
      "38% of comments have a score of less than 0.0006071355368476361\n",
      "39% of comments have a score of less than 0.000650989524438046\n",
      "40% of comments have a score of less than 0.0006986698950640861\n",
      "41% of comments have a score of less than 0.0007506113027920946\n",
      "42% of comments have a score of less than 0.0008079618343617767\n",
      "43% of comments have a score of less than 0.0008698991301935167\n",
      "44% of comments have a score of less than 0.00093832200858742\n",
      "45% of comments have a score of less than 0.0010133484669495373\n",
      "46% of comments have a score of less than 0.0010961663094349206\n",
      "47% of comments have a score of less than 0.0011870523157995192\n",
      "48% of comments have a score of less than 0.001287220912054181\n",
      "49% of comments have a score of less than 0.0013969161291606724\n",
      "50% of comments have a score of less than 0.0015207676333375275\n",
      "51% of comments have a score of less than 0.0016554303781595079\n",
      "52% of comments have a score of less than 0.0018052565120160599\n",
      "53% of comments have a score of less than 0.001973765352740885\n",
      "54% of comments have a score of less than 0.0021600405871868133\n",
      "55% of comments have a score of less than 0.0023713659029453993\n",
      "56% of comments have a score of less than 0.002611720450222494\n",
      "57% of comments have a score of less than 0.002880919578019526\n",
      "58% of comments have a score of less than 0.003186239572241902\n",
      "59% of comments have a score of less than 0.003527613640762852\n",
      "60% of comments have a score of less than 0.00391936376690865\n",
      "61% of comments have a score of less than 0.004363956525921822\n",
      "62% of comments have a score of less than 0.004876531586050988\n",
      "63% of comments have a score of less than 0.005473214695230126\n",
      "64% of comments have a score of less than 0.006159290671348572\n",
      "65% of comments have a score of less than 0.006937866914086043\n",
      "66% of comments have a score of less than 0.007842073366045954\n",
      "67% of comments have a score of less than 0.008876413106918335\n",
      "68% of comments have a score of less than 0.01008838940411807\n",
      "69% of comments have a score of less than 0.011530974945053445\n",
      "70% of comments have a score of less than 0.013219328690320196\n",
      "71% of comments have a score of less than 0.01519688793458043\n",
      "72% of comments have a score of less than 0.017535351291298882\n",
      "73% of comments have a score of less than 0.020323971882462502\n",
      "74% of comments have a score of less than 0.02359050258994103\n",
      "75% of comments have a score of less than 0.02752486662939191\n",
      "76% of comments have a score of less than 0.032231080532073964\n",
      "77% of comments have a score of less than 0.03775924619287252\n",
      "78% of comments have a score of less than 0.04442781560122966\n",
      "79% of comments have a score of less than 0.05237089507281782\n",
      "80% of comments have a score of less than 0.06180671602487566\n",
      "81% of comments have a score of less than 0.07307182282209397\n",
      "82% of comments have a score of less than 0.0864164972305296\n",
      "83% of comments have a score of less than 0.10217970736324758\n",
      "84% of comments have a score of less than 0.1204836279153823\n",
      "85% of comments have a score of less than 0.14252778440713884\n",
      "86% of comments have a score of less than 0.16891480416059498\n",
      "87% of comments have a score of less than 0.19938909336924554\n",
      "88% of comments have a score of less than 0.23511009633541108\n",
      "89% of comments have a score of less than 0.27660134643316264\n",
      "90% of comments have a score of less than 0.3248975664377212\n",
      "91% of comments have a score of less than 0.3805968904495241\n",
      "92% of comments have a score of less than 0.4444474744796755\n",
      "93% of comments have a score of less than 0.518796628117562\n",
      "94% of comments have a score of less than 0.6031810271739957\n",
      "95% of comments have a score of less than 0.6980702638626073\n",
      "96% of comments have a score of less than 0.7990966200828549\n",
      "97% of comments have a score of less than 0.8934608978033068\n",
      "98% of comments have a score of less than 0.9580717134475707\n",
      "99% of comments have a score of less than 0.9900480794906616\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 100):\n",
    "    thres = np.percentile(pred, i)\n",
    "    print('{}% of comments have a score of less than {}'.format(i, thres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.006159290671348572\n",
      "[False False False ... False  True False]\n"
     ]
    }
   ],
   "source": [
    "#FILTER OUT ALL THE EASY COMMENTS (64th percentile)\n",
    "threshold = np.percentile(pred, 64)\n",
    "print(threshold)\n",
    "keep_index = (pred > threshold)\n",
    "print(keep_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train[np.where(keep_index)]\n",
    "y_train = y_train[np.where(keep_index)]\n",
    "y_train_identity = y_train_identity[np.where(keep_index)]\n",
    "y_aux_train = y_aux_train[np.where(keep_index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(649754, 188)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2732"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del pred\n",
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model  0\n",
      "=> Saving a new best\n",
      "Epoch 1/6 \t loss=0.2486 \t time=154.56s\n",
      "=> Saving a new best\n",
      "Epoch 2/6 \t loss=0.2304 \t time=154.48s\n",
      "=> Saving a new best\n",
      "Epoch 3/6 \t loss=0.2273 \t time=153.99s\n",
      "=> Saving a new best\n",
      "Epoch 4/6 \t loss=0.2256 \t time=154.70s\n",
      "=> Saving a new best\n",
      "Epoch 5/6 \t loss=0.2246 \t time=155.08s\n",
      "=> Saving a new best\n",
      "Epoch 6/6 \t loss=0.2239 \t time=154.63s\n",
      "\n",
      "Model  1\n",
      "=> Saving a new best\n",
      "Epoch 1/6 \t loss=0.2487 \t time=154.10s\n",
      "=> Saving a new best\n",
      "Epoch 2/6 \t loss=0.2305 \t time=154.77s\n",
      "=> Saving a new best\n",
      "Epoch 3/6 \t loss=0.2273 \t time=154.67s\n",
      "=> Saving a new best\n",
      "Epoch 4/6 \t loss=0.2256 \t time=154.55s\n",
      "=> Saving a new best\n",
      "Epoch 5/6 \t loss=0.2246 \t time=154.15s\n",
      "=> Saving a new best\n",
      "Epoch 6/6 \t loss=0.2239 \t time=154.66s\n",
      "\n",
      "Kaggle Score:  0.8834263304540407\n",
      "ROC score:  0.9027343674849325\n",
      "=============End-of-Fold================\n",
      "Model  0\n",
      "=> Saving a new best\n",
      "Epoch 1/6 \t loss=0.2483 \t time=154.50s\n",
      "=> Saving a new best\n",
      "Epoch 2/6 \t loss=0.2303 \t time=154.51s\n",
      "=> Saving a new best\n",
      "Epoch 3/6 \t loss=0.2271 \t time=154.18s\n",
      "=> Saving a new best\n",
      "Epoch 4/6 \t loss=0.2255 \t time=154.60s\n",
      "=> Saving a new best\n",
      "Epoch 5/6 \t loss=0.2244 \t time=154.55s\n",
      "=> Saving a new best\n",
      "Epoch 6/6 \t loss=0.2238 \t time=154.48s\n",
      "\n",
      "Model  1\n",
      "=> Saving a new best\n",
      "Epoch 1/6 \t loss=0.2486 \t time=154.07s\n",
      "=> Saving a new best\n",
      "Epoch 2/6 \t loss=0.2304 \t time=154.60s\n",
      "=> Saving a new best\n",
      "Epoch 3/6 \t loss=0.2273 \t time=154.72s\n",
      "=> Saving a new best\n",
      "Epoch 4/6 \t loss=0.2256 \t time=154.22s\n",
      "=> Saving a new best\n",
      "Epoch 5/6 \t loss=0.2245 \t time=154.32s\n",
      "=> Saving a new best\n",
      "Epoch 6/6 \t loss=0.2238 \t time=154.72s\n",
      "\n",
      "Kaggle Score:  0.8830907120400693\n",
      "ROC score:  0.9021457969706173\n",
      "=============End-of-Fold================\n",
      "Model  0\n",
      "=> Saving a new best\n",
      "Epoch 1/6 \t loss=0.2482 \t time=154.51s\n",
      "=> Saving a new best\n",
      "Epoch 2/6 \t loss=0.2304 \t time=154.56s\n",
      "=> Saving a new best\n",
      "Epoch 3/6 \t loss=0.2273 \t time=154.37s\n",
      "=> Saving a new best\n",
      "Epoch 4/6 \t loss=0.2255 \t time=154.80s\n",
      "=> Saving a new best\n",
      "Epoch 5/6 \t loss=0.2246 \t time=154.79s\n",
      "=> Saving a new best\n",
      "Epoch 6/6 \t loss=0.2238 \t time=154.62s\n",
      "\n",
      "Model  1\n",
      "=> Saving a new best\n",
      "Epoch 1/6 \t loss=0.2489 \t time=154.00s\n",
      "=> Saving a new best\n",
      "Epoch 2/6 \t loss=0.2304 \t time=154.68s\n",
      "=> Saving a new best\n",
      "Epoch 3/6 \t loss=0.2271 \t time=154.74s\n",
      "=> Saving a new best\n",
      "Epoch 4/6 \t loss=0.2255 \t time=154.62s\n",
      "=> Saving a new best\n",
      "Epoch 5/6 \t loss=0.2245 \t time=154.30s\n",
      "=> Saving a new best\n",
      "Epoch 6/6 \t loss=0.2238 \t time=154.77s\n",
      "\n",
      "Kaggle Score:  0.8826232776677687\n",
      "ROC score:  0.9015850315231676\n",
      "=============End-of-Fold================\n",
      "Model  0\n",
      "=> Saving a new best\n",
      "Epoch 1/6 \t loss=0.2486 \t time=154.62s\n",
      "=> Saving a new best\n",
      "Epoch 2/6 \t loss=0.2307 \t time=154.59s\n",
      "=> Saving a new best\n",
      "Epoch 3/6 \t loss=0.2275 \t time=154.31s\n",
      "=> Saving a new best\n",
      "Epoch 4/6 \t loss=0.2258 \t time=154.69s\n",
      "=> Saving a new best\n",
      "Epoch 5/6 \t loss=0.2247 \t time=154.73s\n",
      "=> Saving a new best\n",
      "Epoch 6/6 \t loss=0.2241 \t time=154.57s\n",
      "\n",
      "Model  1\n",
      "=> Saving a new best\n",
      "Epoch 1/6 \t loss=0.2488 \t time=153.95s\n",
      "=> Saving a new best\n",
      "Epoch 2/6 \t loss=0.2305 \t time=154.59s\n",
      "=> Saving a new best\n",
      "Epoch 3/6 \t loss=0.2274 \t time=154.14s\n",
      "=> Saving a new best\n",
      "Epoch 4/6 \t loss=0.2257 \t time=154.40s\n",
      "=> Saving a new best\n",
      "Epoch 5/6 \t loss=0.2246 \t time=154.23s\n",
      "=> Saving a new best\n",
      "Epoch 6/6 \t loss=0.2238 \t time=154.51s\n",
      "\n",
      "Kaggle Score:  0.8820354748891639\n",
      "ROC score:  0.9027634988634995\n",
      "=============End-of-Fold================\n",
      "Model  0\n",
      "=> Saving a new best\n",
      "Epoch 1/6 \t loss=0.2483 \t time=154.40s\n",
      "=> Saving a new best\n",
      "Epoch 2/6 \t loss=0.2305 \t time=154.41s\n",
      "=> Saving a new best\n",
      "Epoch 3/6 \t loss=0.2274 \t time=154.10s\n",
      "=> Saving a new best\n",
      "Epoch 4/6 \t loss=0.2257 \t time=154.67s\n",
      "=> Saving a new best\n",
      "Epoch 5/6 \t loss=0.2246 \t time=154.61s\n",
      "=> Saving a new best\n",
      "Epoch 6/6 \t loss=0.2240 \t time=154.54s\n",
      "\n",
      "Model  1\n",
      "=> Saving a new best\n",
      "Epoch 1/6 \t loss=0.2486 \t time=153.91s\n",
      "=> Saving a new best\n",
      "Epoch 2/6 \t loss=0.2306 \t time=154.38s\n",
      "=> Saving a new best\n",
      "Epoch 3/6 \t loss=0.2273 \t time=154.53s\n",
      "=> Saving a new best\n",
      "Epoch 4/6 \t loss=0.2256 \t time=154.44s\n",
      "=> Saving a new best\n",
      "Epoch 5/6 \t loss=0.2246 \t time=154.19s\n",
      "=> Saving a new best\n",
      "Epoch 6/6 \t loss=0.2240 \t time=154.57s\n",
      "\n",
      "Kaggle Score:  0.8829231106298552\n",
      "ROC score:  0.9029899907603467\n",
      "=============End-of-Fold================\n",
      "Time:  9504.520491838455\n",
      "Final Kaggle Score:  0.8826374502680023\n",
      "Final ROC score:  0.9023287111848801\n"
     ]
    }
   ],
   "source": [
    "\n",
    "all_val_preds = []\n",
    "all_test_preds = []\n",
    "num_splits = 5\n",
    "\n",
    "#Add in K fold \n",
    "random_state = 2019\n",
    "\n",
    "#K fold splits\n",
    "splits = list(StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=random_state).split(x_train,y_train))\n",
    "\n",
    "#final validation predictions\n",
    "final_val_preds = np.zeros((x_train.shape[0]))\n",
    "\n",
    "#final test predictions to be stored in this var\n",
    "final_test_preds = np.zeros((x_test.shape[0]))\n",
    "\n",
    "start_time = time.time()\n",
    "for fold in range(num_splits):\n",
    "    tr_ind, val_ind = splits[fold]\n",
    "    all_val_preds = []\n",
    "    all_test_preds = []\n",
    "    #print('Training set size: ', len(tr_ind))\n",
    "    #print('Val set size: ', len(val_ind))\n",
    "    x_training = x_train[tr_ind]\n",
    "    y_training = y_train[tr_ind]\n",
    "    y_aux_training = y_aux_train[tr_ind]\n",
    "    \n",
    "    x_val = x_train[val_ind]\n",
    "    y_val = y_train[val_ind]\n",
    "    y_aux_val = y_aux_train[val_ind]\n",
    "    \n",
    "    \n",
    "    \n",
    "    x_train_torch = torch.tensor(x_training, dtype=torch.long).cuda()\n",
    "    x_val_torch = torch.tensor(x_val, dtype=torch.long).cuda()\n",
    "    y_train_torch = torch.tensor(np.hstack([y_training[:, np.newaxis], y_aux_training]), dtype=torch.float32).cuda()\n",
    "    y_val_torch = torch.tensor(np.hstack([y_val[:, np.newaxis], y_aux_val]), dtype=torch.float32).cuda()\n",
    "    \n",
    "    x_test_torch = torch.tensor(x_test, dtype=torch.long).cuda()\n",
    "    \n",
    "    ###\n",
    "    \n",
    "    #test_dataset = data.TensorDataset(x_test_torch, test_lengths)\n",
    "    #train_dataset = data.TensorDataset(x_train_torch, lengths, y_train_torch)\n",
    "    #val_dataset = data.TensorDataset(x_val_torch)\n",
    "\n",
    "    #train_collator = SequenceBucketCollator(lambda lengths: lengths.max(), sequence_index=0, length_index=1, label_index=2)\n",
    "    #test_collator = SequenceBucketCollator(lambda lengths: lengths.max(), sequence_index=0, length_index=1)\n",
    "    \n",
    "    ####\n",
    "    train_dataset = data.TensorDataset(x_train_torch, lengths[tr_ind], y_train_torch)\n",
    "    val_dataset = data.TensorDataset(x_val_torch, lengths[val_ind], y_val_torch)\n",
    "    test_dataset = data.TensorDataset(x_test_torch, test_lengths)\n",
    "    \n",
    "    #temp_dataset = data.Subset(train_dataset, indices=[0, 1])\n",
    "\n",
    "    for model_idx in range(NUM_MODELS):\n",
    "        print('Model ', model_idx)\n",
    "        seed_everything(1234 + model_idx)\n",
    "\n",
    "        model = NeuralNet(embedding_matrix, y_aux_train.shape[-1])\n",
    "        model.cuda()\n",
    "\n",
    "        #training using training and validation set\n",
    "        model = train_model(model, train_dataset, val_dataset, output_dim=y_train_torch.shape[-1], loss_fn=nn.BCEWithLogitsLoss(reduction='mean'))\n",
    "        \n",
    "        #prediction on validation set (used for score measurement)\n",
    "        val_pred = predict(model, val_dataset, output_dim=y_train_torch.shape[-1], pred_type=\"val\") #val preds on the val split\n",
    "        all_val_preds.append(val_pred)\n",
    "        #print(len(val_pred))\n",
    "        \n",
    "        #prediction on entire test set (actual predictions to be submitted)\n",
    "        test_pred = predict(model, test_dataset, output_dim=y_train_torch.shape[-1], pred_type=\"test\")\n",
    "        all_test_preds.append(test_pred)\n",
    "        \n",
    "        print()\n",
    "        \n",
    "    #average validation prediction amongst all models\n",
    "    avg_val = np.mean(all_val_preds, axis=0)[:, 0] #will be printed out per split\n",
    "    final_val_preds[val_ind] += avg_val\n",
    "    \n",
    "    avg_test = np.mean(all_test_preds, axis=0)[:, 0]\n",
    "    \n",
    "    final_test_preds += avg_test\n",
    "\n",
    "    y_true = y_train[val_ind] #true scores for this validation set\n",
    "    y_identity = y_train_identity[val_ind] #true scores for the identity groups for this validation set\n",
    "    evaluator = JigsawEvaluator(y_true, y_identity)\n",
    "    auc_score = evaluator.get_final_metric(avg_val)\n",
    "\n",
    "    roc_score = roc_auc_score(y_train[val_ind], avg_val)\n",
    "    print('Kaggle Score: ', auc_score)\n",
    "    print('ROC score: ', roc_score)\n",
    "    \n",
    "    del x_train_torch\n",
    "    del x_val_torch\n",
    "    del y_train_torch\n",
    "    del y_val_torch\n",
    "    del x_test_torch\n",
    "    del train_dataset\n",
    "    del val_dataset\n",
    "    del test_dataset\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print('=============End-of-Fold================')\n",
    "    \n",
    "end_time = time.time()\n",
    "print('Time: ', end_time - start_time)\n",
    "\n",
    "#Final combined score\n",
    "y_true = y_train\n",
    "y_identity = y_train_identity\n",
    "evaluator = JigsawEvaluator(y_true, y_identity)\n",
    "auc_score = evaluator.get_final_metric(final_val_preds)\n",
    "print('Final Kaggle Score: ', auc_score)\n",
    "print('Final ROC score: ', roc_auc_score(y_train, final_val_preds))\n",
    "\n",
    "#average test predictions AGAIN this time by number of splits\n",
    "final_test_preds /= num_splits\n",
    "#print(final_test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7000000</td>\n",
       "      <td>0.018381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7000001</td>\n",
       "      <td>0.004410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7000002</td>\n",
       "      <td>0.014784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7000003</td>\n",
       "      <td>0.009222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7000004</td>\n",
       "      <td>0.977801</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  prediction\n",
       "0  7000000    0.018381\n",
       "1  7000001    0.004410\n",
       "2  7000002    0.014784\n",
       "3  7000003    0.009222\n",
       "4  7000004    0.977801"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.DataFrame.from_dict({\n",
    "    'id': test['id'],\n",
    "    'prediction': final_test_preds\n",
    "})\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
