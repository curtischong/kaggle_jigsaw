{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as of time of writing may 25, this takes 5 minutes to gen features. still slow :/\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, time, gc, pickle, random\n",
    "from tqdm._tqdm_notebook import tqdm_notebook as tqdm\n",
    "from keras.preprocessing import text, sequence\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from torch.nn import functional as F\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel\n",
    "from bert_embedding import BertEmbedding\n",
    "import apex # used for 16 bit\n",
    "import re\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import mxnet as mx # used for GPU\n",
    "#for getting num good and bad words\n",
    "from wordcloud import STOPWORDS\n",
    "from collections import defaultdict\n",
    "import operator\n",
    "import swifter # speed up feature gen - multiple cores\n",
    "\n",
    "# Logging for BERT\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "string.printable\n",
    "ascii_chars = string.printable\n",
    "ascii_chars += \" áéíóúàèìòùâêîôûäëïöüñõç\"\n",
    "\n",
    "#checks if a string of text contains any nonenglish characters (excluding punctuations, spanish, and french characters)\n",
    "def contains_non_english(text):\n",
    "    if all(char in ascii_chars for char in text):\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "#clean non english characters from string of text\n",
    "def remove_non_english(text):\n",
    "    return ''.join(filter(lambda x: x in ascii_chars, text))\n",
    "\n",
    "\n",
    "def get_first_word(word):\n",
    "    if(type(word) != \"float\"):\n",
    "        return word.split(\" \")[0]\n",
    "    return \"-1\"\n",
    "\n",
    "def get_caps_vs_length(df):\n",
    "    df['caps_vs_length'] = df['num_caps'].divide(df['num_chars'])\n",
    "    df.loc[~np.isfinite(df['caps_vs_length']), 'caps_vs_length'] = 0\n",
    "    \n",
    "\"\"\"    mask = (df['num_chars'] != 0)\n",
    "    df_valid = df[mask]\n",
    "    \n",
    "    df.loc[mask, 'caps_vs_length'] = df_valid['num_caps'] / df_valid['num_chars']\"\"\"\n",
    "\n",
    "def get_unique_word_over_num_words(df):\n",
    "    df['unique_word_over_num_words'] = df['num_unique_words'].divide(df['num_words'])\n",
    "    df.loc[~np.isfinite(df['unique_word_over_num_words']), 'unique_word_over_num_words'] = 0\n",
    "    \n",
    "def get_avg_word_len(df):\n",
    "    df['avg_word_len'] = df['total_word_length'].divide(df['num_words'])\n",
    "    df.loc[~np.isfinite(df['avg_word_len']), 'avg_word_len'] = 0\n",
    "\n",
    "def get_avg_unique_word_len(df):\n",
    "    df['avg_unique_word_len'] = df['total_unique_word_length'].divide(df['num_unique_words'])\n",
    "    df.loc[~np.isfinite(df['avg_unique_word_len']), 'avg_unique_word_len'] = 0\n",
    "    \n",
    "def calc_max_word_len(sentence):\n",
    "    maxLen = 0\n",
    "    for word in sentence:\n",
    "        maxLen = max(maxLen, len(word))\n",
    "    return maxLen\n",
    "\n",
    "def calc_min_word_len(sentence):\n",
    "    minLen = 999999\n",
    "    for word in sentence:\n",
    "        minLen = min(minLen, len(word))\n",
    "    return minLen\n",
    "\n",
    "def calc_total_word_len(sentence):\n",
    "    cnt = 0\n",
    "    for x in sentence:\n",
    "        cnt+=len(x)\n",
    "    return cnt\n",
    "\n",
    "def calc_total_unique_word_len(sentence):\n",
    "    words = set(sentence)\n",
    "    return calc_total_word_len(words)\n",
    "\n",
    "#removes all single characters except for \"I\" and \"a\"\n",
    "def remove_singles(text):\n",
    "    return ' '.join( [w for w in text.split() if ((len(w)>1) or (w.lower() == \"i\") or (w.lower() == \"a\"))] )\n",
    "    \n",
    "#combines multiple whitespaces into single\n",
    "def clean_text(x):\n",
    "    x = str(x)\n",
    "    for punct in \"&/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~`\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&':\n",
    "        x = x.replace(punct, '')\n",
    "    x = re.sub( '\\s+', ' ', x).strip()\n",
    "    \n",
    "    \n",
    "# Text cleaning\n",
    "# TODO: speed up this func\n",
    "def pad_chars(text,punct):\n",
    "    for p in punct:\n",
    "        text = re.sub('(?<=\\w)([!?,])', r' \\1', text)\n",
    "    return text\n",
    "    \n",
    "symbols_iv = \"\"\"?,./-()\"$=…*&+′[ɾ̃]%:^\\xa0\\\\{}–“”;!<`®ạ°#²|~√_α→>—£，。´×@π÷？ʿ€の↑∞ʻ℅в•−а年！∈∩⊆§℃θ±≤͡⁴™си≠∂³ி½△¿¼∆≥⇒¬∨∫▾Ω＾γµº♭ー̂ɔ∑εντσ日Γ∪φβ¹∘¨″⅓ɑː✅✓（）∠«»்ுλ∧∀،＝ɨʋδɒ¸☹μΔʃɸηΣ₅₆◦·ВΦ☺❤♨✌≡ʌʊா≈⁰‛：ﬁ„¾ρ⟨⟩˂⅔≅－＞¢⁸ʒは⬇♀؟¡⋅ɪ₁₂ɤ◌ʱ、▒ْ；☉＄∴✏ωɹ̅।ـ☝♏̉̄♡₄∼́̀⁶⁵¦¶ƒˆ‰©¥∅・ﾟ⊥ª†ℕ│ɡ∝♣／☁✔❓∗➡ℝ位⎛⎝¯⎞⎠↓ɐ∇⋯˚⁻ˈ₃⊂˜̸̵̶̷̴̡̲̳̱̪̗̣̖̎̿͂̓̑̐̌̾̊̕\\x92\"\"\"        \n",
    "\n",
    "def split_off_symbols_iv(x):\n",
    "    for punct in symbols_iv:\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    return x\n",
    "    \n",
    "def neutrailize_bad_words(train,test):\n",
    "    train1_df = train[train[\"target\"]==1]\n",
    "    train0_df = train[train[\"target\"]==0]\n",
    "\n",
    "    ## custom function for ngram generation ##\n",
    "    def generate_ngrams(text, n_gram=1):\n",
    "        token = [token for token in text.lower().split(\" \") if token != \"\" if token not in STOPWORDS]\n",
    "        ngrams = zip(*[token[i:] for i in range(n_gram)])\n",
    "        return [\" \".join(ngram) for ngram in ngrams]\n",
    "\n",
    "    freq_dict_bad = defaultdict(int)\n",
    "    for sent in train1_df[\"comment_text\"]:\n",
    "        for word in generate_ngrams(sent):\n",
    "            freq_dict_bad[word] += 1\n",
    "    freq_dict_bad = dict(freq_dict_bad)\n",
    "\n",
    "    freq_dict_good = defaultdict(int)\n",
    "    for sent in train0_df[\"comment_text\"]:\n",
    "        for word in generate_ngrams(sent):\n",
    "            freq_dict_good[word] += 1\n",
    "    freq_dict_good = dict(freq_dict_good)\n",
    "\n",
    "    bad_words = sorted(freq_dict_bad, key=freq_dict_bad.get, reverse=True)[:1000]\n",
    "    good_words = sorted(freq_dict_good, key=freq_dict_good.get, reverse=True)[:1000]\n",
    "\n",
    "    print(\"--- Generating num_bad_words\")\n",
    "    train[\"num_bad_words\"] = train[\"comment_text\"].map(lambda x: num_bad_words(x))\n",
    "    test[\"num_bad_words\"] = test[\"comment_text\"].map(lambda x: num_bad_words(x))\n",
    "\n",
    "    print(\"--- Generating num_good_words\")\n",
    "    train[\"num_good_words\"] = train[\"comment_text\"].map(lambda x: num_good_words(x))\n",
    "    test[\"num_good_words\"] = test[\"comment_text\"].map(lambda x: num_good_words(x))\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    '''\n",
    "    Credit goes to https://www.kaggle.com/gpreda/jigsaw-fast-compact-solution\n",
    "    '''\n",
    "    \n",
    "    # Feature generation\n",
    "    \n",
    "    def gen_feats(df):\n",
    "        start_time = time.time()\n",
    "            \n",
    "        print(\"--- Generating non_eng\")\n",
    "        df[\"non_eng\"] = df[\"comment_text\"].swifter.apply(lambda x: contains_non_english(x))\n",
    "\n",
    "        print(\"--- Generating first_word\")\n",
    "        df[\"first_word\"] = df[\"comment_text\"].swifter.apply(lambda x: get_first_word(x))\n",
    "\n",
    "        print(\"--- Generating num_chars (num chars)\")\n",
    "        df['num_chars'] = df['comment_text'].swifter.apply(len)\n",
    "\n",
    "        print(\"--- Generating num_caps\")\n",
    "        df['num_caps'] = df['comment_text'].swifter.apply(lambda comment: sum(1 for c in comment if c.isupper()))\n",
    "\n",
    "        print(\"--- Generating caps_vs_length\")\n",
    "        get_caps_vs_length(df)\n",
    "\n",
    "        print(\"--- Generating num_exclamation_marks\")\n",
    "        df['num_exclamation_marks'] = df['comment_text'].apply(lambda comment: comment.count('!'))\n",
    "\n",
    "        print(\"--- Generating num_question_marks\")\n",
    "        df['num_question_marks'] = df['comment_text'].apply(lambda comment: comment.count('?'))\n",
    "\n",
    "        print(\"--- Generating num_punctuation\")\n",
    "        df['num_punctuation'] = df['comment_text'].apply(lambda comment: sum(comment.count(w) for w in '.,;:'))\n",
    "\n",
    "        print(\"--- Generating num_symbols\")\n",
    "        df['num_symbols'] = df['comment_text'].apply(lambda comment: sum(comment.count(w) for w in '*&$%'))\n",
    "\n",
    "        print(\"--- Generating num_words\")\n",
    "        df['num_words'] = df['comment_text'].swifter.apply(lambda comment: len(re.sub(r'[^\\w\\s]','',comment).split(\" \")))\n",
    "\n",
    "        print(\"--- Generating num_unique_words\")\n",
    "        df['num_unique_words'] = df['comment_text'].swifter.apply(lambda comment: len(set(w for w in comment.split())))\n",
    "\n",
    "        print(\"--- Generating unique_word_over_num_words\")\n",
    "        get_unique_word_over_num_words(df)\n",
    "\n",
    "        print(\"--- Generating num_smilies\")\n",
    "        df['num_smilies'] = df['comment_text'].apply(lambda comment: sum(comment.count(w) for w in (':-)', ':)', ';-)', ';)')))\n",
    "\n",
    "        print(\"--- Generating num_sentences\")\n",
    "        df['num_sentences'] = df['comment_text'].swifter.apply(lambda comment: len(re.split(r'[.!?]+', comment)))\n",
    "\n",
    "        print(\"--- Generating max_word_len\")\n",
    "        df['max_word_len'] = df['comment_text'].swifter.apply(lambda comment: calc_max_word_len(re.sub(r'[^\\w\\s]','',comment).split(\" \")))\n",
    "        \n",
    "        print(\"--- Generating min_word_len\")\n",
    "        df['max_word_len'] = df['comment_text'].swifter.apply(lambda comment: calc_min_word_len(re.sub(r'[^\\w\\s]','',comment).split(\" \")))\n",
    "        \n",
    "        print(\"--- Generating total_word_length (num of chars in words)\")\n",
    "        df['total_word_length'] = df['comment_text'].swifter.apply(lambda comment: calc_total_word_len(re.sub(r'[^\\w\\s]','',comment).split(\" \")))\n",
    "        \n",
    "        print(\"--- Generating avg_word_len\")\n",
    "        get_avg_word_len(df)\n",
    "        \n",
    "        print(\"--- Generating total_unique_word_length (num of chars in words)\")\n",
    "        df['total_unique_word_length'] = df['comment_text'].swifter.apply(lambda comment: calc_total_unique_word_len(re.sub(r'[^\\w\\s]','',comment).split(\" \")))\n",
    "        \n",
    "        print(\"--- Generating avg_unique_word_len\")\n",
    "        get_avg_unique_word_len(df)\n",
    "        \n",
    "        print(\"--- Finished Gen Feats. Took %s seconds ---\" % (time.time() - start_time))\n",
    "        \n",
    "\n",
    "    def cleanText(df):\n",
    "        start_time = time.time()\n",
    "        df['comment_text'] = df['comment_text'].swifter.apply(lambda x: split_off_symbols_iv(x)) #increase score\n",
    "        \"\"\"print(\"--- cleaning text\")\n",
    "        df[\"comment_text\"] = df[\"comment_text\"].apply(lambda x: clean_text(x))\n",
    "\n",
    "        print(\"--- remove single characters\")\n",
    "        df[\"comment_text\"] = df[\"comment_text\"].apply(lambda x: remove_singles(x))\n",
    "\n",
    "        print(\"--- cleaning numbers\")\n",
    "        df[\"comment_text\"] = df[\"comment_text\"].apply(lambda x: clean_numbers(x))\n",
    "\n",
    "        print(\"--- cleaning misspellings\")\n",
    "        df[\"comment_text\"] = df[\"comment_text\"].apply(lambda x: replace_typical_misspell(x))\n",
    "\n",
    "        print(\"--- filling missing values\")\n",
    "        #clean chinese, korean, japanese characters\n",
    "        print('cleaning characters')\n",
    "        df[\"comment_text\"] = df[\"comment_text\"].map(lambda x: remove_non_english(x))\n",
    "        \n",
    "        ## fill up the missing values\n",
    "        df[\"comment_text\"].fillna(\"\").values\"\"\"\n",
    "        print(\"--- Finished cleaning text. Took %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "        \n",
    "    gen_feats(data)\n",
    "    #data[\"comment_text\"] = data[\"comment_text\"].astype(str).apply(lambda x: pad_chars(x, punct))\n",
    "    #cleanText(data)\n",
    "    # print(\"--- Neutralizing bad words\")\n",
    "    # neutrailize_bad_words(train,test)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading Files\n",
      "--- Finished Loading 2.345226287841797\n",
      "Using med data\n",
      "--- Preprocess\n",
      "--- Generating non_eng\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70f25eb9329345abaacca749ce8e59b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=10000, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generating first_word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df0bdc065f154a7e88e64add2ae4c425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=10000, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generating num_chars (num chars)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4c35b4941684425a004f6efd72f741c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=10000, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generating num_caps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19645ca5c99243d3806667ec3425bed8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=10000, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generating caps_vs_length\n",
      "--- Generating num_exclamation_marks\n",
      "--- Generating num_question_marks\n",
      "--- Generating num_punctuation\n",
      "--- Generating num_symbols\n",
      "--- Generating num_words\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "debcca6f9ebd40159a043b98dbe7d4e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=10000, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generating num_unique_words\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6b318c2f4f84670b26c98a0d33cdb0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=10000, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generating unique_word_over_num_words\n",
      "--- Generating num_smilies\n",
      "--- Generating num_sentences\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "370babeabdeb40adaf0dd94c741442af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=10000, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generating max_word_len\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be351c5387d741aaa5b9ca5c4d9da92f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=10000, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generating min_word_len\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94c29c700d434930935291a716de04f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=10000, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generating total_word_length (num of chars in words)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eac217f9f23043b38b3546a3d8f8fab2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=10000, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generating avg_word_len\n",
      "--- Generating total_unique_word_length (num of chars in words)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c60efe0814d4b4c846519035b1c5dc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=10000, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generating avg_unique_word_len\n",
      "--- Finished Gen Feats. Took 1.4963512420654297 seconds ---\n",
      "--- Generating non_eng\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4ab66071d6f43e8aeba091aee924657",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=10000, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generating first_word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78c7b7321ed14127b951b5775530a112",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=10000, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generating num_chars (num chars)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87be25a859af463291408b9a26cc1c9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=10000, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generating num_caps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b172edb694e04fefb816981e2eb3b355",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=10000, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generating caps_vs_length\n",
      "--- Generating num_exclamation_marks\n",
      "--- Generating num_question_marks\n",
      "--- Generating num_punctuation\n",
      "--- Generating num_symbols\n",
      "--- Generating num_words\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33c67517735246eabcff38573307537d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=10000, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generating num_unique_words\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98b3407065f5406e939a96866eec2213",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=10000, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generating unique_word_over_num_words\n",
      "--- Generating num_smilies\n",
      "--- Generating num_sentences\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f493241b803744038c4aa147a7735535",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=10000, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generating max_word_len\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58b51c46e57344e2a2cef0669407d61c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=10000, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generating min_word_len\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d33293e1def4c079c58a0ca8dcbfd83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=10000, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generating total_word_length (num of chars in words)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1729566042e749cc9aa0ff7247bd31b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=10000, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generating avg_word_len\n",
      "--- Generating total_unique_word_length (num of chars in words)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6da417a57714e1bac101aaef19f54d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=10000, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generating avg_unique_word_len\n",
      "--- Finished Gen Feats. Took 1.449253797531128 seconds ---\n",
      "--- Finished preprocess 2.9458351135253906\n"
     ]
    }
   ],
   "source": [
    "DATA_SIZE = \"med\"\n",
    "print(\"--- Loading Files\")\n",
    "start_time = time.time()\n",
    "train = pd.read_hdf('../input/train.h5')\n",
    "test = pd.read_hdf('../input/test.h5')\n",
    "print(\"--- Finished Loading %s\" % (time.time() - start_time))\n",
    "\n",
    "if DATA_SIZE == \"small\":\n",
    "    print(\"Using small data\")\n",
    "    train = train[:100]\n",
    "    test = test[:100]\n",
    "elif DATA_SIZE == \"med\":\n",
    "    print(\"Using med data\")\n",
    "    train = train[:10000]\n",
    "    test = test[:10000]\n",
    "\n",
    "print(\"--- Preprocess\")\n",
    "start_time = time.time()\n",
    "preprocess(train)\n",
    "preprocess(test)\n",
    "print(\"--- Finished preprocess %s\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_perc = 0.9\n",
    "\n",
    "train_len = len(train)\n",
    "split_point = int(split_perc*train_len)\n",
    "\n",
    "#first_word omitted\n",
    "features = ['non_eng','num_chars','num_caps','caps_vs_length','num_exclamation_marks','num_question_marks','num_punctuation','num_symbols','num_words','num_unique_words','unique_word_over_num_words','num_smilies','num_sentences','max_word_len','max_word_len','total_word_length','avg_word_len','total_unique_word_length','avg_unique_word_len']\n",
    "#x_train = train[:split_point][features]\n",
    "#x_test = test[split_point:][features]\n",
    "\n",
    "#y_train = train[:split_point]['target']\n",
    "#y_aux_train = [[] for x in range(len(x_train))]\n",
    "\n",
    "x_train = train[features]\n",
    "y_train = np.where(train['target'] >= 0.5, 1, 0)\n",
    "\n",
    "x_test = test[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceBucketCollator():\n",
    "    def __init__(self, choose_length, sequence_index, length_index, label_index=None):\n",
    "        self.choose_length = choose_length\n",
    "        self.sequence_index = sequence_index\n",
    "        self.length_index = length_index\n",
    "        self.label_index = label_index\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        batch = [torch.stack(x) for x in list(zip(*batch))]\n",
    "        \n",
    "        sequences = batch[self.sequence_index]\n",
    "        lengths = batch[self.length_index]\n",
    "        \n",
    "        length = self.choose_length(lengths)\n",
    "        mask = torch.arange(start=maxlen, end=0, step=-1) < length\n",
    "        padded_sequences = sequences[:, mask]\n",
    "        \n",
    "        batch[self.sequence_index] = padded_sequences\n",
    "        \n",
    "        if self.label_index is not None:\n",
    "            return [x for i, x in enumerate(batch) if i != self.label_index], batch[self.label_index]\n",
    "    \n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def train_model(model, train, test, loss_fn, output_dim, lr=0.001,\n",
    "                batch_size=512, n_epochs=4, n_epochs_embed=2,\n",
    "                enable_checkpoint_ensemble=True):\n",
    "    \n",
    "    \n",
    "    param_lrs = [{'params': param, 'lr': lr} for param in model.parameters()]\n",
    "    optimizer = torch.optim.Adam(param_lrs, lr=lr)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda epoch: 0.6 ** epoch)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n",
    "    all_test_preds = []\n",
    "    checkpoint_weights = [2 ** epoch for epoch in range(n_epochs)]\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        model.train() #set model to train mode\n",
    "        avg_loss = 0.\n",
    "        \n",
    "        for data in tqdm(train_loader, disable=False):\n",
    "            \n",
    "            #training loop\n",
    "            x_batch = data[:-1]\n",
    "            y_batch = data[-1]\n",
    "\n",
    "            y_pred = model(*x_batch)  #feed data into model          \n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            \n",
    "            #calculate error and adjust model params\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item() / len(train_loader) #gets the loss per epoch\n",
    "        \n",
    "        \n",
    "        model.eval() #set model to eval mode for test data\n",
    "        test_preds = np.zeros((len(test), output_dim))\n",
    "    \n",
    "        for i, x_batch in enumerate(test_loader):\n",
    "            y_pred = sigmoid(model(*x_batch).detach().cpu().numpy()) #feed data into model\n",
    "\n",
    "            test_preds[i * batch_size:(i+1) * batch_size, :] = y_pred #get test predictions\n",
    "        \n",
    "        #test_preds has the predictions for the entire test set now\n",
    "        all_test_preds.append(test_preds) #append predictions to the record of all past predictions\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print('Epoch {}/{} \\t loss={:.4f} \\t time={:.2f}s'.format(\n",
    "              epoch + 1, n_epochs, avg_loss, elapsed_time))\n",
    "\n",
    "    return model\n",
    "\n",
    "def predict(model, test, output_dim, batch_size=512):\n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    model.eval() #set model to eval mode for test data\n",
    "    test_preds = np.zeros((len(test), output_dim))\n",
    "    \n",
    "    for i, x_batch in enumerate(test_loader):\n",
    "        y_pred = sigmoid(model(*x_batch).detach().cpu().numpy()) #feed data into model\n",
    "        test_preds[i * batch_size:(i+1) * batch_size, :] = y_pred #get test predictions\n",
    "        \n",
    "    return test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7999])\n",
      "Model  0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "749dffdd49bf450fbb37733a5e6b620d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=16), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b139f2f64b149b294b7cec12ee8078b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=16), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Target size (torch.Size([512])) must be the same as input size (torch.Size([512, 2]))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-127-e0a97fde5793>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     56\u001b[0m             model = train_model(model, train_dataset, val_dataset, output_dim=y_train_torch.shape[-1], \n\u001b[0;32m---> 57\u001b[0;31m                                  loss_fn=nn.BCEWithLogitsLoss(reduction='mean'))\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-117-f0a04644e7e5>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train, test, loss_fn, output_dim, lr, batch_size, n_epochs, n_epochs_embed, enable_checkpoint_ensemble)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#feed data into model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    616\u001b[0m                                                   \u001b[0mpos_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 617\u001b[0;31m                                                   reduction=self.reduction)\n\u001b[0m\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   2160\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2161\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Target size ({}) must be the same as input size ({})\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Target size (torch.Size([512])) must be the same as input size (torch.Size([512, 2]))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-127-e0a97fde5793>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;31m#training using training and validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             model = train_model(model, train_dataset, val_dataset, output_dim=y_train_torch.shape[-1], \n\u001b[0;32m---> 62\u001b[0;31m                                  loss_fn=nn.BCEWithLogitsLoss(reduction='mean'))\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;31m#prediction on validation set (used for score measurement)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-117-f0a04644e7e5>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train, test, loss_fn, output_dim, lr, batch_size, n_epochs, n_epochs_embed, enable_checkpoint_ensemble)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#feed data into model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;31m#calculate error and adjust model params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    615\u001b[0m                                                   \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m                                                   \u001b[0mpos_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 617\u001b[0;31m                                                   reduction=self.reduction)\n\u001b[0m\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   2159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2160\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2161\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Target size ({}) must be the same as input size ({})\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2163\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction_enum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Target size (torch.Size([512])) must be the same as input size (torch.Size([512, 2]))"
     ]
    }
   ],
   "source": [
    "all_val_preds = []\n",
    "all_test_preds = []\n",
    "num_splits = 5\n",
    "NUM_MODELS = 5\n",
    "\n",
    "#Add in K fold \n",
    "random_state = 2019\n",
    "\n",
    "#K fold splits\n",
    "splits = list(StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=random_state).split(x_train,y_train))\n",
    "\n",
    "#final validation predictions\n",
    "final_val_preds = np.zeros((x_train.shape[0]))\n",
    "\n",
    "#final test predictions to be stored in this var\n",
    "final_test_preds = np.zeros((x_test.shape[0]))\n",
    "\n",
    "start_time = time.time()\n",
    "for fold in range(num_splits):\n",
    "    tr_ind, val_ind = splits[fold]\n",
    "    all_val_preds = []\n",
    "    all_test_preds = []\n",
    "    #print('Training set size: ', len(tr_ind))\n",
    "    #print('Val set size: ', len(val_ind))\n",
    "    x_training = x_train.iloc[tr_ind]\n",
    "    y_training = y_train[tr_ind]\n",
    "    #y_aux_training = y_aux_train[tr_ind]\n",
    "    \n",
    "    x_val = x_train.iloc[val_ind]\n",
    "    y_val = y_train[val_ind]\n",
    "    #y_aux_val = y_aux_train[val_ind]\n",
    "    \n",
    "    x_train_torch = torch.tensor(x_training.values, dtype=torch.float32).cuda()\n",
    "    x_val_torch = torch.tensor(x_val.values, dtype=torch.float32).cuda()\n",
    "    #y_train_torch = torch.tensor(np.hstack([y_training[:, np.newaxis], y_aux_training]), dtype=torch.float32).cuda()\n",
    "    y_train_torch = torch.tensor(y_training, dtype=torch.float32).cuda()\n",
    "    \n",
    "    x_test_torch = torch.tensor(x_test.values, dtype=torch.float32).cuda()\n",
    "    \n",
    "    print(y_train_torch.shape)\n",
    "    #print(y_train_torch.shape)\n",
    "    \n",
    "    #train_dataset = data.TensorDataset(x_train_torch, y_train_torch\n",
    "    train_dataset = data.TensorDataset(x_train_torch, y_train_torch)\n",
    "    val_dataset = data.TensorDataset(x_val_torch)\n",
    "    test_dataset = data.TensorDataset(x_test_torch)\n",
    "\n",
    "    for model_idx in range(NUM_MODELS):\n",
    "        print('Model ', model_idx)\n",
    "        seed_everything(1234 + model_idx)\n",
    "\n",
    "        model = NeuralNet(input_size=x_training.shape[1], hidden_size=20, num_classes=2)\n",
    "        try:\n",
    "            model.cuda()\n",
    "            #training using training and validation set\n",
    "            model = train_model(model, train_dataset, val_dataset, output_dim=y_train_torch.shape[-1], \n",
    "                                 loss_fn=nn.BCEWithLogitsLoss(reduction='mean'))\n",
    "        except:\n",
    "            model.cuda()\n",
    "            #training using training and validation set\n",
    "            model = train_model(model, train_dataset, val_dataset, output_dim=y_train_torch.shape[-1], \n",
    "                                 loss_fn=nn.BCEWithLogitsLoss(reduction='mean'))\n",
    "        \n",
    "        #prediction on validation set (used for score measurement)\n",
    "        val_pred = predict(model, val_dataset, output_dim=y_train_torch.shape[-1]) #val preds on the val split\n",
    "        all_val_preds.append(val_pred)\n",
    "        #print(len(val_pred))\n",
    "        \n",
    "        #prediction on entire test set (actual predictions to be submitted)\n",
    "        test_pred = predict(model, test_dataset, output_dim=y_train_torch.shape[-1])\n",
    "        all_test_preds.append(test_pred)\n",
    "        \n",
    "        print()\n",
    "        \n",
    "    #average validation prediction amongst all models\n",
    "    avg_val = np.mean(all_val_preds, axis=0)[:, 0] #will be printed out per split\n",
    "    final_val_preds[val_ind] += avg_val\n",
    "    \n",
    "    avg_test = np.mean(all_test_preds, axis=0)[:, 0]\n",
    "    \n",
    "    final_test_preds += avg_test\n",
    "\n",
    "    y_true = y_train[val_ind] #true scores for this validation set\n",
    "    y_identity = y_train_identity[val_ind] #true scores for the identity groups for this validation set\n",
    "    #print(y_true)\n",
    "    #print(y_identity)\n",
    "    evaluator = JigsawEvaluator(y_true, y_identity)\n",
    "    #print(avg_val)\n",
    "    auc_score = evaluator.get_final_metric(avg_val)\n",
    "\n",
    "    roc_score = roc_auc_score(y_train[val_ind], avg_val)\n",
    "    print('Kaggle Score: ', auc_score)\n",
    "    print('ROC score: ', roc_score)\n",
    "    \n",
    "    del x_train_torch\n",
    "    del x_val_torch\n",
    "    del y_train_torch\n",
    "    del x_test_torch\n",
    "    del train_dataset\n",
    "    del val_dataset\n",
    "    del test_dataset\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print('=============End-of-Fold================')\n",
    "    \n",
    "end_time = time.time()\n",
    "print('Time: ', end_time - start_time)\n",
    "\n",
    "#Final combined score\n",
    "y_true = y_train\n",
    "y_identity = y_train_identity\n",
    "evaluator = JigsawEvaluator(y_true, y_identity)\n",
    "auc_score = evaluator.get_final_metric(final_val_preds)\n",
    "print('Final Kaggle Score: ', auc_score)\n",
    "print('Final ROC score: ', roc_auc_score(y_train, final_val_preds))\n",
    "\n",
    "#average test predictions AGAIN this time by number of splits\n",
    "final_test_preds /= num_splits\n",
    "#print(final_test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0.        ,  101.        ,    3.        , ...,    3.89473684,\n",
       "          74.        ,    3.89473684],\n",
       "       [   0.        ,  114.        ,    3.        , ...,    3.90909091,\n",
       "          86.        ,    3.90909091],\n",
       "       [   0.        ,   84.        ,    3.        , ...,    3.82352941,\n",
       "          63.        ,    3.9375    ],\n",
       "       ...,\n",
       "       [   1.        ,   86.        ,    5.        , ...,    3.5       ,\n",
       "          56.        ,    3.5       ],\n",
       "       [   0.        ,  190.        ,   11.        , ...,    4.8125    ,\n",
       "         152.        ,    4.90322581],\n",
       "       [   0.        , 1000.        ,   37.        , ...,    4.98757764,\n",
       "         643.        ,    5.2704918 ]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_training.values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
