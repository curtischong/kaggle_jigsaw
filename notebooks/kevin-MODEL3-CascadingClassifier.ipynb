{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "import random\n",
    "from tqdm._tqdm_notebook import tqdm_notebook as tqdm\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "lc = LancasterStemmer()\n",
    "from nltk.stem import SnowballStemmer\n",
    "sb = SnowballStemmer(\"english\")\n",
    "from keras.preprocessing import text, sequence\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from torch.nn import functional as F\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import spacy\n",
    "import gensim\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable progress bars when submitting\n",
    "def is_interactive():\n",
    "   return 'SHLVL' not in os.environ\n",
    "\n",
    "if not is_interactive():\n",
    "    def nop(it, *a, **k):\n",
    "        return it\n",
    "\n",
    "    tqdm = nop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CRAWL_EMBEDDING_PATH = '../input/crawl-300d-2M.vec'\n",
    "GLOVE_EMBEDDING_PATH = '../input/glove.840B.300d.txt'\n",
    "NUM_MODELS = 2\n",
    "LSTM_UNITS = 128\n",
    "DENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\n",
    "MAX_LEN = 220"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nstart_time = time.time()\\nspell_model = gensim.models.KeyedVectors.load_word2vec_format(\\'../input/wiki-news-300d-1M.vec\\')\\nwords = spell_model.index2word\\nw_rank = {}\\nfor i,word in enumerate(words):\\n    w_rank[word] = i\\nWORDS = w_rank\\ndel words\\ndel w_rank\\ndel spell_model\\ngc.collect()\\nprint(\"--- %s seconds ---\" % (time.time() - start_time))\\n\\n# Use fast text as vocabulary\\ndef words(text): return re.findall(r\\'\\\\w+\\', text.lower())\\ndef P(word): \\n    \"Probability of `word`.\"\\n    # use inverse of rank as proxy\\n    # returns 0 if the word isn\\'t in the dictionary\\n    return - WORDS.get(word, 0)\\ndef correction(word): \\n    \"Most probable spelling correction for word.\"\\n    return max(candidates(word), key=P)\\ndef candidates(word): \\n    \"Generate possible spelling corrections for word.\"\\n    return (known([word]) or known(edits1(word)) or [word])\\ndef known(words): \\n    \"The subset of `words` that appear in the dictionary of WORDS.\"\\n    return set(w for w in words if w in WORDS)\\ndef edits1(word):\\n    \"All edits that are one edit away from `word`.\"\\n    letters    = \\'abcdefghijklmnopqrstuvwxyz\\'\\n    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\\n    deletes    = [L + R[1:]               for L, R in splits if R]\\n    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\\n    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\\n    inserts    = [L + c + R               for L, R in splits for c in letters]\\n    return set(deletes + transposes + replaces + inserts)\\ndef edits2(word): \\n    \"All edits that are two edits away from `word`.\"\\n    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\\ndef singlify(word):\\n    return \"\".join([letter for i,letter in enumerate(word) if i == 0 or letter != word[i-1]])\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "start_time = time.time()\n",
    "spell_model = gensim.models.KeyedVectors.load_word2vec_format('../input/wiki-news-300d-1M.vec')\n",
    "words = spell_model.index2word\n",
    "w_rank = {}\n",
    "for i,word in enumerate(words):\n",
    "    w_rank[word] = i\n",
    "WORDS = w_rank\n",
    "del words\n",
    "del w_rank\n",
    "del spell_model\n",
    "gc.collect()\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# Use fast text as vocabulary\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "def P(word): \n",
    "    \"Probability of `word`.\"\n",
    "    # use inverse of rank as proxy\n",
    "    # returns 0 if the word isn't in the dictionary\n",
    "    return - WORDS.get(word, 0)\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or [word])\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "def singlify(word):\n",
    "    return \"\".join([letter for i,letter in enumerate(word) if i == 0 or letter != word[i-1]])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef build_matrix(word_dict, lemma_dict, path):\\n    embed_size = 300\\n    embeddings_index = load_embeddings(path)\\n    embedding_matrix = np.zeros((len(word_dict) + 1, embed_size), dtype=np.float32)\\n    unknown_words = []\\n    unknown_vector = np.zeros((embed_size,), dtype=np.float32) - 1\\n    \\n    for key in tqdm(word_dict):\\n        word = key\\n        embedding_vector = embeddings_index.get(word)\\n        if embedding_vector is not None:\\n            embedding_matrix[word_dict[key]] = embedding_vector\\n            continue\\n        word = key.lower()\\n        embedding_vector = embeddings_index.get(word)\\n        if embedding_vector is not None:\\n            embedding_matrix[word_dict[key]] = embedding_vector\\n            continue\\n        word = key.upper()\\n        embedding_vector = embeddings_index.get(word)\\n        if embedding_vector is not None:\\n            embedding_matrix[word_dict[key]] = embedding_vector\\n            continue\\n        word = key.capitalize()\\n        embedding_vector = embeddings_index.get(word)\\n        if embedding_vector is not None:\\n            embedding_matrix[word_dict[key]] = embedding_vector\\n            continue\\n        word = ps.stem(key)\\n        embedding_vector = embeddings_index.get(word)\\n        if embedding_vector is not None:\\n            embedding_matrix[word_dict[key]] = embedding_vector\\n            continue\\n        word = lc.stem(key)\\n        embedding_vector = embeddings_index.get(word)\\n        if embedding_vector is not None:\\n            embedding_matrix[word_dict[key]] = embedding_vector\\n            continue\\n        word = sb.stem(key)\\n        embedding_vector = embeddings_index.get(word)\\n        if embedding_vector is not None:\\n            embedding_matrix[word_dict[key]] = embedding_vector\\n            continue\\n        word = lemma_dict[key]\\n        embedding_vector = embeddings_index.get(word)\\n        if embedding_vector is not None:\\n            embedding_matrix[word_dict[key]] = embedding_vector\\n            continue\\n        if len(key) > 1:\\n            word = correction(key)\\n            embedding_vector = embeddings_index.get(word)\\n            if embedding_vector is not None:\\n                embedding_matrix[word_dict[key]] = embedding_vector\\n                continue\\n        \\n        #Unknown word, does not exist in dictionary\\n        embedding_matrix[word_dict[key]] = unknown_vector\\n        unknown_words.append(word)\\n    return embedding_matrix, unknown_words\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "def load_embeddings(path):\n",
    "    with open(path) as f:\n",
    "        return dict(get_coefs(*line.strip().split(' ')) for line in tqdm(f))\n",
    "    \n",
    "def build_matrix(word_index, path):\n",
    "    embedding_index = load_embeddings(path)\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "    unknown_words = []\n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        try:\n",
    "            embedding_matrix[i] = embedding_index[word]\n",
    "        except KeyError:\n",
    "            unknown_words.append(word)\n",
    "    return embedding_matrix, unknown_words\n",
    "\n",
    "'''\n",
    "def build_matrix(word_dict, lemma_dict, path):\n",
    "    embed_size = 300\n",
    "    embeddings_index = load_embeddings(path)\n",
    "    embedding_matrix = np.zeros((len(word_dict) + 1, embed_size), dtype=np.float32)\n",
    "    unknown_words = []\n",
    "    unknown_vector = np.zeros((embed_size,), dtype=np.float32) - 1\n",
    "    \n",
    "    for key in tqdm(word_dict):\n",
    "        word = key\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = key.lower()\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = key.upper()\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = key.capitalize()\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = ps.stem(key)\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = lc.stem(key)\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = sb.stem(key)\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = lemma_dict[key]\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        if len(key) > 1:\n",
    "            word = correction(key)\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[word_dict[key]] = embedding_vector\n",
    "                continue\n",
    "        \n",
    "        #Unknown word, does not exist in dictionary\n",
    "        embedding_matrix[word_dict[key]] = unknown_vector\n",
    "        unknown_words.append(word)\n",
    "    return embedding_matrix, unknown_words\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceBucketCollator():\n",
    "    def __init__(self, choose_length, sequence_index, length_index, label_index=None):\n",
    "        self.choose_length = choose_length\n",
    "        self.sequence_index = sequence_index\n",
    "        self.length_index = length_index\n",
    "        self.label_index = label_index\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        batch = [torch.stack(x) for x in list(zip(*batch))]\n",
    "        \n",
    "        sequences = batch[self.sequence_index]\n",
    "        lengths = batch[self.length_index]\n",
    "        \n",
    "        length = self.choose_length(lengths)\n",
    "        mask = torch.arange(start=maxlen, end=0, step=-1) < length\n",
    "        padded_sequences = sequences[:, mask]\n",
    "        \n",
    "        batch[self.sequence_index] = padded_sequences\n",
    "        \n",
    "        if self.label_index is not None:\n",
    "            return [x for i, x in enumerate(batch) if i != self.label_index], batch[self.label_index]\n",
    "    \n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "filepath = './model_files/checkpoint.pth'\n",
    "\n",
    "def train_model(model, train, test, loss_fn, output_dim, lr=0.001,\n",
    "                batch_size=512, n_epochs=4, n_epochs_embed=2,\n",
    "                enable_checkpoint_ensemble=True):\n",
    "    \n",
    "    train_collator = SequenceBucketCollator(lambda lengths: lengths.max(), \n",
    "                                            sequence_index=0, \n",
    "                                            length_index=1, \n",
    "                                            label_index=2)\n",
    "    \n",
    "    param_lrs = [{'params': param, 'lr': lr} for param in model.parameters()]\n",
    "    optimizer = torch.optim.Adam(param_lrs, lr=lr)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda epoch: 0.6 ** epoch)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True, collate_fn=train_collator)\n",
    "    val_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False, collate_fn=train_collator)\n",
    "    all_test_preds = []\n",
    "    checkpoint_weights = [2 ** epoch for epoch in range(n_epochs)]\n",
    "    \n",
    "    best_loss = 1\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        model.train() #set model to train mode\n",
    "        avg_loss = 0.\n",
    "        \n",
    "        for data in tqdm(train_loader, disable=False):\n",
    "            \n",
    "            #training loop\n",
    "            x_batch = data[:-1]\n",
    "            #print(\"First: \", x_batch[0][0])\n",
    "            #print(\"Second: \", x_batch[0][1])\n",
    "            first = x_batch[0][0]\n",
    "            second = x_batch[0][1]\n",
    "            \n",
    "            y_batch = data[-1]\n",
    "\n",
    "            y_pred = model(first, second)  #feed data into model          \n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            \n",
    "            #calculate error and adjust model params\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item() / len(train_loader)\n",
    "        \n",
    "        #Check if loss is better than current best loss, if so, save the model\n",
    "        is_best = (avg_loss < best_loss)\n",
    "        \n",
    "        if is_best:\n",
    "            print (\"=> Saving a new best\")\n",
    "            best_loss = avg_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_loss': best_loss\n",
    "            }, filepath)  # save checkpoint\n",
    "        else:\n",
    "            print (\"=> Model Accuracy did not improve\")\n",
    "            \n",
    "        \n",
    "        model.eval() #set model to eval mode for test data\n",
    "        test_preds = np.zeros((len(test), output_dim))\n",
    "        \n",
    "    \n",
    "        for i, x_batch in enumerate(val_loader):\n",
    "            #print(\"X_Batch: \", x_batch)\n",
    "            data_param = x_batch[0][0]\n",
    "            lengths_param = x_batch[0][1]\n",
    "            y_pred = sigmoid(model(data_param, lengths_param).detach().cpu().numpy()) #feed data into model\n",
    "\n",
    "            test_preds[i * batch_size:(i+1) * batch_size, :] = y_pred #get test predictions\n",
    "        \n",
    "        #test_preds has the predictions for the entire test set now\n",
    "        all_test_preds.append(test_preds) #append predictions to the record of all past predictions\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print('Epoch {}/{} \\t loss={:.4f} \\t time={:.2f}s'.format(\n",
    "              epoch + 1, n_epochs, avg_loss, elapsed_time))\n",
    "        \n",
    "    #Make embeddings layer only layer unfreezed, train again (literally run through the n_epochs)\n",
    "    #maybe define a n_epochs_embedding\n",
    "    \n",
    "    #parameters = model.parameters()\n",
    "    #for param in parameters:\n",
    "    #        param.requires_grad = False\n",
    "    #parameters[0].requires_grad = True\n",
    "    \n",
    "    '''\n",
    "    ct = 0\n",
    "    for child in model.children():\n",
    "        if ct == 0:\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = True\n",
    "        else:\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = False\n",
    "        ct += 1\n",
    "    \n",
    "    for epoch in range(n_epochs_embed):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        model.train() #set model to train mode\n",
    "        avg_loss = 0.\n",
    "        \n",
    "        for data in tqdm(train_loader, disable=False):\n",
    "            \n",
    "            #training loop\n",
    "            x_batch = data[:-1]\n",
    "            y_batch = data[-1]\n",
    "\n",
    "            y_pred = model(*x_batch)  #feed data into model          \n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            \n",
    "            #calculate error and adjust model params\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item() / len(train_loader) #gets the loss per epoch\n",
    "        \n",
    "            \n",
    "        model.eval() #set model to eval mode for test data\n",
    "        test_preds = np.zeros((len(test), output_dim))\n",
    "    \n",
    "        for i, x_batch in enumerate(test_loader):\n",
    "            y_pred = sigmoid(model(*x_batch).detach().cpu().numpy()) #feed data into model\n",
    "\n",
    "            test_preds[i * batch_size:(i+1) * batch_size, :] = y_pred #get test predictions\n",
    "        \n",
    "        #test_preds has the predictions for the entire test set now\n",
    "        #all_test_preds.append(test_preds) #append predictions to the record of all past predictions\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print('[EMBEDDING TRAINING] Epoch {}/{} \\t loss={:.4f} \\t time={:.2f}s '.format(\n",
    "              epoch + 1, n_epochs_embed, avg_loss, elapsed_time))\n",
    "    '''\n",
    "    \n",
    "    #PREDICTION CODE\n",
    "    '''\n",
    "    if enable_checkpoint_ensemble:\n",
    "        #if our approach is an ensemble then we average it amongst all the historical predictions\n",
    "        test_preds = np.average(all_test_preds, weights=checkpoint_weights, axis=0)    \n",
    "    else:\n",
    "        #if our approach is not an ensemble then we just take the last set of predictions\n",
    "        test_preds = all_test_preds[-1]\n",
    "        \n",
    "    return test_preds\n",
    "    '''\n",
    "    \n",
    "    #return trained model\n",
    "    return model\n",
    "\n",
    "def predict(model, test, output_dim, batch_size=512, pred_type=\"val\"):\n",
    "    checkpoint = torch.load(filepath)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    #optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    if pred_type == \"test\":\n",
    "        test_collator = SequenceBucketCollator(lambda lengths: lengths.max(), sequence_index=0, length_index=1)\n",
    "        test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False, collate_fn=test_collator)\n",
    "\n",
    "        model.eval() #set model to eval mode for test data\n",
    "        test_preds = np.zeros((len(test), output_dim))\n",
    "\n",
    "        for i, x_batch in enumerate(test_loader):\n",
    "            #print(x_batch[0])\n",
    "            data_param = x_batch[0]\n",
    "            lengths_param = x_batch[1]\n",
    "            y_pred = sigmoid(model(data_param, lengths_param).detach().cpu().numpy()) #feed data into model\n",
    "            test_preds[i * batch_size:(i+1) * batch_size, :] = y_pred #get test predictions\n",
    "\n",
    "        return test_preds\n",
    "    else:\n",
    "        test_collator = SequenceBucketCollator(lambda lengths: lengths.max(), sequence_index=0, length_index=1, label_index=2)\n",
    "        test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False, collate_fn=test_collator)\n",
    "\n",
    "        model.eval() #set model to eval mode for test data\n",
    "        test_preds = np.zeros((len(test), output_dim))\n",
    "\n",
    "        for i, x_batch in enumerate(test_loader):\n",
    "            #print(x_batch)\n",
    "            data_param = x_batch[0][0]\n",
    "            lengths_param = x_batch[0][1]\n",
    "            y_pred = sigmoid(model(data_param, lengths_param).detach().cpu().numpy()) #feed data into model\n",
    "            test_preds[i * batch_size:(i+1) * batch_size, :] = y_pred #get test predictions\n",
    "\n",
    "        return test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialDropout(nn.Dropout2d):\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(2)    # (N, T, 1, K)\n",
    "        x = x.permute(0, 3, 2, 1)  # (N, K, 1, T)\n",
    "        #call the forward method in Dropout2d (super function specifies the subclass and instance)\n",
    "        x = super(SpatialDropout, self).forward(x)  # (N, K, 1, T), some features are masked\n",
    "        x = x.permute(0, 3, 2, 1)  # (N, T, 1, K)\n",
    "        x = x.squeeze(2)  # (N, T, K)\n",
    "        return x\n",
    "    \n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, embedding_matrix, num_aux_targets):\n",
    "        #call the init mthod in Module (super function specifies the subclass and instance)\n",
    "        super(NeuralNet, self).__init__() \n",
    "        embed_size = embedding_matrix.shape[1]\n",
    "        \n",
    "        self.embedding = nn.Embedding(max_features, embed_size)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.embedding_dropout = SpatialDropout(0.3)\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(embed_size, LSTM_UNITS, bidirectional=True, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(LSTM_UNITS * 2, LSTM_UNITS, bidirectional=True, batch_first=True)\n",
    "    \n",
    "        self.linear1 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n",
    "        self.linear2 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n",
    "        \n",
    "        self.linear_out = nn.Linear(DENSE_HIDDEN_UNITS, 1)\n",
    "        self.linear_aux_out = nn.Linear(DENSE_HIDDEN_UNITS, num_aux_targets)\n",
    "        \n",
    "    def forward(self, x, lengths=None):\n",
    "        h_embedding = self.embedding(x)\n",
    "        h_embedding = self.embedding_dropout(h_embedding)\n",
    "        \n",
    "        #first variable h_(lstm #) holds the output, _ is the (hidden state, cell state)\n",
    "        h_lstm1, _ = self.lstm1(h_embedding) \n",
    "        h_lstm2, _ = self.lstm2(h_lstm1)\n",
    "        \n",
    "        # global average pooling\n",
    "        avg_pool = torch.mean(h_lstm2, 1) #get the mean value of the first dimension in h_lstm2\n",
    "        # global max pooling\n",
    "        max_pool, _ = torch.max(h_lstm2, 1) #get the max value of the first dimension in h_lstm2\n",
    "        \n",
    "        h_conc = torch.cat((max_pool, avg_pool), 1)\n",
    "        h_conc_linear1  = F.relu(self.linear1(h_conc))\n",
    "        h_conc_linear2  = F.relu(self.linear2(h_conc))\n",
    "        \n",
    "        hidden = h_conc + h_conc_linear1 + h_conc_linear2\n",
    "        \n",
    "        result = self.linear_out(hidden)\n",
    "        aux_result = self.linear_aux_out(hidden)\n",
    "        out = torch.cat([result, aux_result], 1)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def preprocess(data):\n",
    "\n",
    "    punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~`\" + '\"\"â€œâ€â€™' + 'âˆžÎ¸Ã·Î±â€¢Ã âˆ’Î²âˆ…Â³Ï€â€˜â‚¹Â´Â°Â£â‚¬\\Ã—â„¢âˆšÂ²â€”â€“&'\n",
    "    def clean_special_chars(text, punct):\n",
    "        for p in punct:\n",
    "            text = text.replace(p, ' ')\n",
    "        return text\n",
    "\n",
    "    data = data.astype(str).apply(lambda x: clean_special_chars(x, punct))\n",
    "    return data\n",
    "'''\n",
    "symbols_to_isolate = '.,?!-;*\"â€¦:â€”()%#$&_/@ï¼¼ãƒ»Ï‰+=â€â€œ[]^â€“>\\\\Â°<~â€¢â‰ â„¢ËˆÊŠÉ’âˆžÂ§{}Â·Ï„Î±â¤â˜ºÉ¡|Â¢â†’Ì¶`â¥â”â”£â”«â”—ï¼¯â–ºâ˜…Â©â€•Éªâœ”Â®\\x96\\x92â—Â£â™¥âž¤Â´Â¹â˜•â‰ˆÃ·â™¡â—â•‘â–¬â€²É”Ëâ‚¬Û©Ûžâ€ Î¼âœ’âž¥â•â˜†ËŒâ—„Â½Ê»Ï€Î´Î·Î»ÏƒÎµÏÎ½Êƒâœ¬ï¼³ï¼µï¼°ï¼¥ï¼²ï¼©ï¼´â˜»Â±â™ÂµÂºÂ¾âœ“â—¾ØŸï¼Žâ¬…â„…Â»Ð’Ð°Ð²â£â‹…Â¿Â¬â™«ï¼£ï¼­Î²â–ˆâ–“â–’â–‘â‡’â­â€ºÂ¡â‚‚â‚ƒâ§â–°â–”â—žâ–€â–‚â–ƒâ–„â–…â–†â–‡â†™Î³Ì„â€³â˜¹âž¡Â«Ï†â…“â€žâœ‹ï¼šÂ¥Ì²Ì…Ìâˆ™â€›â—‡âœâ–·â“â—Â¶ËšË™ï¼‰ÑÐ¸Ê¿âœ¨ã€‚É‘\\x80â—•ï¼ï¼…Â¯âˆ’ï¬‚ï¬â‚Â²ÊŒÂ¼â´â„â‚„âŒ â™­âœ˜â•ªâ–¶â˜­âœ­â™ªâ˜”â˜ â™‚â˜ƒâ˜ŽâœˆâœŒâœ°â†â˜™â—‹â€£âš“å¹´âˆŽâ„’â–ªâ–™â˜â…›ï½ƒï½ï½“Ç€â„®Â¸ï½—â€šâˆ¼â€–â„³â„â†â˜¼â‹†Ê’âŠ‚ã€â…”Â¨Í¡à¹âš¾âš½Î¦Ã—Î¸ï¿¦ï¼Ÿï¼ˆâ„ƒâ©â˜®âš æœˆâœŠâŒâ­•â–¸â– â‡Œâ˜â˜‘âš¡â˜„Ç«â•­âˆ©â•®ï¼Œä¾‹ï¼žÊ•ÉÌ£Î”â‚€âœžâ”ˆâ•±â•²â–â–•â”ƒâ•°â–Šâ–‹â•¯â”³â”Šâ‰¥â˜’â†‘â˜É¹âœ…â˜›â™©â˜žï¼¡ï¼ªï¼¢â—”â—¡â†“â™€â¬†Ì±â„\\x91â €Ë¤â•šâ†ºâ‡¤âˆâœ¾â—¦â™¬Â³ã®ï½œï¼âˆµâˆ´âˆšÎ©Â¤â˜œâ–²â†³â–«â€¿â¬‡âœ§ï½ï½–ï½ï¼ï¼’ï¼ï¼˜ï¼‡â€°â‰¤âˆ•Ë†âšœâ˜'\n",
    "symbols_to_delete = '\\nðŸ•\\rðŸµðŸ˜‘\\xa0\\ue014\\t\\uf818\\uf04a\\xadðŸ˜¢ðŸ¶ï¸\\uf0e0ðŸ˜œðŸ˜ŽðŸ‘Š\\u200b\\u200eðŸ˜Ø¹Ø¯ÙˆÙŠÙ‡ØµÙ‚Ø£Ù†Ø§Ø®Ù„Ù‰Ø¨Ù…ØºØ±ðŸ˜ðŸ’–ðŸ’µÐ•ðŸ‘ŽðŸ˜€ðŸ˜‚\\u202a\\u202cðŸ”¥ðŸ˜„ðŸ»ðŸ’¥á´ÊÊ€á´‡É´á´…á´á´€á´‹Êœá´œÊŸá´›á´„á´˜Ê™Ò“á´Šá´¡É¢ðŸ˜‹ðŸ‘×©×œ×•××‘×™ðŸ˜±â€¼\\x81ã‚¨ãƒ³ã‚¸æ•…éšœ\\u2009ðŸšŒá´µÍžðŸŒŸðŸ˜ŠðŸ˜³ðŸ˜§ðŸ™€ðŸ˜ðŸ˜•\\u200fðŸ‘ðŸ˜®ðŸ˜ƒðŸ˜˜××¢×›×—ðŸ’©ðŸ’¯â›½ðŸš„ðŸ¼à®œðŸ˜–á´ ðŸš²â€ðŸ˜ŸðŸ˜ˆðŸ’ªðŸ™ðŸŽ¯ðŸŒ¹ðŸ˜‡ðŸ’”ðŸ˜¡\\x7fðŸ‘Œá¼á½¶Î®Î¹á½²Îºá¼€Î¯á¿ƒá¼´Î¾ðŸ™„ï¼¨ðŸ˜ \\ufeff\\u2028ðŸ˜‰ðŸ˜¤â›ºðŸ™‚\\u3000ØªØ­ÙƒØ³Ø©ðŸ‘®ðŸ’™ÙØ²Ø·ðŸ˜ðŸ¾ðŸŽ‰ðŸ˜ž\\u2008ðŸ¾ðŸ˜…ðŸ˜­ðŸ‘»ðŸ˜¥ðŸ˜”ðŸ˜“ðŸ½ðŸŽ†ðŸ»ðŸ½ðŸŽ¶ðŸŒºðŸ¤”ðŸ˜ª\\x08â€‘ðŸ°ðŸ‡ðŸ±ðŸ™†ðŸ˜¨ðŸ™ƒðŸ’•ð˜Šð˜¦ð˜³ð˜¢ð˜µð˜°ð˜¤ð˜ºð˜´ð˜ªð˜§ð˜®ð˜£ðŸ’—ðŸ’šåœ°ç„è°·ÑƒÐ»ÐºÐ½ÐŸÐ¾ÐÐðŸ¾ðŸ•ðŸ˜†×”ðŸ”—ðŸš½æ­Œèˆžä¼ŽðŸ™ˆðŸ˜´ðŸ¿ðŸ¤—ðŸ‡ºðŸ‡¸Ð¼Ï…Ñ‚Ñ•â¤µðŸ†ðŸŽƒðŸ˜©\\u200aðŸŒ ðŸŸðŸ’«ðŸ’°ðŸ’ŽÑÐ¿Ñ€Ð´\\x95ðŸ–ðŸ™…â›²ðŸ°ðŸ¤ðŸ‘†ðŸ™Œ\\u2002ðŸ’›ðŸ™ðŸ‘€ðŸ™ŠðŸ™‰\\u2004Ë¢áµ’Ê³Ê¸á´¼á´·á´ºÊ·áµ—Ê°áµ‰áµ˜\\x13ðŸš¬ðŸ¤“\\ue602ðŸ˜µÎ¬Î¿ÏŒÏ‚Î­á½¸×ª×ž×“×£× ×¨×š×¦×˜ðŸ˜’ÍðŸ†•ðŸ‘…ðŸ‘¥ðŸ‘„ðŸ”„ðŸ”¤ðŸ‘‰ðŸ‘¤ðŸ‘¶ðŸ‘²ðŸ”›ðŸŽ“\\uf0b7\\uf04c\\x9f\\x10æˆéƒ½ðŸ˜£âºðŸ˜ŒðŸ¤‘ðŸŒðŸ˜¯ÐµÑ…ðŸ˜²á¼¸á¾¶á½ðŸ’žðŸš“ðŸ””ðŸ“šðŸ€ðŸ‘\\u202dðŸ’¤ðŸ‡\\ue613å°åœŸè±†ðŸ¡â”â‰\\u202fðŸ‘ ã€‹à¤•à¤°à¥à¤®à¤¾ðŸ‡¹ðŸ‡¼ðŸŒ¸è”¡è‹±æ–‡ðŸŒžðŸŽ²ãƒ¬ã‚¯ã‚µã‚¹ðŸ˜›å¤–å›½äººå…³ç³»Ð¡Ð±ðŸ’‹ðŸ’€ðŸŽ„ðŸ’œðŸ¤¢ÙÙŽÑŒÑ‹Ð³Ñä¸æ˜¯\\x9c\\x9dðŸ—‘\\u2005ðŸ’ƒðŸ“£ðŸ‘¿à¼¼ã¤à¼½ðŸ˜°á¸·Ð—Ð·â–±Ñ†ï¿¼ðŸ¤£å–æ¸©å“¥åŽè®®ä¼šä¸‹é™ä½ å¤±åŽ»æ‰€æœ‰çš„é’±åŠ æ‹¿å¤§åç¨Žéª—å­ðŸãƒ„ðŸŽ…\\x85ðŸºØ¢Ø¥Ø´Ø¡ðŸŽµðŸŒŽÍŸá¼”æ²¹åˆ«å…‹ðŸ¤¡ðŸ¤¥ðŸ˜¬ðŸ¤§Ð¹\\u2003ðŸš€ðŸ¤´Ê²ÑˆÑ‡Ð˜ÐžÐ Ð¤Ð”Ð¯ÐœÑŽÐ¶ðŸ˜ðŸ–‘á½á½»Ïç‰¹æ®Šä½œæˆ¦ç¾¤Ñ‰ðŸ’¨åœ†æ˜Žå›­×§â„ðŸˆðŸ˜ºðŸŒâá»‡ðŸ”ðŸ®ðŸðŸ†ðŸ‘ðŸŒ®ðŸŒ¯ðŸ¤¦\\u200dð“’ð“²ð“¿ð“µì•ˆì˜í•˜ì„¸ìš”Ð–Ñ™ÐšÑ›ðŸ€ðŸ˜«ðŸ¤¤á¿¦æˆ‘å‡ºç”Ÿåœ¨äº†å¯ä»¥è¯´æ™®é€šè¯æ±‰è¯­å¥½æžðŸŽ¼ðŸ•ºðŸ¸ðŸ¥‚ðŸ—½ðŸŽ‡ðŸŽŠðŸ†˜ðŸ¤ ðŸ‘©ðŸ–’ðŸšªå¤©ä¸€å®¶âš²\\u2006âš­âš†â¬­â¬¯â–æ–°âœ€â•ŒðŸ‡«ðŸ‡·ðŸ‡©ðŸ‡ªðŸ‡®ðŸ‡¬ðŸ‡§ðŸ˜·ðŸ‡¨ðŸ‡¦Ð¥Ð¨ðŸŒ\\x1fæ€é¸¡ç»™çŒ´çœ‹Êð—ªð—µð—²ð—»ð˜†ð—¼ð˜‚ð—¿ð—®ð—¹ð—¶ð˜‡ð—¯ð˜ð—°ð˜€ð˜…ð—½ð˜„ð—±ðŸ“ºÏ–\\u2000Ò¯Õ½á´¦áŽ¥Ò»Íº\\u2007Õ°\\u2001É©ï½™ï½…àµ¦ï½ŒÆ½ï½ˆð“ð¡ðžð«ð®ððšðƒðœð©ð­ð¢ð¨ð§Æ„á´¨×Ÿá‘¯à»Î¤á§à¯¦Ð†á´‘Üð¬ð°ð²ð›ð¦ð¯ð‘ð™ð£ð‡ð‚ð˜ðŸŽÔœÐ¢á—žà±¦ã€”áŽ«ð³ð”ð±ðŸ”ðŸ“ð…ðŸ‹ï¬ƒðŸ’˜ðŸ’“Ñ‘ð˜¥ð˜¯ð˜¶ðŸ’ðŸŒ‹ðŸŒ„ðŸŒ…ð™¬ð™–ð™¨ð™¤ð™£ð™¡ð™®ð™˜ð™ ð™šð™™ð™œð™§ð™¥ð™©ð™ªð™—ð™žð™ð™›ðŸ‘ºðŸ·â„‹ð€ð¥ðªðŸš¶ð™¢á¼¹ðŸ¤˜Í¦ðŸ’¸Ø¬íŒ¨í‹°ï¼·ð™‡áµ»ðŸ‘‚ðŸ‘ƒÉœðŸŽ«\\uf0a7Ð‘Ð£Ñ–ðŸš¢ðŸš‚àª—à«àªœàª°àª¾àª¤à«€á¿†ðŸƒð“¬ð“»ð“´ð“®ð“½ð“¼â˜˜ï´¾Ì¯ï´¿â‚½\\ue807ð‘»ð’†ð’ð’•ð’‰ð’“ð’–ð’‚ð’ð’…ð’”ð’Žð’—ð’ŠðŸ‘½ðŸ˜™\\u200cÐ›â€’ðŸŽ¾ðŸ‘¹âŽŒðŸ’â›¸å…¬å¯“å…»å® ç‰©å—ðŸ„ðŸ€ðŸš‘ðŸ¤·æ“ç¾Žð’‘ð’šð’ð‘´ðŸ¤™ðŸ’æ¬¢è¿Žæ¥åˆ°é˜¿æ‹‰æ–¯×¡×¤ð™«ðŸˆð’Œð™Šð™­ð™†ð™‹ð™ð˜¼ð™…ï·»ðŸ¦„å·¨æ”¶èµ¢å¾—ç™½é¬¼æ„¤æ€’è¦ä¹°é¢áº½ðŸš—ðŸ³ðŸðŸðŸ–ðŸ‘ðŸ•ð’„ðŸ—ð ð™„ð™ƒðŸ‘‡é”Ÿæ–¤æ‹·ð—¢ðŸ³ðŸ±ðŸ¬â¦ãƒžãƒ«ãƒãƒ‹ãƒãƒ­æ ªå¼ç¤¾â›·í•œêµ­ì–´ã„¸ã…“ë‹ˆÍœÊ–ð˜¿ð™”â‚µð’©â„¯ð’¾ð“ð’¶ð“‰ð“‡ð“Šð“ƒð“ˆð“…â„´ð’»ð’½ð“€ð“Œð’¸ð“Žð™Î¶ð™Ÿð˜ƒð—ºðŸ®ðŸ­ðŸ¯ðŸ²ðŸ‘‹ðŸ¦Šå¤šä¼¦ðŸ½ðŸŽ»ðŸŽ¹â›“ðŸ¹ðŸ·ðŸ¦†ä¸ºå’Œä¸­å‹è°Šç¥è´ºä¸Žå…¶æƒ³è±¡å¯¹æ³•å¦‚ç›´æŽ¥é—®ç”¨è‡ªå·±çŒœæœ¬ä¼ æ•™å£«æ²¡ç§¯å”¯è®¤è¯†åŸºç£å¾’æ›¾ç»è®©ç›¸ä¿¡è€¶ç¨£å¤æ´»æ­»æ€ªä»–ä½†å½“ä»¬èŠäº›æ”¿æ²»é¢˜æ—¶å€™æˆ˜èƒœå› åœ£æŠŠå…¨å ‚ç»“å©šå­©ææƒ§ä¸”æ —è°“è¿™æ ·è¿˜â™¾ðŸŽ¸ðŸ¤•ðŸ¤’â›‘ðŸŽæ‰¹åˆ¤æ£€è®¨ðŸðŸ¦ðŸ™‹ðŸ˜¶ì¥ìŠ¤íƒ±íŠ¸ë¤¼ë„ì„ìœ ê°€ê²©ì¸ìƒì´ê²½ì œí™©ì„ë µê²Œë§Œë“¤ì§€ì•Šë¡ìž˜ê´€ë¦¬í•´ì•¼í•©ë‹¤ìºë‚˜ì—ì„œëŒ€ë§ˆì´ˆì™€í™”ì•½ê¸ˆì˜í’ˆëŸ°ì„±ë¶„ê°ˆë•ŒëŠ”ë°˜ë“œì‹œí—ˆëœì‚¬ìš©ðŸ”«ðŸ‘å‡¸á½°ðŸ’²ðŸ—¯ð™ˆá¼Œð’‡ð’ˆð’˜ð’ƒð‘¬ð‘¶ð•¾ð–™ð–—ð–†ð–Žð–Œð–ð–•ð–Šð–”ð–‘ð–‰ð–“ð–ð–œð–žð–šð–‡ð•¿ð–˜ð–„ð–›ð–’ð–‹ð–‚ð•´ð–Ÿð–ˆð•¸ðŸ‘‘ðŸš¿ðŸ’¡çŸ¥å½¼ç™¾\\uf005ð™€ð’›ð‘²ð‘³ð‘¾ð’‹ðŸ’ðŸ˜¦ð™’ð˜¾ð˜½ðŸð˜©ð˜¨á½¼á¹‘ð‘±ð‘¹ð‘«ð‘µð‘ªðŸ‡°ðŸ‡µðŸ‘¾á“‡á’§á”­áƒá§á¦á‘³á¨á“ƒá“‚á‘²á¸á‘­á‘Žá“€á£ðŸ„ðŸŽˆðŸ”¨ðŸŽðŸ¤žðŸ¸ðŸ’ŸðŸŽ°ðŸŒðŸ›³ç‚¹å‡»æŸ¥ç‰ˆðŸ­ð‘¥ð‘¦ð‘§ï¼®ï¼§ðŸ‘£\\uf020ã£ðŸ‰Ñ„ðŸ’­ðŸŽ¥ÎžðŸ´ðŸ‘¨ðŸ¤³ðŸ¦\\x0bðŸ©ð‘¯ð’’ðŸ˜—ðŸðŸ‚ðŸ‘³ðŸ—ðŸ•‰ðŸ²Ú†ÛŒð‘®ð—•ð—´ðŸ’êœ¥â²£â²ðŸ‘â°é‰„ãƒªäº‹ä»¶Ñ—ðŸ’Šã€Œã€\\uf203\\uf09a\\uf222\\ue608\\uf202\\uf099\\uf469\\ue607\\uf410\\ue600ç‡»è£½ã‚·è™šå½å±ç†å±ˆÐ“ð‘©ð‘°ð’€ð‘ºðŸŒ¤ð—³ð—œð—™ð—¦ð—§ðŸŠá½ºá¼ˆá¼¡Ï‡á¿–Î›â¤ðŸ‡³ð’™ÏˆÕÕ´Õ¥Õ¼Õ¡ÕµÕ«Õ¶Ö€Ö‚Õ¤Õ±å†¬è‡³á½€ð’ðŸ”¹ðŸ¤šðŸŽð‘·ðŸ‚ðŸ’…ð˜¬ð˜±ð˜¸ð˜·ð˜ð˜­ð˜“ð˜–ð˜¹ð˜²ð˜«Ú©Î’ÏŽðŸ’¢ÎœÎŸÎÎ‘Î•ðŸ‡±â™²ðˆâ†´ðŸ’’âŠ˜È»ðŸš´ðŸ–•ðŸ–¤ðŸ¥˜ðŸ“ðŸ‘ˆâž•ðŸš«ðŸŽ¨ðŸŒ‘ðŸ»ðŽððŠð‘­ðŸ¤–ðŸŽŽðŸ˜¼ðŸ•·ï½‡ï½’ï½Žï½”ï½‰ï½„ï½•ï½†ï½‚ï½‹ðŸ°ðŸ‡´ðŸ‡­ðŸ‡»ðŸ‡²ð—žð—­ð—˜ð—¤ðŸ‘¼ðŸ“‰ðŸŸðŸ¦ðŸŒˆðŸ”­ã€ŠðŸŠðŸ\\uf10aáƒšÚ¡ðŸ¦\\U0001f92f\\U0001f92aðŸ¡ðŸ’³á¼±ðŸ™‡ð—¸ð—Ÿð— ð—·ðŸ¥œã•ã‚ˆã†ãªã‚‰ðŸ”¼'\n",
    "\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "nltk_tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "\n",
    "isolate_dict = {ord(c):f' {c} ' for c in symbols_to_isolate}\n",
    "remove_dict = {ord(c):f'' for c in symbols_to_delete}\n",
    "\n",
    "\n",
    "def handle_punctuation(x):\n",
    "    x = x.translate(remove_dict)\n",
    "    x = x.translate(isolate_dict)\n",
    "    return x\n",
    "\n",
    "def handle_contractions(x):\n",
    "    x = nltk_tokenizer.tokenize(x)\n",
    "    return x\n",
    "\n",
    "def fix_quote(x):\n",
    "    x = [x_[1:] if x_.startswith(\"'\") else x_ for x_ in x]\n",
    "    x = ' '.join(x)\n",
    "    return x\n",
    "\n",
    "def preprocess(x):\n",
    "    x = handle_punctuation(x)\n",
    "    x = handle_contractions(x)\n",
    "    x = fix_quote(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/curtis/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:18: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('../input/train.csv')\n",
    "test = pd.read_csv('../input/test.csv')\n",
    "#pd.read_csv(\"P00000001-ALL.csv\", nrows=20)\n",
    "#train = pd.read_hdf('../input/train.h5')\n",
    "#test = pd.read_hdf('../input/test.h5')\n",
    "#tqdm.pandas()\n",
    "\n",
    "identity_columns = [\n",
    "    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n",
    "    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n",
    "\n",
    "x_train = train['comment_text'].apply(lambda x:preprocess(x))\n",
    "y_train = np.where(train['target'] >= 0.5, 1, 0)\n",
    "num_train_data = y_train.shape[0]\n",
    "y_train_identity = np.where(train[identity_columns] >= 0.5, 1, 0)\n",
    "y_aux_train = train[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']]\n",
    "x_test = test['comment_text'].apply(lambda x:preprocess(x))\n",
    "y_aux_train = y_aux_train.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmax_features = None\\n\\n#Create the dictionary of all words that exist in our data\\nnlp = spacy.load(\"en_core_web_lg\", disable=[\\'parser\\',\\'ner\\',\\'tagger\\'])\\ntext_list = pd.concat([x_train, x_test])\\nnlp.vocab.add_flag(lambda s: s.lower() in spacy.lang.en.stop_words.STOP_WORDS, spacy.attrs.IS_STOP)\\nword_dict = {}\\nlemma_dict = {}\\nword_index = 1\\ndocs = nlp.pipe(text_list, n_threads = 2)\\nword_sequences = []\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "max_features = None\n",
    "\n",
    "#Create the dictionary of all words that exist in our data\n",
    "nlp = spacy.load(\"en_core_web_lg\", disable=['parser','ner','tagger'])\n",
    "text_list = pd.concat([x_train, x_test])\n",
    "nlp.vocab.add_flag(lambda s: s.lower() in spacy.lang.en.stop_words.STOP_WORDS, spacy.attrs.IS_STOP)\n",
    "word_dict = {}\n",
    "lemma_dict = {}\n",
    "word_index = 1\n",
    "docs = nlp.pipe(text_list, n_threads = 2)\n",
    "word_sequences = []\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nstart_time = time.time()\\nfor doc in tqdm(docs): #one doc is one comment(row)\\n    #print(count)\\n    word_seq = []\\n    for token in doc:\\n        if (token.text not in word_dict) and (token.pos_ is not \"PUNCT\"):\\n            word_dict[token.text] = word_index\\n            word_index += 1\\n            lemma_dict[token.text] = token.lemma_\\n        if token.pos_ is not \"PUNCT\":\\n            word_seq.append(word_dict[token.text])\\n    word_sequences.append(word_seq)\\n    #count+= 1\\n\\nprint(\"--- %s seconds ---\" % (time.time() - start_time))\\ndel docs\\ndel text_list\\ngc.collect()\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create dictionary of word mapping to integers as wel as lemma dictionary\n",
    "#count = 1\n",
    "'''\n",
    "start_time = time.time()\n",
    "for doc in tqdm(docs): #one doc is one comment(row)\n",
    "    #print(count)\n",
    "    word_seq = []\n",
    "    for token in doc:\n",
    "        if (token.text not in word_dict) and (token.pos_ is not \"PUNCT\"):\n",
    "            word_dict[token.text] = word_index\n",
    "            word_index += 1\n",
    "            lemma_dict[token.text] = token.lemma_\n",
    "        if token.pos_ is not \"PUNCT\":\n",
    "            word_seq.append(word_dict[token.text])\n",
    "    word_sequences.append(word_seq)\n",
    "    #count+= 1\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "del docs\n",
    "del text_list\n",
    "gc.collect()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 400000\n",
    "tokenizer = text.Tokenizer(num_words = max_features, filters='',lower=False)\n",
    "tokenizer.fit_on_texts(list(x_train) + list(x_test))\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_test = tokenizer.texts_to_sequences(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#max_features = max_features or len(word_dict) + 1\n",
    "max_features = max_features or len(tokenizer.word_index) + 1\n",
    "max_features #number of unique words there are in the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n unknown words (crawl):  222943\n",
      "--- 79.53426361083984 seconds ---\n",
      "Size:  (487814, 300)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "#crawl_matrix, unknown_words_crawl = build_matrix(word_dict, lemma_dict, CRAWL_EMBEDDING_PATH)\n",
    "crawl_matrix, unknown_words_crawl = build_matrix(tokenizer.word_index, CRAWL_EMBEDDING_PATH)\n",
    "print('n unknown words (crawl): ', len(unknown_words_crawl))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print('Size: ', crawl_matrix.shape)\n",
    "del unknown_words_crawl\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n unknown words (glove):  225246\n",
      "--- 90.23643827438354 seconds ---\n",
      "Size:  (487814, 300)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "#glove_matrix, unknown_words_glove = build_matrix(word_dict, lemma_dict, GLOVE_EMBEDDING_PATH)\n",
    "glove_matrix, unknown_words_glove = build_matrix(tokenizer.word_index, GLOVE_EMBEDDING_PATH)\n",
    "print('n unknown words (glove): ', len(unknown_words_glove))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print('Size: ', glove_matrix.shape)\n",
    "del unknown_words_glove\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = np.concatenate([crawl_matrix, glove_matrix], axis=-1)\n",
    "\n",
    "del crawl_matrix\n",
    "del glove_matrix\n",
    "#del word_dict\n",
    "#del lemma_dict\n",
    "#del WORDS\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JigsawEvaluator:\n",
    "\n",
    "    def __init__(self, y_true, y_identity, power=-5, overall_model_weight=0.25):\n",
    "        self.y = y_true\n",
    "        self.y_i = y_identity\n",
    "        self.n_subgroups = self.y_i.shape[1]\n",
    "        self.power = power\n",
    "        self.overall_model_weight = overall_model_weight\n",
    "\n",
    "    @staticmethod\n",
    "    def _compute_auc(y_true, y_pred):\n",
    "        #print(\"Here: \", y_true)\n",
    "        #print(y_pred)\n",
    "        try:\n",
    "            return roc_auc_score(y_true, y_pred)\n",
    "        except ValueError:\n",
    "            return np.nan\n",
    "\n",
    "    def _compute_subgroup_auc(self, i, y_pred):\n",
    "        mask = self.y_i[:, i] == 1\n",
    "        #print(self.y)\n",
    "        return self._compute_auc(self.y[mask], y_pred[mask])\n",
    "\n",
    "    def _compute_bpsn_auc(self, i, y_pred):\n",
    "        mask = self.y_i[:, i] + self.y == 1\n",
    "        return self._compute_auc(self.y[mask], y_pred[mask])\n",
    "\n",
    "    def _compute_bnsp_auc(self, i, y_pred):\n",
    "        mask = self.y_i[:, i] + self.y != 1\n",
    "        return self._compute_auc(self.y[mask], y_pred[mask])\n",
    "\n",
    "    def compute_bias_metrics_for_model(self, y_pred):\n",
    "        #print(y_pred)\n",
    "        records = np.zeros((3, self.n_subgroups))\n",
    "        for i in range(self.n_subgroups):\n",
    "            #print(y_pred)\n",
    "            records[0, i] = self._compute_subgroup_auc(i, y_pred)\n",
    "            records[1, i] = self._compute_bpsn_auc(i, y_pred)\n",
    "            records[2, i] = self._compute_bnsp_auc(i, y_pred)\n",
    "        return records\n",
    "\n",
    "    def _calculate_overall_auc(self, y_pred):\n",
    "        return roc_auc_score(self.y, y_pred)\n",
    "\n",
    "    def _power_mean(self, array):\n",
    "        total = sum(np.power(array, self.power))\n",
    "        return np.power(total / len(array), 1 / self.power)\n",
    "\n",
    "    def get_final_metric(self, y_pred):\n",
    "        bias_metrics = self.compute_bias_metrics_for_model(y_pred)\n",
    "        bias_score = np.average([\n",
    "            self._power_mean(bias_metrics[0]),\n",
    "            self._power_mean(bias_metrics[1]),\n",
    "            self._power_mean(bias_metrics[2])\n",
    "        ])\n",
    "        overall_score = self.overall_model_weight * self._calculate_overall_auc(y_pred)\n",
    "        bias_score = (1 - self.overall_model_weight) * bias_score\n",
    "        return overall_score + bias_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1804874\n",
      "torch.Size([1804874])\n",
      "188\n",
      "tensor(713)\n"
     ]
    }
   ],
   "source": [
    "#x_train = word_sequences[:num_train_data]\n",
    "#x_test = word_sequences[num_train_data:]\n",
    "lengths = torch.from_numpy(np.array([len(x) for x in x_train]))\n",
    "test_lengths = torch.from_numpy(np.array([len(x) for x in x_test]))\n",
    "print(len(x_train))\n",
    "print(lengths.shape)\n",
    "maxlen = int(np.percentile(lengths, 95)) \n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "print(maxlen)\n",
    "print(lengths.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1804874, 188)\n",
      "(1804874,)\n",
      "1804874\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(num_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: PRELIM FILTER\n",
      "=> Saving a new best\n",
      "Epoch 1/4 \t loss=0.1110 \t time=654.27s\n",
      "=> Saving a new best\n",
      "Epoch 2/4 \t loss=0.1040 \t time=657.24s\n",
      "=> Saving a new best\n",
      "Epoch 3/4 \t loss=0.1027 \t time=658.25s\n",
      "=> Saving a new best\n",
      "Epoch 4/4 \t loss=0.1018 \t time=658.48s\n",
      "====================== END OF FILTER =============================\n"
     ]
    }
   ],
   "source": [
    "#Here, we use one model that filters out all the easy predictions\n",
    "x_train_torch = torch.tensor(x_train, dtype=torch.long).cuda()\n",
    "y_train_torch = torch.tensor(np.hstack([y_train[:, np.newaxis], y_aux_train]), dtype=torch.float32).cuda()\n",
    "train_dataset = data.TensorDataset(x_train_torch, lengths ,y_train_torch)\n",
    "\n",
    "print('Model: PRELIM FILTER')\n",
    "seed_everything(1234)\n",
    "\n",
    "model = NeuralNet(embedding_matrix, y_aux_train.shape[-1])\n",
    "model.cuda()\n",
    "\n",
    "#training using training and validation set\n",
    "model = train_model(model, train_dataset, train_dataset, output_dim=y_train_torch.shape[-1], \n",
    "                         loss_fn=nn.BCEWithLogitsLoss(reduction='mean'))\n",
    "\n",
    "\n",
    "#prediction on entire train set (actual predictions to be submitted)\n",
    "pred = predict(model, train_dataset, output_dim=y_train_torch.shape[-1])\n",
    "\n",
    "        \n",
    "print('====================== END OF FILTER =============================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CLEAR \n",
    "del x_train_torch\n",
    "del y_train_torch\n",
    "del train_dataset\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#AND GET DISTRIBUTIONS OF PREDICTIONS\n",
    "plt.hist(pred)\n",
    "plt.title(\"Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1% of comments have a score of less than 9.652673790697008e-05\n",
      "2% of comments have a score of less than 0.0001583055141963996\n",
      "3% of comments have a score of less than 0.00024051292217336595\n",
      "4% of comments have a score of less than 0.0003585173445753753\n",
      "5% of comments have a score of less than 0.0004796827561222017\n",
      "6% of comments have a score of less than 0.0005651388259138912\n",
      "7% of comments have a score of less than 0.0006377371691633016\n",
      "8% of comments have a score of less than 0.0007059412589296699\n",
      "9% of comments have a score of less than 0.0007715343963354826\n",
      "10% of comments have a score of less than 0.0008338902844116092\n",
      "11% of comments have a score of less than 0.0008919966639950871\n",
      "12% of comments have a score of less than 0.0009456573915667833\n",
      "13% of comments have a score of less than 0.0009949334990233183\n",
      "14% of comments have a score of less than 0.0010408510593697428\n",
      "15% of comments have a score of less than 0.0010845016222447157\n",
      "16% of comments have a score of less than 0.0011262454325333237\n",
      "17% of comments have a score of less than 0.0011672666296362877\n",
      "18% of comments have a score of less than 0.001207595260348171\n",
      "19% of comments have a score of less than 0.0012478829594329\n",
      "20% of comments have a score of less than 0.0012884115567430856\n",
      "21% of comments have a score of less than 0.0013296151300892234\n",
      "22% of comments have a score of less than 0.0013714728923514487\n",
      "23% of comments have a score of less than 0.001414638754213229\n",
      "24% of comments have a score of less than 0.0014594064559787512\n",
      "25% of comments have a score of less than 0.0015059564029797912\n",
      "26% of comments have a score of less than 0.001554381800815463\n",
      "27% of comments have a score of less than 0.0016054154839366674\n",
      "28% of comments have a score of less than 0.0016590590821579099\n",
      "29% of comments have a score of less than 0.0017158343689516187\n",
      "30% of comments have a score of less than 0.001776435412466526\n",
      "31% of comments have a score of less than 0.0018404921656474471\n",
      "32% of comments have a score of less than 0.0019087290856987238\n",
      "33% of comments have a score of less than 0.0019812612887471917\n",
      "34% of comments have a score of less than 0.002058817073702812\n",
      "35% of comments have a score of less than 0.002141911769285798\n",
      "36% of comments have a score of less than 0.0022310460917651653\n",
      "37% of comments have a score of less than 0.0023266678117215634\n",
      "38% of comments have a score of less than 0.0024295157846063375\n",
      "39% of comments have a score of less than 0.002540118992328644\n",
      "40% of comments have a score of less than 0.002658935496583581\n",
      "41% of comments have a score of less than 0.0027868433040566737\n",
      "42% of comments have a score of less than 0.0029245326295495033\n",
      "43% of comments have a score of less than 0.003072971710935235\n",
      "44% of comments have a score of less than 0.0032333943527191877\n",
      "45% of comments have a score of less than 0.003405093192122877\n",
      "46% of comments have a score of less than 0.0035914997570216656\n",
      "47% of comments have a score of less than 0.003791736846324052\n",
      "48% of comments have a score of less than 0.004006253853440285\n",
      "49% of comments have a score of less than 0.00423491420224309\n",
      "50% of comments have a score of less than 0.004478659247979522\n",
      "51% of comments have a score of less than 0.004739206749945879\n",
      "52% of comments have a score of less than 0.005017643887549638\n",
      "53% of comments have a score of less than 0.005313807660713795\n",
      "54% of comments have a score of less than 0.0056284322217106855\n",
      "55% of comments have a score of less than 0.005965487612411381\n",
      "56% of comments have a score of less than 0.00632356246933341\n",
      "57% of comments have a score of less than 0.006705731605179605\n",
      "58% of comments have a score of less than 0.007113862736150618\n",
      "59% of comments have a score of less than 0.007552286707796154\n",
      "60% of comments have a score of less than 0.008023591712117195\n",
      "61% of comments have a score of less than 0.008525350131094456\n",
      "62% of comments have a score of less than 0.009068542961031198\n",
      "63% of comments have a score of less than 0.00965511198155582\n",
      "64% of comments have a score of less than 0.01028275191783905\n",
      "65% of comments have a score of less than 0.01096758060157299\n",
      "66% of comments have a score of less than 0.011706734169274572\n",
      "67% of comments have a score of less than 0.012509658932685852\n",
      "68% of comments have a score of less than 0.013382652774453163\n",
      "69% of comments have a score of less than 0.014332008901983492\n",
      "70% of comments have a score of less than 0.015371972694993019\n",
      "71% of comments have a score of less than 0.01651302231475711\n",
      "72% of comments have a score of less than 0.017763339281082157\n",
      "73% of comments have a score of less than 0.01915008693933487\n",
      "74% of comments have a score of less than 0.020684620179235936\n",
      "75% of comments have a score of less than 0.022396211978048086\n",
      "76% of comments have a score of less than 0.024311068207025523\n",
      "77% of comments have a score of less than 0.02645148606970906\n",
      "78% of comments have a score of less than 0.028854546323418613\n",
      "79% of comments have a score of less than 0.03157337330281734\n",
      "80% of comments have a score of less than 0.03468832075595857\n",
      "81% of comments have a score of less than 0.03822129048407079\n",
      "82% of comments have a score of less than 0.04227576553821562\n",
      "83% of comments have a score of less than 0.04697448138147592\n",
      "84% of comments have a score of less than 0.052385530620813345\n",
      "85% of comments have a score of less than 0.05878251343965529\n",
      "86% of comments have a score of less than 0.06635297060012812\n",
      "87% of comments have a score of less than 0.07532927908003323\n",
      "88% of comments have a score of less than 0.0860590317845345\n",
      "89% of comments have a score of less than 0.09881547093391418\n",
      "90% of comments have a score of less than 0.11425168812274944\n",
      "91% of comments have a score of less than 0.13307727679610273\n",
      "92% of comments have a score of less than 0.15671518445014954\n",
      "93% of comments have a score of less than 0.18639047160744668\n",
      "94% of comments have a score of less than 0.2243790417909618\n",
      "95% of comments have a score of less than 0.27400245070456997\n",
      "96% of comments have a score of less than 0.3387580931186682\n",
      "97% of comments have a score of less than 0.4261223515868187\n",
      "98% of comments have a score of less than 0.5461580252647402\n",
      "99% of comments have a score of less than 0.7143878722190857\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 100):\n",
    "    thres = np.percentile(pred, i)\n",
    "    print('{}% of comments have a score of less than {}'.format(i, thres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model  0\n",
      "=> Saving a new best\n",
      "Epoch 1/4 \t loss=0.3344 \t time=2.41s\n",
      "=> Saving a new best\n",
      "Epoch 2/4 \t loss=0.1517 \t time=2.59s\n",
      "=> Saving a new best\n",
      "Epoch 3/4 \t loss=0.1408 \t time=2.52s\n",
      "=> Saving a new best\n",
      "Epoch 4/4 \t loss=0.1392 \t time=2.58s\n",
      "\n",
      "Model  1\n",
      "=> Saving a new best\n",
      "Epoch 1/4 \t loss=0.3379 \t time=2.59s\n",
      "=> Saving a new best\n",
      "Epoch 2/4 \t loss=0.1564 \t time=2.52s\n",
      "=> Saving a new best\n",
      "Epoch 3/4 \t loss=0.1426 \t time=2.54s\n",
      "=> Saving a new best\n",
      "Epoch 4/4 \t loss=0.1400 \t time=2.54s\n",
      "\n",
      "Kaggle Score:  0.38996508687675613\n",
      "ROC score:  0.6015915119363395\n",
      "=============End-of-Fold================\n",
      "Model  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/curtis/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:46: RuntimeWarning: divide by zero encountered in power\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving a new best\n",
      "Epoch 1/4 \t loss=0.3336 \t time=2.65s\n",
      "=> Saving a new best\n",
      "Epoch 2/4 \t loss=0.1499 \t time=2.61s\n",
      "=> Saving a new best\n",
      "Epoch 3/4 \t loss=0.1400 \t time=2.59s\n",
      "=> Saving a new best\n",
      "Epoch 4/4 \t loss=0.1380 \t time=2.60s\n",
      "\n",
      "Model  1\n",
      "=> Saving a new best\n",
      "Epoch 1/4 \t loss=0.3379 \t time=2.56s\n",
      "=> Saving a new best\n",
      "Epoch 2/4 \t loss=0.1498 \t time=2.57s\n",
      "=> Saving a new best\n",
      "Epoch 3/4 \t loss=0.1406 \t time=2.56s\n",
      "=> Saving a new best\n",
      "Epoch 4/4 \t loss=0.1379 \t time=2.60s\n",
      "\n",
      "Kaggle Score:  nan\n",
      "ROC score:  0.6420241470776549\n",
      "=============End-of-Fold================\n",
      "Model  0\n",
      "=> Saving a new best\n",
      "Epoch 1/4 \t loss=0.3353 \t time=2.56s\n",
      "=> Saving a new best\n",
      "Epoch 2/4 \t loss=0.1500 \t time=2.57s\n",
      "=> Saving a new best\n",
      "Epoch 3/4 \t loss=0.1399 \t time=2.57s\n",
      "=> Saving a new best\n",
      "Epoch 4/4 \t loss=0.1387 \t time=2.57s\n",
      "\n",
      "Model  1\n",
      "=> Saving a new best\n",
      "Epoch 1/4 \t loss=0.3393 \t time=2.61s\n",
      "=> Saving a new best\n",
      "Epoch 2/4 \t loss=0.1509 \t time=2.58s\n",
      "=> Saving a new best\n",
      "Epoch 3/4 \t loss=0.1399 \t time=2.58s\n",
      "=> Saving a new best\n",
      "Epoch 4/4 \t loss=0.1381 \t time=2.57s\n",
      "\n",
      "Kaggle Score:  0.4247580939404688\n",
      "ROC score:  0.560102313492935\n",
      "=============End-of-Fold================\n",
      "Model  0\n",
      "=> Saving a new best\n",
      "Epoch 1/4 \t loss=0.3326 \t time=2.57s\n",
      "=> Saving a new best\n",
      "Epoch 2/4 \t loss=0.1514 \t time=2.57s\n",
      "=> Saving a new best\n",
      "Epoch 3/4 \t loss=0.1410 \t time=2.60s\n",
      "=> Saving a new best\n",
      "Epoch 4/4 \t loss=0.1392 \t time=2.60s\n",
      "\n",
      "Model  1\n",
      "=> Saving a new best\n",
      "Epoch 1/4 \t loss=0.3452 \t time=2.65s\n",
      "=> Saving a new best\n",
      "Epoch 2/4 \t loss=0.1528 \t time=2.57s\n",
      "=> Saving a new best\n",
      "Epoch 3/4 \t loss=0.1417 \t time=2.58s\n",
      "=> Saving a new best\n",
      "Epoch 4/4 \t loss=0.1396 \t time=2.57s\n",
      "\n",
      "Kaggle Score:  0.6596854363944484\n",
      "ROC score:  0.574808455644789\n",
      "=============End-of-Fold================\n",
      "Model  0\n",
      "=> Saving a new best\n",
      "Epoch 1/4 \t loss=0.3350 \t time=2.57s\n",
      "=> Saving a new best\n",
      "Epoch 2/4 \t loss=0.1518 \t time=2.58s\n",
      "=> Saving a new best\n",
      "Epoch 3/4 \t loss=0.1422 \t time=2.65s\n",
      "=> Saving a new best\n",
      "Epoch 4/4 \t loss=0.1402 \t time=2.61s\n",
      "\n",
      "Model  1\n",
      "=> Saving a new best\n",
      "Epoch 1/4 \t loss=0.3433 \t time=2.56s\n",
      "=> Saving a new best\n",
      "Epoch 2/4 \t loss=0.1531 \t time=2.60s\n",
      "=> Saving a new best\n",
      "Epoch 3/4 \t loss=0.1419 \t time=2.59s\n",
      "=> Saving a new best\n",
      "Epoch 4/4 \t loss=0.1396 \t time=2.59s\n",
      "\n",
      "Kaggle Score:  nan\n",
      "ROC score:  0.6313486568817502\n",
      "=============End-of-Fold================\n",
      "Time:  121.70721864700317\n",
      "Final Kaggle Score:  0.6116510369068346\n",
      "Final ROC score:  0.5939795068451295\n"
     ]
    }
   ],
   "source": [
    "\n",
    "all_val_preds = []\n",
    "all_test_preds = []\n",
    "num_splits = 5\n",
    "\n",
    "#Add in K fold \n",
    "random_state = 2019\n",
    "\n",
    "#K fold splits\n",
    "splits = list(StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=random_state).split(x_train,y_train))\n",
    "\n",
    "#final validation predictions\n",
    "final_val_preds = np.zeros((x_train.shape[0]))\n",
    "\n",
    "#final test predictions to be stored in this var\n",
    "final_test_preds = np.zeros((x_test.shape[0]))\n",
    "\n",
    "start_time = time.time()\n",
    "for fold in range(num_splits):\n",
    "    tr_ind, val_ind = splits[fold]\n",
    "    all_val_preds = []\n",
    "    all_test_preds = []\n",
    "    #print('Training set size: ', len(tr_ind))\n",
    "    #print('Val set size: ', len(val_ind))\n",
    "    x_training = x_train[tr_ind]\n",
    "    y_training = y_train[tr_ind]\n",
    "    y_aux_training = y_aux_train[tr_ind]\n",
    "    \n",
    "    x_val = x_train[val_ind]\n",
    "    y_val = y_train[val_ind]\n",
    "    y_aux_val = y_aux_train[val_ind]\n",
    "    \n",
    "    \n",
    "    \n",
    "    x_train_torch = torch.tensor(x_training, dtype=torch.long).cuda()\n",
    "    x_val_torch = torch.tensor(x_val, dtype=torch.long).cuda()\n",
    "    y_train_torch = torch.tensor(np.hstack([y_training[:, np.newaxis], y_aux_training]), dtype=torch.float32).cuda()\n",
    "    y_val_torch = torch.tensor(np.hstack([y_val[:, np.newaxis], y_aux_val]), dtype=torch.float32).cuda()\n",
    "    \n",
    "    x_test_torch = torch.tensor(x_test, dtype=torch.long).cuda()\n",
    "    \n",
    "    ###\n",
    "    \n",
    "    #test_dataset = data.TensorDataset(x_test_torch, test_lengths)\n",
    "    #train_dataset = data.TensorDataset(x_train_torch, lengths, y_train_torch)\n",
    "    #val_dataset = data.TensorDataset(x_val_torch)\n",
    "\n",
    "    #train_collator = SequenceBucketCollator(lambda lengths: lengths.max(), sequence_index=0, length_index=1, label_index=2)\n",
    "    #test_collator = SequenceBucketCollator(lambda lengths: lengths.max(), sequence_index=0, length_index=1)\n",
    "    \n",
    "    ####\n",
    "    train_dataset = data.TensorDataset(x_train_torch, lengths[tr_ind], y_train_torch)\n",
    "    val_dataset = data.TensorDataset(x_val_torch, lengths[val_ind], y_val_torch)\n",
    "    test_dataset = data.TensorDataset(x_test_torch, test_lengths)\n",
    "    \n",
    "    #temp_dataset = data.Subset(train_dataset, indices=[0, 1])\n",
    "\n",
    "    for model_idx in range(NUM_MODELS):\n",
    "        print('Model ', model_idx)\n",
    "        seed_everything(1234 + model_idx)\n",
    "\n",
    "        model = NeuralNet(embedding_matrix, y_aux_train.shape[-1])\n",
    "        model.cuda()\n",
    "\n",
    "        #training using training and validation set\n",
    "        model = train_model(model, train_dataset, val_dataset, output_dim=y_train_torch.shape[-1], loss_fn=nn.BCEWithLogitsLoss(reduction='mean'))\n",
    "        \n",
    "        #prediction on validation set (used for score measurement)\n",
    "        val_pred = predict(model, val_dataset, output_dim=y_train_torch.shape[-1], pred_type=\"val\") #val preds on the val split\n",
    "        all_val_preds.append(val_pred)\n",
    "        #print(len(val_pred))\n",
    "        \n",
    "        #prediction on entire test set (actual predictions to be submitted)\n",
    "        test_pred = predict(model, test_dataset, output_dim=y_train_torch.shape[-1], pred_type=\"test\")\n",
    "        all_test_preds.append(test_pred)\n",
    "        \n",
    "        print()\n",
    "        \n",
    "    #average validation prediction amongst all models\n",
    "    avg_val = np.mean(all_val_preds, axis=0)[:, 0] #will be printed out per split\n",
    "    final_val_preds[val_ind] += avg_val\n",
    "    \n",
    "    avg_test = np.mean(all_test_preds, axis=0)[:, 0]\n",
    "    \n",
    "    final_test_preds += avg_test\n",
    "\n",
    "    y_true = y_train[val_ind] #true scores for this validation set\n",
    "    y_identity = y_train_identity[val_ind] #true scores for the identity groups for this validation set\n",
    "    evaluator = JigsawEvaluator(y_true, y_identity)\n",
    "    auc_score = evaluator.get_final_metric(avg_val)\n",
    "\n",
    "    roc_score = roc_auc_score(y_train[val_ind], avg_val)\n",
    "    print('Kaggle Score: ', auc_score)\n",
    "    print('ROC score: ', roc_score)\n",
    "    \n",
    "    del x_train_torch\n",
    "    del x_val_torch\n",
    "    del y_train_torch\n",
    "    del y_val_torch\n",
    "    del x_test_torch\n",
    "    del train_dataset\n",
    "    del val_dataset\n",
    "    del test_dataset\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print('=============End-of-Fold================')\n",
    "    \n",
    "end_time = time.time()\n",
    "print('Time: ', end_time - start_time)\n",
    "\n",
    "#Final combined score\n",
    "y_true = y_train\n",
    "y_identity = y_train_identity\n",
    "evaluator = JigsawEvaluator(y_true, y_identity)\n",
    "auc_score = evaluator.get_final_metric(final_val_preds)\n",
    "print('Final Kaggle Score: ', auc_score)\n",
    "print('Final ROC score: ', roc_auc_score(y_train, final_val_preds))\n",
    "\n",
    "#average test predictions AGAIN this time by number of splits\n",
    "final_test_preds /= num_splits\n",
    "#print(final_test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7000000</td>\n",
       "      <td>0.002171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7000001</td>\n",
       "      <td>0.000101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7000002</td>\n",
       "      <td>0.003995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7000003</td>\n",
       "      <td>0.003117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7000004</td>\n",
       "      <td>0.982500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  prediction\n",
       "0  7000000    0.002171\n",
       "1  7000001    0.000101\n",
       "2  7000002    0.003995\n",
       "3  7000003    0.003117\n",
       "4  7000004    0.982500"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.DataFrame.from_dict({\n",
    "    'id': test['id'],\n",
    "    'prediction': final_test_preds\n",
    "})\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
