{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "import random\n",
    "from tqdm._tqdm_notebook import tqdm_notebook as tqdm\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "lc = LancasterStemmer()\n",
    "from nltk.stem import SnowballStemmer\n",
    "sb = SnowballStemmer(\"english\")\n",
    "from keras.preprocessing import text, sequence\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from torch.nn import functional as F\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import spacy\n",
    "import gensim\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import apex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable progress bars when submitting\n",
    "def is_interactive():\n",
    "   return 'SHLVL' not in os.environ\n",
    "\n",
    "if not is_interactive():\n",
    "    def nop(it, *a, **k):\n",
    "        return it\n",
    "\n",
    "    tqdm = nop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CRAWL_EMBEDDING_PATH = '../input/crawl-300d-2M.vec'\n",
    "GLOVE_EMBEDDING_PATH = '../input/glove.840B.300d.txt'\n",
    "NUM_MODELS = 2\n",
    "LSTM_UNITS = 128\n",
    "DENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\n",
    "MAX_LEN = 220"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nstart_time = time.time()\\nspell_model = gensim.models.KeyedVectors.load_word2vec_format(\\'../input/wiki-news-300d-1M.vec\\')\\nwords = spell_model.index2word\\nw_rank = {}\\nfor i,word in enumerate(words):\\n    w_rank[word] = i\\nWORDS = w_rank\\ndel words\\ndel w_rank\\ndel spell_model\\ngc.collect()\\nprint(\"--- %s seconds ---\" % (time.time() - start_time))\\n\\n# Use fast text as vocabulary\\ndef words(text): return re.findall(r\\'\\\\w+\\', text.lower())\\ndef P(word): \\n    \"Probability of `word`.\"\\n    # use inverse of rank as proxy\\n    # returns 0 if the word isn\\'t in the dictionary\\n    return - WORDS.get(word, 0)\\ndef correction(word): \\n    \"Most probable spelling correction for word.\"\\n    return max(candidates(word), key=P)\\ndef candidates(word): \\n    \"Generate possible spelling corrections for word.\"\\n    return (known([word]) or known(edits1(word)) or [word])\\ndef known(words): \\n    \"The subset of `words` that appear in the dictionary of WORDS.\"\\n    return set(w for w in words if w in WORDS)\\ndef edits1(word):\\n    \"All edits that are one edit away from `word`.\"\\n    letters    = \\'abcdefghijklmnopqrstuvwxyz\\'\\n    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\\n    deletes    = [L + R[1:]               for L, R in splits if R]\\n    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\\n    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\\n    inserts    = [L + c + R               for L, R in splits for c in letters]\\n    return set(deletes + transposes + replaces + inserts)\\ndef edits2(word): \\n    \"All edits that are two edits away from `word`.\"\\n    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\\ndef singlify(word):\\n    return \"\".join([letter for i,letter in enumerate(word) if i == 0 or letter != word[i-1]])\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "start_time = time.time()\n",
    "spell_model = gensim.models.KeyedVectors.load_word2vec_format('../input/wiki-news-300d-1M.vec')\n",
    "words = spell_model.index2word\n",
    "w_rank = {}\n",
    "for i,word in enumerate(words):\n",
    "    w_rank[word] = i\n",
    "WORDS = w_rank\n",
    "del words\n",
    "del w_rank\n",
    "del spell_model\n",
    "gc.collect()\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# Use fast text as vocabulary\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "def P(word): \n",
    "    \"Probability of `word`.\"\n",
    "    # use inverse of rank as proxy\n",
    "    # returns 0 if the word isn't in the dictionary\n",
    "    return - WORDS.get(word, 0)\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or [word])\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "def singlify(word):\n",
    "    return \"\".join([letter for i,letter in enumerate(word) if i == 0 or letter != word[i-1]])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef build_matrix(word_dict, lemma_dict, path):\\n    embed_size = 300\\n    embeddings_index = load_embeddings(path)\\n    embedding_matrix = np.zeros((len(word_dict) + 1, embed_size), dtype=np.float32)\\n    unknown_words = []\\n    unknown_vector = np.zeros((embed_size,), dtype=np.float32) - 1\\n    \\n    for key in tqdm(word_dict):\\n        word = key\\n        embedding_vector = embeddings_index.get(word)\\n        if embedding_vector is not None:\\n            embedding_matrix[word_dict[key]] = embedding_vector\\n            continue\\n        word = key.lower()\\n        embedding_vector = embeddings_index.get(word)\\n        if embedding_vector is not None:\\n            embedding_matrix[word_dict[key]] = embedding_vector\\n            continue\\n        word = key.upper()\\n        embedding_vector = embeddings_index.get(word)\\n        if embedding_vector is not None:\\n            embedding_matrix[word_dict[key]] = embedding_vector\\n            continue\\n        word = key.capitalize()\\n        embedding_vector = embeddings_index.get(word)\\n        if embedding_vector is not None:\\n            embedding_matrix[word_dict[key]] = embedding_vector\\n            continue\\n        word = ps.stem(key)\\n        embedding_vector = embeddings_index.get(word)\\n        if embedding_vector is not None:\\n            embedding_matrix[word_dict[key]] = embedding_vector\\n            continue\\n        word = lc.stem(key)\\n        embedding_vector = embeddings_index.get(word)\\n        if embedding_vector is not None:\\n            embedding_matrix[word_dict[key]] = embedding_vector\\n            continue\\n        word = sb.stem(key)\\n        embedding_vector = embeddings_index.get(word)\\n        if embedding_vector is not None:\\n            embedding_matrix[word_dict[key]] = embedding_vector\\n            continue\\n        word = lemma_dict[key]\\n        embedding_vector = embeddings_index.get(word)\\n        if embedding_vector is not None:\\n            embedding_matrix[word_dict[key]] = embedding_vector\\n            continue\\n        if len(key) > 1:\\n            word = correction(key)\\n            embedding_vector = embeddings_index.get(word)\\n            if embedding_vector is not None:\\n                embedding_matrix[word_dict[key]] = embedding_vector\\n                continue\\n        \\n        #Unknown word, does not exist in dictionary\\n        embedding_matrix[word_dict[key]] = unknown_vector\\n        unknown_words.append(word)\\n    return embedding_matrix, unknown_words\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "def load_embeddings(path):\n",
    "    with open(path) as f:\n",
    "        return dict(get_coefs(*line.strip().split(' ')) for line in tqdm(f))\n",
    "    \n",
    "def build_matrix(word_index, path):\n",
    "    embedding_index = load_embeddings(path)\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "    unknown_words = []\n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        try:\n",
    "            embedding_matrix[i] = embedding_index[word]\n",
    "        except KeyError:\n",
    "            unknown_words.append(word)\n",
    "    return embedding_matrix, unknown_words\n",
    "\n",
    "'''\n",
    "def build_matrix(word_dict, lemma_dict, path):\n",
    "    embed_size = 300\n",
    "    embeddings_index = load_embeddings(path)\n",
    "    embedding_matrix = np.zeros((len(word_dict) + 1, embed_size), dtype=np.float32)\n",
    "    unknown_words = []\n",
    "    unknown_vector = np.zeros((embed_size,), dtype=np.float32) - 1\n",
    "    \n",
    "    for key in tqdm(word_dict):\n",
    "        word = key\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = key.lower()\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = key.upper()\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = key.capitalize()\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = ps.stem(key)\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = lc.stem(key)\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = sb.stem(key)\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = lemma_dict[key]\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        if len(key) > 1:\n",
    "            word = correction(key)\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[word_dict[key]] = embedding_vector\n",
    "                continue\n",
    "        \n",
    "        #Unknown word, does not exist in dictionary\n",
    "        embedding_matrix[word_dict[key]] = unknown_vector\n",
    "        unknown_words.append(word)\n",
    "    return embedding_matrix, unknown_words\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceBucketCollator():\n",
    "    def __init__(self, choose_length, sequence_index, length_index, label_index=None):\n",
    "        self.choose_length = choose_length\n",
    "        self.sequence_index = sequence_index\n",
    "        self.length_index = length_index\n",
    "        self.label_index = label_index\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        batch = [torch.stack(x) for x in list(zip(*batch))]\n",
    "        \n",
    "        sequences = batch[self.sequence_index]\n",
    "        lengths = batch[self.length_index]\n",
    "        \n",
    "        length = self.choose_length(lengths)\n",
    "        mask = torch.arange(start=maxlen, end=0, step=-1) < length\n",
    "        padded_sequences = sequences[:, mask]\n",
    "        \n",
    "        batch[self.sequence_index] = padded_sequences\n",
    "        \n",
    "        if self.label_index is not None:\n",
    "            return [x for i, x in enumerate(batch) if i != self.label_index], batch[self.label_index]\n",
    "    \n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "filepath = './model_files/checkpoint.pth'\n",
    "\n",
    "def train_model(model, train, test, loss_fn, output_dim, lr=0.001,\n",
    "                batch_size=512, n_epochs=4, n_epochs_embed=2,\n",
    "                enable_checkpoint_ensemble=True):\n",
    "    \n",
    "    train_collator = SequenceBucketCollator(lambda lengths: lengths.max(), \n",
    "                                            sequence_index=0, \n",
    "                                            length_index=1, \n",
    "                                            label_index=2)\n",
    "    \n",
    "    param_lrs = [{'params': param, 'lr': lr} for param in model.parameters()]\n",
    "    optimizer = torch.optim.Adam(param_lrs, lr=lr)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda epoch: 0.6 ** epoch)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True, collate_fn=train_collator)\n",
    "    val_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False, collate_fn=train_collator)\n",
    "    all_test_preds = []\n",
    "    checkpoint_weights = [2 ** epoch for epoch in range(n_epochs)]\n",
    "    \n",
    "    best_loss = 1\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        model.train() #set model to train mode\n",
    "        avg_loss = 0.\n",
    "        \n",
    "        for data in tqdm(train_loader, disable=False):\n",
    "            \n",
    "            #training loop\n",
    "            x_batch = data[:-1]\n",
    "            #print(\"First: \", x_batch[0][0])\n",
    "            #print(\"Second: \", x_batch[0][1])\n",
    "            first = x_batch[0][0]\n",
    "            second = x_batch[0][1]\n",
    "            \n",
    "            y_batch = data[-1]\n",
    "\n",
    "            y_pred = model(first, second)  #feed data into model          \n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            \n",
    "            #calculate error and adjust model params\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item() / len(train_loader)\n",
    "        \n",
    "        #Check if loss is better than current best loss, if so, save the model\n",
    "        is_best = (avg_loss < best_loss)\n",
    "        \n",
    "        if is_best:\n",
    "            print (\"=> Saving a new best\")\n",
    "            best_loss = avg_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_loss': best_loss\n",
    "            }, filepath)  # save checkpoint\n",
    "        else:\n",
    "            print (\"=> Model Accuracy did not improve\")\n",
    "            \n",
    "        \n",
    "        model.eval() #set model to eval mode for test data\n",
    "        test_preds = np.zeros((len(test), output_dim))\n",
    "        \n",
    "    \n",
    "        for i, x_batch in enumerate(val_loader):\n",
    "            #print(\"X_Batch: \", x_batch)\n",
    "            data_param = x_batch[0][0]\n",
    "            lengths_param = x_batch[0][1]\n",
    "            y_pred = sigmoid(model(data_param, lengths_param).detach().cpu().numpy()) #feed data into model\n",
    "\n",
    "            test_preds[i * batch_size:(i+1) * batch_size, :] = y_pred #get test predictions\n",
    "        \n",
    "        #test_preds has the predictions for the entire test set now\n",
    "        all_test_preds.append(test_preds) #append predictions to the record of all past predictions\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print('Epoch {}/{} \\t loss={:.4f} \\t time={:.2f}s'.format(\n",
    "              epoch + 1, n_epochs, avg_loss, elapsed_time))\n",
    "        \n",
    "    #Make embeddings layer only layer unfreezed, train again (literally run through the n_epochs)\n",
    "    #maybe define a n_epochs_embedding\n",
    "    \n",
    "    #parameters = model.parameters()\n",
    "    #for param in parameters:\n",
    "    #        param.requires_grad = False\n",
    "    #parameters[0].requires_grad = True\n",
    "    \n",
    "    '''\n",
    "    ct = 0\n",
    "    for child in model.children():\n",
    "        if ct == 0:\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = True\n",
    "        else:\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = False\n",
    "        ct += 1\n",
    "    \n",
    "    for epoch in range(n_epochs_embed):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        model.train() #set model to train mode\n",
    "        avg_loss = 0.\n",
    "        \n",
    "        for data in tqdm(train_loader, disable=False):\n",
    "            \n",
    "            #training loop\n",
    "            x_batch = data[:-1]\n",
    "            y_batch = data[-1]\n",
    "\n",
    "            y_pred = model(*x_batch)  #feed data into model          \n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            \n",
    "            #calculate error and adjust model params\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item() / len(train_loader) #gets the loss per epoch\n",
    "        \n",
    "            \n",
    "        model.eval() #set model to eval mode for test data\n",
    "        test_preds = np.zeros((len(test), output_dim))\n",
    "    \n",
    "        for i, x_batch in enumerate(test_loader):\n",
    "            y_pred = sigmoid(model(*x_batch).detach().cpu().numpy()) #feed data into model\n",
    "\n",
    "            test_preds[i * batch_size:(i+1) * batch_size, :] = y_pred #get test predictions\n",
    "        \n",
    "        #test_preds has the predictions for the entire test set now\n",
    "        #all_test_preds.append(test_preds) #append predictions to the record of all past predictions\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print('[EMBEDDING TRAINING] Epoch {}/{} \\t loss={:.4f} \\t time={:.2f}s '.format(\n",
    "              epoch + 1, n_epochs_embed, avg_loss, elapsed_time))\n",
    "    '''\n",
    "    \n",
    "    #PREDICTION CODE\n",
    "    '''\n",
    "    if enable_checkpoint_ensemble:\n",
    "        #if our approach is an ensemble then we average it amongst all the historical predictions\n",
    "        test_preds = np.average(all_test_preds, weights=checkpoint_weights, axis=0)    \n",
    "    else:\n",
    "        #if our approach is not an ensemble then we just take the last set of predictions\n",
    "        test_preds = all_test_preds[-1]\n",
    "        \n",
    "    return test_preds\n",
    "    '''\n",
    "    \n",
    "    #return trained model\n",
    "    return model\n",
    "\n",
    "def predict(model, test, output_dim, batch_size=512, pred_type=\"val\"):\n",
    "    checkpoint = torch.load(filepath)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    #optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    if pred_type == \"test\":\n",
    "        test_collator = SequenceBucketCollator(lambda lengths: lengths.max(), sequence_index=0, length_index=1)\n",
    "        test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False, collate_fn=test_collator)\n",
    "\n",
    "        model.eval() #set model to eval mode for test data\n",
    "        test_preds = np.zeros((len(test), output_dim))\n",
    "\n",
    "        for i, x_batch in enumerate(test_loader):\n",
    "            #print(x_batch[0])\n",
    "            data_param = x_batch[0]\n",
    "            lengths_param = x_batch[1]\n",
    "            y_pred = sigmoid(model(data_param, lengths_param).detach().cpu().numpy()) #feed data into model\n",
    "            test_preds[i * batch_size:(i+1) * batch_size, :] = y_pred #get test predictions\n",
    "\n",
    "        return test_preds\n",
    "    else:\n",
    "        test_collator = SequenceBucketCollator(lambda lengths: lengths.max(), sequence_index=0, length_index=1, label_index=2)\n",
    "        test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False, collate_fn=test_collator)\n",
    "\n",
    "        model.eval() #set model to eval mode for test data\n",
    "        test_preds = np.zeros((len(test), output_dim))\n",
    "\n",
    "        for i, x_batch in enumerate(test_loader):\n",
    "            #print(x_batch)\n",
    "            data_param = x_batch[0][0]\n",
    "            lengths_param = x_batch[0][1]\n",
    "            y_pred = sigmoid(model(data_param, lengths_param).detach().cpu().numpy()) #feed data into model\n",
    "            test_preds[i * batch_size:(i+1) * batch_size, :] = y_pred #get test predictions\n",
    "\n",
    "        return test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialDropout(nn.Dropout2d):\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(2)    # (N, T, 1, K)\n",
    "        x = x.permute(0, 3, 2, 1)  # (N, K, 1, T)\n",
    "        #call the forward method in Dropout2d (super function specifies the subclass and instance)\n",
    "        x = super(SpatialDropout, self).forward(x)  # (N, K, 1, T), some features are masked\n",
    "        x = x.permute(0, 3, 2, 1)  # (N, T, 1, K)\n",
    "        x = x.squeeze(2)  # (N, T, K)\n",
    "        return x\n",
    "    \n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, embedding_matrix, num_aux_targets):\n",
    "        #call the init mthod in Module (super function specifies the subclass and instance)\n",
    "        super(NeuralNet, self).__init__() \n",
    "        embed_size = embedding_matrix.shape[1]\n",
    "        \n",
    "        self.embedding = nn.Embedding(max_features, embed_size)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.embedding_dropout = SpatialDropout(0.3)\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(embed_size, LSTM_UNITS, bidirectional=True, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(LSTM_UNITS * 2, LSTM_UNITS, bidirectional=True, batch_first=True)\n",
    "    \n",
    "        self.linear1 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n",
    "        self.linear2 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n",
    "        \n",
    "        self.linear_out = nn.Linear(DENSE_HIDDEN_UNITS, 1)\n",
    "        self.linear_aux_out = nn.Linear(DENSE_HIDDEN_UNITS, num_aux_targets)\n",
    "        \n",
    "    def forward(self, x, lengths=None):\n",
    "        h_embedding = self.embedding(x)\n",
    "        h_embedding = self.embedding_dropout(h_embedding)\n",
    "        \n",
    "        #first variable h_(lstm #) holds the output, _ is the (hidden state, cell state)\n",
    "        h_lstm1, _ = self.lstm1(h_embedding) \n",
    "        h_lstm2, _ = self.lstm2(h_lstm1)\n",
    "        \n",
    "        # global average pooling\n",
    "        avg_pool = torch.mean(h_lstm2, 1) #get the mean value of the first dimension in h_lstm2\n",
    "        # global max pooling\n",
    "        max_pool, _ = torch.max(h_lstm2, 1) #get the max value of the first dimension in h_lstm2\n",
    "        \n",
    "        h_conc = torch.cat((max_pool, avg_pool), 1)\n",
    "        h_conc_linear1  = F.relu(self.linear1(h_conc))\n",
    "        h_conc_linear2  = F.relu(self.linear2(h_conc))\n",
    "        \n",
    "        hidden = h_conc + h_conc_linear1 + h_conc_linear2\n",
    "        \n",
    "        result = self.linear_out(hidden)\n",
    "        aux_result = self.linear_aux_out(hidden)\n",
    "        out = torch.cat([result, aux_result], 1)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def preprocess(data):\n",
    "\n",
    "    punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~`\" + '\"\"â€œâ€â€™' + 'âˆžÎ¸Ã·Î±â€¢Ã âˆ’Î²âˆ…Â³Ï€â€˜â‚¹Â´Â°Â£â‚¬\\Ã—â„¢âˆšÂ²â€”â€“&'\n",
    "    def clean_special_chars(text, punct):\n",
    "        for p in punct:\n",
    "            text = text.replace(p, ' ')\n",
    "        return text\n",
    "\n",
    "    data = data.astype(str).apply(lambda x: clean_special_chars(x, punct))\n",
    "    return data\n",
    "'''\n",
    "symbols_to_isolate = '.,?!-;*\"â€¦:â€”()%#$&_/@ï¼¼ãƒ»Ï‰+=â€â€œ[]^â€“>\\\\Â°<~â€¢â‰ â„¢ËˆÊŠÉ’âˆžÂ§{}Â·Ï„Î±â¤â˜ºÉ¡|Â¢â†’Ì¶`â¥â”â”£â”«â”—ï¼¯â–ºâ˜…Â©â€•Éªâœ”Â®\\x96\\x92â—Â£â™¥âž¤Â´Â¹â˜•â‰ˆÃ·â™¡â—â•‘â–¬â€²É”Ëâ‚¬Û©Ûžâ€ Î¼âœ’âž¥â•â˜†ËŒâ—„Â½Ê»Ï€Î´Î·Î»ÏƒÎµÏÎ½Êƒâœ¬ï¼³ï¼µï¼°ï¼¥ï¼²ï¼©ï¼´â˜»Â±â™ÂµÂºÂ¾âœ“â—¾ØŸï¼Žâ¬…â„…Â»Ð’Ð°Ð²â£â‹…Â¿Â¬â™«ï¼£ï¼­Î²â–ˆâ–“â–’â–‘â‡’â­â€ºÂ¡â‚‚â‚ƒâ§â–°â–”â—žâ–€â–‚â–ƒâ–„â–…â–†â–‡â†™Î³Ì„â€³â˜¹âž¡Â«Ï†â…“â€žâœ‹ï¼šÂ¥Ì²Ì…Ìâˆ™â€›â—‡âœâ–·â“â—Â¶ËšË™ï¼‰ÑÐ¸Ê¿âœ¨ã€‚É‘\\x80â—•ï¼ï¼…Â¯âˆ’ï¬‚ï¬â‚Â²ÊŒÂ¼â´â„â‚„âŒ â™­âœ˜â•ªâ–¶â˜­âœ­â™ªâ˜”â˜ â™‚â˜ƒâ˜ŽâœˆâœŒâœ°â†â˜™â—‹â€£âš“å¹´âˆŽâ„’â–ªâ–™â˜â…›ï½ƒï½ï½“Ç€â„®Â¸ï½—â€šâˆ¼â€–â„³â„â†â˜¼â‹†Ê’âŠ‚ã€â…”Â¨Í¡à¹âš¾âš½Î¦Ã—Î¸ï¿¦ï¼Ÿï¼ˆâ„ƒâ©â˜®âš æœˆâœŠâŒâ­•â–¸â– â‡Œâ˜â˜‘âš¡â˜„Ç«â•­âˆ©â•®ï¼Œä¾‹ï¼žÊ•ÉÌ£Î”â‚€âœžâ”ˆâ•±â•²â–â–•â”ƒâ•°â–Šâ–‹â•¯â”³â”Šâ‰¥â˜’â†‘â˜É¹âœ…â˜›â™©â˜žï¼¡ï¼ªï¼¢â—”â—¡â†“â™€â¬†Ì±â„\\x91â €Ë¤â•šâ†ºâ‡¤âˆâœ¾â—¦â™¬Â³ã®ï½œï¼âˆµâˆ´âˆšÎ©Â¤â˜œâ–²â†³â–«â€¿â¬‡âœ§ï½ï½–ï½ï¼ï¼’ï¼ï¼˜ï¼‡â€°â‰¤âˆ•Ë†âšœâ˜'\n",
    "symbols_to_delete = '\\nðŸ•\\rðŸµðŸ˜‘\\xa0\\ue014\\t\\uf818\\uf04a\\xadðŸ˜¢ðŸ¶ï¸\\uf0e0ðŸ˜œðŸ˜ŽðŸ‘Š\\u200b\\u200eðŸ˜Ø¹Ø¯ÙˆÙŠÙ‡ØµÙ‚Ø£Ù†Ø§Ø®Ù„Ù‰Ø¨Ù…ØºØ±ðŸ˜ðŸ’–ðŸ’µÐ•ðŸ‘ŽðŸ˜€ðŸ˜‚\\u202a\\u202cðŸ”¥ðŸ˜„ðŸ»ðŸ’¥á´ÊÊ€á´‡É´á´…á´á´€á´‹Êœá´œÊŸá´›á´„á´˜Ê™Ò“á´Šá´¡É¢ðŸ˜‹ðŸ‘×©×œ×•××‘×™ðŸ˜±â€¼\\x81ã‚¨ãƒ³ã‚¸æ•…éšœ\\u2009ðŸšŒá´µÍžðŸŒŸðŸ˜ŠðŸ˜³ðŸ˜§ðŸ™€ðŸ˜ðŸ˜•\\u200fðŸ‘ðŸ˜®ðŸ˜ƒðŸ˜˜××¢×›×—ðŸ’©ðŸ’¯â›½ðŸš„ðŸ¼à®œðŸ˜–á´ ðŸš²â€ðŸ˜ŸðŸ˜ˆðŸ’ªðŸ™ðŸŽ¯ðŸŒ¹ðŸ˜‡ðŸ’”ðŸ˜¡\\x7fðŸ‘Œá¼á½¶Î®Î¹á½²Îºá¼€Î¯á¿ƒá¼´Î¾ðŸ™„ï¼¨ðŸ˜ \\ufeff\\u2028ðŸ˜‰ðŸ˜¤â›ºðŸ™‚\\u3000ØªØ­ÙƒØ³Ø©ðŸ‘®ðŸ’™ÙØ²Ø·ðŸ˜ðŸ¾ðŸŽ‰ðŸ˜ž\\u2008ðŸ¾ðŸ˜…ðŸ˜­ðŸ‘»ðŸ˜¥ðŸ˜”ðŸ˜“ðŸ½ðŸŽ†ðŸ»ðŸ½ðŸŽ¶ðŸŒºðŸ¤”ðŸ˜ª\\x08â€‘ðŸ°ðŸ‡ðŸ±ðŸ™†ðŸ˜¨ðŸ™ƒðŸ’•ð˜Šð˜¦ð˜³ð˜¢ð˜µð˜°ð˜¤ð˜ºð˜´ð˜ªð˜§ð˜®ð˜£ðŸ’—ðŸ’šåœ°ç„è°·ÑƒÐ»ÐºÐ½ÐŸÐ¾ÐÐðŸ¾ðŸ•ðŸ˜†×”ðŸ”—ðŸš½æ­Œèˆžä¼ŽðŸ™ˆðŸ˜´ðŸ¿ðŸ¤—ðŸ‡ºðŸ‡¸Ð¼Ï…Ñ‚Ñ•â¤µðŸ†ðŸŽƒðŸ˜©\\u200aðŸŒ ðŸŸðŸ’«ðŸ’°ðŸ’ŽÑÐ¿Ñ€Ð´\\x95ðŸ–ðŸ™…â›²ðŸ°ðŸ¤ðŸ‘†ðŸ™Œ\\u2002ðŸ’›ðŸ™ðŸ‘€ðŸ™ŠðŸ™‰\\u2004Ë¢áµ’Ê³Ê¸á´¼á´·á´ºÊ·áµ—Ê°áµ‰áµ˜\\x13ðŸš¬ðŸ¤“\\ue602ðŸ˜µÎ¬Î¿ÏŒÏ‚Î­á½¸×ª×ž×“×£× ×¨×š×¦×˜ðŸ˜’ÍðŸ†•ðŸ‘…ðŸ‘¥ðŸ‘„ðŸ”„ðŸ”¤ðŸ‘‰ðŸ‘¤ðŸ‘¶ðŸ‘²ðŸ”›ðŸŽ“\\uf0b7\\uf04c\\x9f\\x10æˆéƒ½ðŸ˜£âºðŸ˜ŒðŸ¤‘ðŸŒðŸ˜¯ÐµÑ…ðŸ˜²á¼¸á¾¶á½ðŸ’žðŸš“ðŸ””ðŸ“šðŸ€ðŸ‘\\u202dðŸ’¤ðŸ‡\\ue613å°åœŸè±†ðŸ¡â”â‰\\u202fðŸ‘ ã€‹à¤•à¤°à¥à¤®à¤¾ðŸ‡¹ðŸ‡¼ðŸŒ¸è”¡è‹±æ–‡ðŸŒžðŸŽ²ãƒ¬ã‚¯ã‚µã‚¹ðŸ˜›å¤–å›½äººå…³ç³»Ð¡Ð±ðŸ’‹ðŸ’€ðŸŽ„ðŸ’œðŸ¤¢ÙÙŽÑŒÑ‹Ð³Ñä¸æ˜¯\\x9c\\x9dðŸ—‘\\u2005ðŸ’ƒðŸ“£ðŸ‘¿à¼¼ã¤à¼½ðŸ˜°á¸·Ð—Ð·â–±Ñ†ï¿¼ðŸ¤£å–æ¸©å“¥åŽè®®ä¼šä¸‹é™ä½ å¤±åŽ»æ‰€æœ‰çš„é’±åŠ æ‹¿å¤§åç¨Žéª—å­ðŸãƒ„ðŸŽ…\\x85ðŸºØ¢Ø¥Ø´Ø¡ðŸŽµðŸŒŽÍŸá¼”æ²¹åˆ«å…‹ðŸ¤¡ðŸ¤¥ðŸ˜¬ðŸ¤§Ð¹\\u2003ðŸš€ðŸ¤´Ê²ÑˆÑ‡Ð˜ÐžÐ Ð¤Ð”Ð¯ÐœÑŽÐ¶ðŸ˜ðŸ–‘á½á½»Ïç‰¹æ®Šä½œæˆ¦ç¾¤Ñ‰ðŸ’¨åœ†æ˜Žå›­×§â„ðŸˆðŸ˜ºðŸŒâá»‡ðŸ”ðŸ®ðŸðŸ†ðŸ‘ðŸŒ®ðŸŒ¯ðŸ¤¦\\u200dð“’ð“²ð“¿ð“µì•ˆì˜í•˜ì„¸ìš”Ð–Ñ™ÐšÑ›ðŸ€ðŸ˜«ðŸ¤¤á¿¦æˆ‘å‡ºç”Ÿåœ¨äº†å¯ä»¥è¯´æ™®é€šè¯æ±‰è¯­å¥½æžðŸŽ¼ðŸ•ºðŸ¸ðŸ¥‚ðŸ—½ðŸŽ‡ðŸŽŠðŸ†˜ðŸ¤ ðŸ‘©ðŸ–’ðŸšªå¤©ä¸€å®¶âš²\\u2006âš­âš†â¬­â¬¯â–æ–°âœ€â•ŒðŸ‡«ðŸ‡·ðŸ‡©ðŸ‡ªðŸ‡®ðŸ‡¬ðŸ‡§ðŸ˜·ðŸ‡¨ðŸ‡¦Ð¥Ð¨ðŸŒ\\x1fæ€é¸¡ç»™çŒ´çœ‹Êð—ªð—µð—²ð—»ð˜†ð—¼ð˜‚ð—¿ð—®ð—¹ð—¶ð˜‡ð—¯ð˜ð—°ð˜€ð˜…ð—½ð˜„ð—±ðŸ“ºÏ–\\u2000Ò¯Õ½á´¦áŽ¥Ò»Íº\\u2007Õ°\\u2001É©ï½™ï½…àµ¦ï½ŒÆ½ï½ˆð“ð¡ðžð«ð®ððšðƒðœð©ð­ð¢ð¨ð§Æ„á´¨×Ÿá‘¯à»Î¤á§à¯¦Ð†á´‘Üð¬ð°ð²ð›ð¦ð¯ð‘ð™ð£ð‡ð‚ð˜ðŸŽÔœÐ¢á—žà±¦ã€”áŽ«ð³ð”ð±ðŸ”ðŸ“ð…ðŸ‹ï¬ƒðŸ’˜ðŸ’“Ñ‘ð˜¥ð˜¯ð˜¶ðŸ’ðŸŒ‹ðŸŒ„ðŸŒ…ð™¬ð™–ð™¨ð™¤ð™£ð™¡ð™®ð™˜ð™ ð™šð™™ð™œð™§ð™¥ð™©ð™ªð™—ð™žð™ð™›ðŸ‘ºðŸ·â„‹ð€ð¥ðªðŸš¶ð™¢á¼¹ðŸ¤˜Í¦ðŸ’¸Ø¬íŒ¨í‹°ï¼·ð™‡áµ»ðŸ‘‚ðŸ‘ƒÉœðŸŽ«\\uf0a7Ð‘Ð£Ñ–ðŸš¢ðŸš‚àª—à«àªœàª°àª¾àª¤à«€á¿†ðŸƒð“¬ð“»ð“´ð“®ð“½ð“¼â˜˜ï´¾Ì¯ï´¿â‚½\\ue807ð‘»ð’†ð’ð’•ð’‰ð’“ð’–ð’‚ð’ð’…ð’”ð’Žð’—ð’ŠðŸ‘½ðŸ˜™\\u200cÐ›â€’ðŸŽ¾ðŸ‘¹âŽŒðŸ’â›¸å…¬å¯“å…»å® ç‰©å—ðŸ„ðŸ€ðŸš‘ðŸ¤·æ“ç¾Žð’‘ð’šð’ð‘´ðŸ¤™ðŸ’æ¬¢è¿Žæ¥åˆ°é˜¿æ‹‰æ–¯×¡×¤ð™«ðŸˆð’Œð™Šð™­ð™†ð™‹ð™ð˜¼ð™…ï·»ðŸ¦„å·¨æ”¶èµ¢å¾—ç™½é¬¼æ„¤æ€’è¦ä¹°é¢áº½ðŸš—ðŸ³ðŸðŸðŸ–ðŸ‘ðŸ•ð’„ðŸ—ð ð™„ð™ƒðŸ‘‡é”Ÿæ–¤æ‹·ð—¢ðŸ³ðŸ±ðŸ¬â¦ãƒžãƒ«ãƒãƒ‹ãƒãƒ­æ ªå¼ç¤¾â›·í•œêµ­ì–´ã„¸ã…“ë‹ˆÍœÊ–ð˜¿ð™”â‚µð’©â„¯ð’¾ð“ð’¶ð“‰ð“‡ð“Šð“ƒð“ˆð“…â„´ð’»ð’½ð“€ð“Œð’¸ð“Žð™Î¶ð™Ÿð˜ƒð—ºðŸ®ðŸ­ðŸ¯ðŸ²ðŸ‘‹ðŸ¦Šå¤šä¼¦ðŸ½ðŸŽ»ðŸŽ¹â›“ðŸ¹ðŸ·ðŸ¦†ä¸ºå’Œä¸­å‹è°Šç¥è´ºä¸Žå…¶æƒ³è±¡å¯¹æ³•å¦‚ç›´æŽ¥é—®ç”¨è‡ªå·±çŒœæœ¬ä¼ æ•™å£«æ²¡ç§¯å”¯è®¤è¯†åŸºç£å¾’æ›¾ç»è®©ç›¸ä¿¡è€¶ç¨£å¤æ´»æ­»æ€ªä»–ä½†å½“ä»¬èŠäº›æ”¿æ²»é¢˜æ—¶å€™æˆ˜èƒœå› åœ£æŠŠå…¨å ‚ç»“å©šå­©ææƒ§ä¸”æ —è°“è¿™æ ·è¿˜â™¾ðŸŽ¸ðŸ¤•ðŸ¤’â›‘ðŸŽæ‰¹åˆ¤æ£€è®¨ðŸðŸ¦ðŸ™‹ðŸ˜¶ì¥ìŠ¤íƒ±íŠ¸ë¤¼ë„ì„ìœ ê°€ê²©ì¸ìƒì´ê²½ì œí™©ì„ë µê²Œë§Œë“¤ì§€ì•Šë¡ìž˜ê´€ë¦¬í•´ì•¼í•©ë‹¤ìºë‚˜ì—ì„œëŒ€ë§ˆì´ˆì™€í™”ì•½ê¸ˆì˜í’ˆëŸ°ì„±ë¶„ê°ˆë•ŒëŠ”ë°˜ë“œì‹œí—ˆëœì‚¬ìš©ðŸ”«ðŸ‘å‡¸á½°ðŸ’²ðŸ—¯ð™ˆá¼Œð’‡ð’ˆð’˜ð’ƒð‘¬ð‘¶ð•¾ð–™ð–—ð–†ð–Žð–Œð–ð–•ð–Šð–”ð–‘ð–‰ð–“ð–ð–œð–žð–šð–‡ð•¿ð–˜ð–„ð–›ð–’ð–‹ð–‚ð•´ð–Ÿð–ˆð•¸ðŸ‘‘ðŸš¿ðŸ’¡çŸ¥å½¼ç™¾\\uf005ð™€ð’›ð‘²ð‘³ð‘¾ð’‹ðŸ’ðŸ˜¦ð™’ð˜¾ð˜½ðŸð˜©ð˜¨á½¼á¹‘ð‘±ð‘¹ð‘«ð‘µð‘ªðŸ‡°ðŸ‡µðŸ‘¾á“‡á’§á”­áƒá§á¦á‘³á¨á“ƒá“‚á‘²á¸á‘­á‘Žá“€á£ðŸ„ðŸŽˆðŸ”¨ðŸŽðŸ¤žðŸ¸ðŸ’ŸðŸŽ°ðŸŒðŸ›³ç‚¹å‡»æŸ¥ç‰ˆðŸ­ð‘¥ð‘¦ð‘§ï¼®ï¼§ðŸ‘£\\uf020ã£ðŸ‰Ñ„ðŸ’­ðŸŽ¥ÎžðŸ´ðŸ‘¨ðŸ¤³ðŸ¦\\x0bðŸ©ð‘¯ð’’ðŸ˜—ðŸðŸ‚ðŸ‘³ðŸ—ðŸ•‰ðŸ²Ú†ÛŒð‘®ð—•ð—´ðŸ’êœ¥â²£â²ðŸ‘â°é‰„ãƒªäº‹ä»¶Ñ—ðŸ’Šã€Œã€\\uf203\\uf09a\\uf222\\ue608\\uf202\\uf099\\uf469\\ue607\\uf410\\ue600ç‡»è£½ã‚·è™šå½å±ç†å±ˆÐ“ð‘©ð‘°ð’€ð‘ºðŸŒ¤ð—³ð—œð—™ð—¦ð—§ðŸŠá½ºá¼ˆá¼¡Ï‡á¿–Î›â¤ðŸ‡³ð’™ÏˆÕÕ´Õ¥Õ¼Õ¡ÕµÕ«Õ¶Ö€Ö‚Õ¤Õ±å†¬è‡³á½€ð’ðŸ”¹ðŸ¤šðŸŽð‘·ðŸ‚ðŸ’…ð˜¬ð˜±ð˜¸ð˜·ð˜ð˜­ð˜“ð˜–ð˜¹ð˜²ð˜«Ú©Î’ÏŽðŸ’¢ÎœÎŸÎÎ‘Î•ðŸ‡±â™²ðˆâ†´ðŸ’’âŠ˜È»ðŸš´ðŸ–•ðŸ–¤ðŸ¥˜ðŸ“ðŸ‘ˆâž•ðŸš«ðŸŽ¨ðŸŒ‘ðŸ»ðŽððŠð‘­ðŸ¤–ðŸŽŽðŸ˜¼ðŸ•·ï½‡ï½’ï½Žï½”ï½‰ï½„ï½•ï½†ï½‚ï½‹ðŸ°ðŸ‡´ðŸ‡­ðŸ‡»ðŸ‡²ð—žð—­ð—˜ð—¤ðŸ‘¼ðŸ“‰ðŸŸðŸ¦ðŸŒˆðŸ”­ã€ŠðŸŠðŸ\\uf10aáƒšÚ¡ðŸ¦\\U0001f92f\\U0001f92aðŸ¡ðŸ’³á¼±ðŸ™‡ð—¸ð—Ÿð— ð—·ðŸ¥œã•ã‚ˆã†ãªã‚‰ðŸ”¼'\n",
    "\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "nltk_tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "\n",
    "isolate_dict = {ord(c):f' {c} ' for c in symbols_to_isolate}\n",
    "remove_dict = {ord(c):f'' for c in symbols_to_delete}\n",
    "\n",
    "\n",
    "def handle_punctuation(x):\n",
    "    x = x.translate(remove_dict)\n",
    "    x = x.translate(isolate_dict)\n",
    "    return x\n",
    "\n",
    "def handle_contractions(x):\n",
    "    x = nltk_tokenizer.tokenize(x)\n",
    "    return x\n",
    "\n",
    "def fix_quote(x):\n",
    "    x = [x_[1:] if x_.startswith(\"'\") else x_ for x_ in x]\n",
    "    x = ' '.join(x)\n",
    "    return x\n",
    "\n",
    "def preprocess(x):\n",
    "    x = handle_punctuation(x)\n",
    "    x = handle_contractions(x)\n",
    "    x = fix_quote(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/curtis/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:18: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('../input/train.csv')\n",
    "test = pd.read_csv('../input/test.csv')\n",
    "#pd.read_csv(\"P00000001-ALL.csv\", nrows=20)\n",
    "#train = pd.read_hdf('../input/train.h5')\n",
    "#test = pd.read_hdf('../input/test.h5')\n",
    "#tqdm.pandas()\n",
    "\n",
    "identity_columns = [\n",
    "    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n",
    "    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n",
    "\n",
    "x_train = train['comment_text'].apply(lambda x:preprocess(x))\n",
    "y_train = np.where(train['target'] >= 0.5, 1, 0)\n",
    "num_train_data = y_train.shape[0]\n",
    "y_train_identity = np.where(train[identity_columns] >= 0.5, 1, 0)\n",
    "y_aux_train = train[['severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat', 'sexual_explicit']]\n",
    "x_test = test['comment_text'].apply(lambda x:preprocess(x))\n",
    "y_aux_train = y_aux_train.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmax_features = None\\n\\n#Create the dictionary of all words that exist in our data\\nnlp = spacy.load(\"en_core_web_lg\", disable=[\\'parser\\',\\'ner\\',\\'tagger\\'])\\ntext_list = pd.concat([x_train, x_test])\\nnlp.vocab.add_flag(lambda s: s.lower() in spacy.lang.en.stop_words.STOP_WORDS, spacy.attrs.IS_STOP)\\nword_dict = {}\\nlemma_dict = {}\\nword_index = 1\\ndocs = nlp.pipe(text_list, n_threads = 2)\\nword_sequences = []\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "max_features = None\n",
    "\n",
    "#Create the dictionary of all words that exist in our data\n",
    "nlp = spacy.load(\"en_core_web_lg\", disable=['parser','ner','tagger'])\n",
    "text_list = pd.concat([x_train, x_test])\n",
    "nlp.vocab.add_flag(lambda s: s.lower() in spacy.lang.en.stop_words.STOP_WORDS, spacy.attrs.IS_STOP)\n",
    "word_dict = {}\n",
    "lemma_dict = {}\n",
    "word_index = 1\n",
    "docs = nlp.pipe(text_list, n_threads = 2)\n",
    "word_sequences = []\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nstart_time = time.time()\\nfor doc in tqdm(docs): #one doc is one comment(row)\\n    #print(count)\\n    word_seq = []\\n    for token in doc:\\n        if (token.text not in word_dict) and (token.pos_ is not \"PUNCT\"):\\n            word_dict[token.text] = word_index\\n            word_index += 1\\n            lemma_dict[token.text] = token.lemma_\\n        if token.pos_ is not \"PUNCT\":\\n            word_seq.append(word_dict[token.text])\\n    word_sequences.append(word_seq)\\n    #count+= 1\\n\\nprint(\"--- %s seconds ---\" % (time.time() - start_time))\\ndel docs\\ndel text_list\\ngc.collect()\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create dictionary of word mapping to integers as wel as lemma dictionary\n",
    "#count = 1\n",
    "'''\n",
    "start_time = time.time()\n",
    "for doc in tqdm(docs): #one doc is one comment(row)\n",
    "    #print(count)\n",
    "    word_seq = []\n",
    "    for token in doc:\n",
    "        if (token.text not in word_dict) and (token.pos_ is not \"PUNCT\"):\n",
    "            word_dict[token.text] = word_index\n",
    "            word_index += 1\n",
    "            lemma_dict[token.text] = token.lemma_\n",
    "        if token.pos_ is not \"PUNCT\":\n",
    "            word_seq.append(word_dict[token.text])\n",
    "    word_sequences.append(word_seq)\n",
    "    #count+= 1\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "del docs\n",
    "del text_list\n",
    "gc.collect()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 400000\n",
    "tokenizer = text.Tokenizer(num_words = max_features, filters='',lower=False)\n",
    "tokenizer.fit_on_texts(list(x_train) + list(x_test))\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_test = tokenizer.texts_to_sequences(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#max_features = max_features or len(word_dict) + 1\n",
    "max_features = max_features or len(tokenizer.word_index) + 1\n",
    "max_features #number of unique words there are in the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n unknown words (crawl):  222943\n",
      "--- 74.586101770401 seconds ---\n",
      "Size:  (487814, 300)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "#crawl_matrix, unknown_words_crawl = build_matrix(word_dict, lemma_dict, CRAWL_EMBEDDING_PATH)\n",
    "crawl_matrix, unknown_words_crawl = build_matrix(tokenizer.word_index, CRAWL_EMBEDDING_PATH)\n",
    "print('n unknown words (crawl): ', len(unknown_words_crawl))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print('Size: ', crawl_matrix.shape)\n",
    "del unknown_words_crawl\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n unknown words (glove):  225246\n",
      "--- 84.45934987068176 seconds ---\n",
      "Size:  (487814, 300)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "#glove_matrix, unknown_words_glove = build_matrix(word_dict, lemma_dict, GLOVE_EMBEDDING_PATH)\n",
    "glove_matrix, unknown_words_glove = build_matrix(tokenizer.word_index, GLOVE_EMBEDDING_PATH)\n",
    "print('n unknown words (glove): ', len(unknown_words_glove))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print('Size: ', glove_matrix.shape)\n",
    "del unknown_words_glove\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = np.concatenate([crawl_matrix, glove_matrix], axis=-1)\n",
    "\n",
    "del crawl_matrix\n",
    "del glove_matrix\n",
    "#del word_dict\n",
    "#del lemma_dict\n",
    "#del WORDS\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JigsawEvaluator:\n",
    "\n",
    "    def __init__(self, y_true, y_identity, power=-5, overall_model_weight=0.25):\n",
    "        self.y = y_true\n",
    "        self.y_i = y_identity\n",
    "        self.n_subgroups = self.y_i.shape[1]\n",
    "        self.power = power\n",
    "        self.overall_model_weight = overall_model_weight\n",
    "\n",
    "    @staticmethod\n",
    "    def _compute_auc(y_true, y_pred):\n",
    "        try:\n",
    "            return roc_auc_score(y_true, y_pred)\n",
    "        except ValueError:\n",
    "            return np.nan\n",
    "\n",
    "    def _compute_subgroup_auc(self, i, y_pred):\n",
    "        mask = self.y_i[:, i] == 1\n",
    "        #print(self.y)\n",
    "        return self._compute_auc(self.y[mask], y_pred[mask])\n",
    "\n",
    "    def _compute_bpsn_auc(self, i, y_pred):\n",
    "        mask = self.y_i[:, i] + self.y == 1\n",
    "        return self._compute_auc(self.y[mask], y_pred[mask])\n",
    "\n",
    "    def _compute_bnsp_auc(self, i, y_pred):\n",
    "        mask = self.y_i[:, i] + self.y != 1\n",
    "        return self._compute_auc(self.y[mask], y_pred[mask])\n",
    "\n",
    "    def compute_bias_metrics_for_model(self, y_pred):\n",
    "        #print(y_pred)\n",
    "        records = np.zeros((3, self.n_subgroups))\n",
    "        for i in range(self.n_subgroups):\n",
    "            #print(y_pred)\n",
    "            records[0, i] = self._compute_subgroup_auc(i, y_pred)\n",
    "            records[1, i] = self._compute_bpsn_auc(i, y_pred)\n",
    "            records[2, i] = self._compute_bnsp_auc(i, y_pred)\n",
    "        return records\n",
    "\n",
    "    def _calculate_overall_auc(self, y_pred):\n",
    "        return roc_auc_score(self.y, y_pred)\n",
    "\n",
    "    def _power_mean(self, array):\n",
    "        total = sum(np.power(array, self.power))\n",
    "        return np.power(total / len(array), 1 / self.power)\n",
    "\n",
    "    def get_final_metric(self, y_pred):\n",
    "        bias_metrics = self.compute_bias_metrics_for_model(y_pred)\n",
    "        bias_score = np.average([\n",
    "            self._power_mean(bias_metrics[0]),\n",
    "            self._power_mean(bias_metrics[1]),\n",
    "            self._power_mean(bias_metrics[2])\n",
    "        ])\n",
    "        overall_score = self.overall_model_weight * self._calculate_overall_auc(y_pred)\n",
    "        bias_score = (1 - self.overall_model_weight) * bias_score\n",
    "        return overall_score + bias_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1804874\n",
      "torch.Size([1804874])\n",
      "188\n",
      "tensor(713)\n"
     ]
    }
   ],
   "source": [
    "#x_train = word_sequences[:num_train_data]\n",
    "#x_test = word_sequences[num_train_data:]\n",
    "lengths = torch.from_numpy(np.array([len(x) for x in x_train]))\n",
    "test_lengths = torch.from_numpy(np.array([len(x) for x in x_test]))\n",
    "print(len(x_train))\n",
    "print(lengths.shape)\n",
    "maxlen = int(np.percentile(lengths, 95)) \n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "print(maxlen)\n",
    "print(lengths.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1804874, 188)\n",
      "(1804874,)\n",
      "1804874\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(num_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: PRELIM FILTER\n",
      "=> Saving a new best\n",
      "Epoch 1/4 \t loss=0.0809 \t time=655.38s\n",
      "=> Saving a new best\n",
      "Epoch 2/4 \t loss=0.0746 \t time=657.73s\n",
      "=> Saving a new best\n",
      "Epoch 3/4 \t loss=0.0735 \t time=658.63s\n",
      "=> Saving a new best\n",
      "Epoch 4/4 \t loss=0.0728 \t time=659.25s\n",
      "Kaggle Score:  0.9398711312814203\n",
      "ROC score:  0.9735292218007326\n",
      "====================== END OF FILTER =============================\n"
     ]
    }
   ],
   "source": [
    "#Here, we use one model that filters out all the easy predictions\n",
    "x_train_torch = torch.tensor(x_train, dtype=torch.long).cuda()\n",
    "y_train_torch = torch.tensor(np.hstack([y_train[:, np.newaxis], y_aux_train]), dtype=torch.float32).cuda()\n",
    "train_dataset = data.TensorDataset(x_train_torch, lengths ,y_train_torch)\n",
    "\n",
    "print('Model: PRELIM FILTER')\n",
    "seed_everything(1234)\n",
    "\n",
    "model = NeuralNet(embedding_matrix, y_aux_train.shape[-1])\n",
    "model.cuda()\n",
    "\n",
    "#training using training and validation set\n",
    "model = train_model(model, train_dataset, train_dataset, output_dim=y_train_torch.shape[-1], \n",
    "                         loss_fn=nn.BCEWithLogitsLoss(reduction='mean'))\n",
    "\n",
    "\n",
    "#prediction on entire train set\n",
    "pred = predict(model, train_dataset, output_dim=y_train_torch.shape[-1])[:,0]\n",
    "\n",
    "evaluator = JigsawEvaluator(y_train, y_train_identity)\n",
    "auc_score = evaluator.get_final_metric(pred)\n",
    "roc_score = roc_auc_score(y_train, pred)\n",
    "\n",
    "print('Kaggle Score: ', auc_score)\n",
    "print('ROC score: ', roc_score)\n",
    "\n",
    "        \n",
    "print('====================== END OF FILTER =============================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CLEAR \n",
    "del x_train_torch\n",
    "del y_train_torch\n",
    "del train_dataset\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#AND GET DISTRIBUTIONS OF PREDICTIONS\n",
    "plt.hist(pred)\n",
    "plt.title(\"Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1% of comments have a score of less than 4.716167048172792e-05\n",
      "2% of comments have a score of less than 5.7090710470220074e-05\n",
      "3% of comments have a score of less than 6.539529422298074e-05\n",
      "4% of comments have a score of less than 7.316590199479833e-05\n",
      "5% of comments have a score of less than 8.071067350101657e-05\n",
      "6% of comments have a score of less than 8.820692572044208e-05\n",
      "7% of comments have a score of less than 9.578832396073267e-05\n",
      "8% of comments have a score of less than 0.00010333054960938172\n",
      "9% of comments have a score of less than 0.00011116965142718982\n",
      "10% of comments have a score of less than 0.00011926486113225112\n",
      "11% of comments have a score of less than 0.00012766249346896075\n",
      "12% of comments have a score of less than 0.00013632517249789088\n",
      "13% of comments have a score of less than 0.00014525270671583712\n",
      "14% of comments have a score of less than 0.00015454382781172172\n",
      "15% of comments have a score of less than 0.00016420407991972754\n",
      "16% of comments have a score of less than 0.00017427570826839656\n",
      "17% of comments have a score of less than 0.0001849189418135211\n",
      "18% of comments have a score of less than 0.00019609125592978674\n",
      "19% of comments have a score of less than 0.00020773560099769385\n",
      "20% of comments have a score of less than 0.0002199432346969843\n",
      "21% of comments have a score of less than 0.00023279977904167026\n",
      "22% of comments have a score of less than 0.0002465142752043903\n",
      "23% of comments have a score of less than 0.0002609371722792276\n",
      "24% of comments have a score of less than 0.0002764105261303484\n",
      "25% of comments have a score of less than 0.0002925345252151601\n",
      "26% of comments have a score of less than 0.00030935908842366215\n",
      "27% of comments have a score of less than 0.0003276766955968924\n",
      "28% of comments have a score of less than 0.0003468418051488698\n",
      "29% of comments have a score of less than 0.00036707503895740953\n",
      "30% of comments have a score of less than 0.0003890378575306386\n",
      "31% of comments have a score of less than 0.00041274267859989775\n",
      "32% of comments have a score of less than 0.0004375141556374728\n",
      "33% of comments have a score of less than 0.0004643700024462306\n",
      "34% of comments have a score of less than 0.0004930949653498828\n",
      "35% of comments have a score of less than 0.0005238182609900832\n",
      "36% of comments have a score of less than 0.0005576464463956654\n",
      "37% of comments have a score of less than 0.000593552555073984\n",
      "38% of comments have a score of less than 0.0006328939157538116\n",
      "39% of comments have a score of less than 0.0006750111375004053\n",
      "40% of comments have a score of less than 0.0007207707269117238\n",
      "41% of comments have a score of less than 0.000771115068346262\n",
      "42% of comments have a score of less than 0.000825897444738075\n",
      "43% of comments have a score of less than 0.0008844074892112986\n",
      "44% of comments have a score of less than 0.0009486901806667447\n",
      "45% of comments have a score of less than 0.0010194474889431149\n",
      "46% of comments have a score of less than 0.0010954139288514853\n",
      "47% of comments have a score of less than 0.0011781755706761028\n",
      "48% of comments have a score of less than 0.0012696176441386333\n",
      "49% of comments have a score of less than 0.0013725134427659215\n",
      "50% of comments have a score of less than 0.0014860194642096758\n",
      "51% of comments have a score of less than 0.0016101924167014653\n",
      "52% of comments have a score of less than 0.0017450046725571164\n",
      "53% of comments have a score of less than 0.0018972514662891626\n",
      "54% of comments have a score of less than 0.0020661201467737558\n",
      "55% of comments have a score of less than 0.0022539524361491203\n",
      "56% of comments have a score of less than 0.0024650495499372485\n",
      "57% of comments have a score of less than 0.002699255226179955\n",
      "58% of comments have a score of less than 0.0029667096864432092\n",
      "59% of comments have a score of less than 0.0032673595473170273\n",
      "60% of comments have a score of less than 0.0036066169384866955\n",
      "61% of comments have a score of less than 0.003982108947820962\n",
      "62% of comments have a score of less than 0.004420527275651693\n",
      "63% of comments have a score of less than 0.004913476877845823\n",
      "64% of comments have a score of less than 0.005485780164599414\n",
      "65% of comments have a score of less than 0.006136916717514395\n",
      "66% of comments have a score of less than 0.006883946079760801\n",
      "67% of comments have a score of less than 0.007761038634926082\n",
      "68% of comments have a score of less than 0.008762084655463697\n",
      "69% of comments have a score of less than 0.009933228837326166\n",
      "70% of comments have a score of less than 0.011268755700439186\n",
      "71% of comments have a score of less than 0.012858468107879162\n",
      "72% of comments have a score of less than 0.01470700699836017\n",
      "73% of comments have a score of less than 0.016905223224312066\n",
      "74% of comments have a score of less than 0.01947661701589823\n",
      "75% of comments have a score of less than 0.022557934280484915\n",
      "76% of comments have a score of less than 0.026146729215979575\n",
      "77% of comments have a score of less than 0.030391234289854747\n",
      "78% of comments have a score of less than 0.035409477353096\n",
      "79% of comments have a score of less than 0.04139949638396502\n",
      "80% of comments have a score of less than 0.048512473702430885\n",
      "81% of comments have a score of less than 0.05687040556222201\n",
      "82% of comments have a score of less than 0.06684084326028819\n",
      "83% of comments have a score of less than 0.07881968349218349\n",
      "84% of comments have a score of less than 0.09285068035125707\n",
      "85% of comments have a score of less than 0.10983802229166033\n",
      "86% of comments have a score of less than 0.1298387387394905\n",
      "87% of comments have a score of less than 0.15357289686799053\n",
      "88% of comments have a score of less than 0.18182746827602383\n",
      "89% of comments have a score of less than 0.21598361760377877\n",
      "90% of comments have a score of less than 0.2568695276975631\n",
      "91% of comments have a score of less than 0.30501440703868926\n",
      "92% of comments have a score of less than 0.3636422562599195\n",
      "93% of comments have a score of less than 0.43410396933555606\n",
      "94% of comments have a score of less than 0.5166156578063961\n",
      "95% of comments have a score of less than 0.614405676722524\n",
      "96% of comments have a score of less than 0.7230000114440913\n",
      "97% of comments have a score of less than 0.834487146139145\n",
      "98% of comments have a score of less than 0.9262879693508149\n",
      "99% of comments have a score of less than 0.9800746512413026\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 100):\n",
    "    thres = np.percentile(pred, i)\n",
    "    print('{}% of comments have a score of less than {}'.format(i, thres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03767282366752625\n",
      "[ True  True  True ...  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "#FILTER OUT ALL THE EASY COMMENTS (64th percentile)\n",
    "threshold = np.percentile(pred, 64)\n",
    "print(threshold)\n",
    "keep_index = (pred > threshold)\n",
    "print(keep_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_orig = x_train\n",
    "y_train_orig = y_train\n",
    "y_train_identity_orig = y_train_identity\n",
    "\n",
    "x_train = x_train[np.where(keep_index)]\n",
    "y_train = y_train[np.where(keep_index)]\n",
    "y_train_identity = y_train_identity[np.where(keep_index)]\n",
    "y_aux_train = y_aux_train[np.where(keep_index)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pred[np.where(keep_index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle Score:  0.9041709031652599\n",
      "ROC score:  0.9217058856253003\n"
     ]
    }
   ],
   "source": [
    "evaluator = JigsawEvaluator(y_train, y_train_identity)\n",
    "auc_score = evaluator.get_final_metric(pred)\n",
    "roc_score = roc_auc_score(y_train, pred)\n",
    "\n",
    "print('Kaggle Score: ', auc_score)\n",
    "print('ROC score: ', roc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2798"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del pred\n",
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered indices:  [     0      1      2 ... 649752 649753 649754]\n",
      "Filtered indices len:  [     0      1      2 ... 649752 649753 649754]\n",
      "Val indices:  [     0      9     23 ... 649734 649741 649752]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 389854 is out of bounds for axis 1 with size 389853",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-9abc468e946f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mtr_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Val indices: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mval_ind_orig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiltered_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_ind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Original Val Indicies: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ind_orig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mall_val_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 389854 is out of bounds for axis 1 with size 389853"
     ]
    }
   ],
   "source": [
    "filtered_indices = np.where(keep_index)[0]\n",
    "print(\"Filtered indices: \", filtered_indices)\n",
    "print(\"Filtered indices len: \", filtered_indices)\n",
    "all_val_preds = []\n",
    "all_test_preds = []\n",
    "num_splits = 5\n",
    "\n",
    "#Add in K fold \n",
    "random_state = 2019\n",
    "\n",
    "#K fold splits\n",
    "splits = list(StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=random_state).split(x_train,y_train))\n",
    "\n",
    "#final validation predictions\n",
    "final_val_preds = np.zeros((x_train_orig.shape[0])) #change to x train original shape\n",
    "\n",
    "#final test predictions to be stored in this var\n",
    "final_test_preds = np.zeros((x_test.shape[0]))\n",
    "\n",
    "start_time = time.time()\n",
    "for fold in range(num_splits):\n",
    "    tr_ind, val_ind = splits[fold]\n",
    "    print('Val indices: ', val_ind)\n",
    "    val_ind_orig = filtered_indices[val_ind]\n",
    "    print('Original Val Indicies: ', val_ind_orig)\n",
    "    all_val_preds = []\n",
    "    all_test_preds = []\n",
    "    #print('Training set size: ', len(tr_ind))\n",
    "    #print('Val set size: ', len(val_ind))\n",
    "    x_training = x_train[tr_ind]\n",
    "    y_training = y_train[tr_ind]\n",
    "    y_aux_training = y_aux_train[tr_ind]\n",
    "    \n",
    "    x_val = x_train[val_ind]\n",
    "    y_val = y_train[val_ind]\n",
    "    y_aux_val = y_aux_train[val_ind]\n",
    "    \n",
    "    \n",
    "    \n",
    "    x_train_torch = torch.tensor(x_training, dtype=torch.long).cuda()\n",
    "    x_val_torch = torch.tensor(x_val, dtype=torch.long).cuda()\n",
    "    y_train_torch = torch.tensor(np.hstack([y_training[:, np.newaxis], y_aux_training]), dtype=torch.float32).cuda()\n",
    "    y_val_torch = torch.tensor(np.hstack([y_val[:, np.newaxis], y_aux_val]), dtype=torch.float32).cuda()\n",
    "    \n",
    "    x_test_torch = torch.tensor(x_test, dtype=torch.long).cuda()\n",
    "    \n",
    "    ###\n",
    "    \n",
    "    #test_dataset = data.TensorDataset(x_test_torch, test_lengths)\n",
    "    #train_dataset = data.TensorDataset(x_train_torch, lengths, y_train_torch)\n",
    "    #val_dataset = data.TensorDataset(x_val_torch)\n",
    "\n",
    "    #train_collator = SequenceBucketCollator(lambda lengths: lengths.max(), sequence_index=0, length_index=1, label_index=2)\n",
    "    #test_collator = SequenceBucketCollator(lambda lengths: lengths.max(), sequence_index=0, length_index=1)\n",
    "    \n",
    "    ####\n",
    "    train_dataset = data.TensorDataset(x_train_torch, lengths[tr_ind], y_train_torch)\n",
    "    val_dataset = data.TensorDataset(x_val_torch, lengths[val_ind], y_val_torch)\n",
    "    test_dataset = data.TensorDataset(x_test_torch, test_lengths)\n",
    "    \n",
    "    #temp_dataset = data.Subset(train_dataset, indices=[0, 1])\n",
    "\n",
    "    for model_idx in range(NUM_MODELS):\n",
    "        print('Model ', model_idx)\n",
    "        seed_everything(1234 + model_idx)\n",
    "\n",
    "        model = NeuralNet(embedding_matrix, y_aux_train.shape[-1])\n",
    "        model.cuda()\n",
    "\n",
    "        #training using training and validation set\n",
    "        model = train_model(model, train_dataset, val_dataset, output_dim=y_train_torch.shape[-1], loss_fn=nn.BCEWithLogitsLoss(reduction='mean'))\n",
    "        \n",
    "        #prediction on validation set (used for score measurement)\n",
    "        val_pred = predict(model, val_dataset, output_dim=y_train_torch.shape[-1], pred_type=\"val\") #val preds on the val split\n",
    "        all_val_preds.append(val_pred)\n",
    "        #print(len(val_pred))\n",
    "        \n",
    "        #prediction on entire test set (actual predictions to be submitted)\n",
    "        test_pred = predict(model, test_dataset, output_dim=y_train_torch.shape[-1], pred_type=\"test\")\n",
    "        all_test_preds.append(test_pred)\n",
    "        \n",
    "        print()\n",
    "        \n",
    "    #average validation prediction amongst all models\n",
    "    avg_val = np.mean(all_val_preds, axis=0)[:, 0] #will be printed out per split\n",
    "    final_val_preds[val_ind_orig] += avg_val\n",
    "    \n",
    "    avg_test = np.mean(all_test_preds, axis=0)[:, 0]\n",
    "    \n",
    "    final_test_preds += avg_test #get all test scores for every split (will be averaged out at the end)\n",
    "\n",
    "    y_true = y_train[val_ind] #true scores for this validation set\n",
    "    y_identity = y_train_identity[val_ind] #true scores for the identity groups for this validation set\n",
    "    evaluator = JigsawEvaluator(y_true, y_identity)\n",
    "    auc_score = evaluator.get_final_metric(avg_val)\n",
    "\n",
    "    roc_score = roc_auc_score(y_train[val_ind], avg_val)\n",
    "    print('Kaggle Score: ', auc_score)\n",
    "    print('ROC score: ', roc_score)\n",
    "    \n",
    "    del x_train_torch\n",
    "    del x_val_torch\n",
    "    del y_train_torch\n",
    "    del y_val_torch\n",
    "    del x_test_torch\n",
    "    del train_dataset\n",
    "    del val_dataset\n",
    "    del test_dataset\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print('=============End-of-Fold================')\n",
    "    \n",
    "end_time = time.time()\n",
    "print('Time: ', end_time - start_time)\n",
    "\n",
    "#Final combined score\n",
    "y_true = y_train_orig #y train original\n",
    "y_identity = y_train_identity_orig #y train identity original\n",
    "evaluator = JigsawEvaluator(y_true, y_identity)\n",
    "auc_score = evaluator.get_final_metric(final_val_preds)\n",
    "print('Final Kaggle Score: ', auc_score)\n",
    "print('Final ROC score: ', roc_auc_score(y_train_orig, final_val_preds))\n",
    "\n",
    "#average test predictions AGAIN this time by number of splits\n",
    "final_test_preds /= num_splits\n",
    "#print(final_test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef threshold_search(y_true, y_identity, y_proba):\\n    best_threshold = 0\\n    best_score = 0\\n    for threshold in [i * 0.01 for i in range(100)]:\\n        evaluator = JigsawEvaluator(y_true, y_identity)\\n        score = evaluator.get_final_metric(y_proba > threshold)\\n        print(\"Threshold: {} , Score: {} \".format(threshold, score))\\n        if score > best_score:\\n            best_threshold = threshold\\n            best_score = score\\n    search_result = {\\'threshold\\': best_threshold, \\'kaggle_score\\': best_score}\\n    return search_result\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def threshold_search(y_true, y_identity, y_proba):\n",
    "    best_threshold = 0\n",
    "    best_score = 0\n",
    "    for threshold in [i * 0.01 for i in range(100)]:\n",
    "        evaluator = JigsawEvaluator(y_true, y_identity)\n",
    "        score = evaluator.get_final_metric(y_proba > threshold)\n",
    "        print(\"Threshold: {} , Score: {} \".format(threshold, score))\n",
    "        if score > best_score:\n",
    "            best_threshold = threshold\n",
    "            best_score = score\n",
    "    search_result = {'threshold': best_threshold, 'kaggle_score': best_score}\n",
    "    return search_result\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'threshold = threshold_search(y_true, y_identity, final_val_preds)\\nprint(threshold)\\nthres_diff = 0.5 - threshold[\"threshold\"]\\nprint(\"Threshold difference: \", thres_diff)'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''threshold = threshold_search(y_true, y_identity, final_val_preds)\n",
    "print(threshold)\n",
    "thres_diff = 0.5 - threshold[\"threshold\"]\n",
    "print(\"Threshold difference: \", thres_diff)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7000000</td>\n",
       "      <td>0.020993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7000001</td>\n",
       "      <td>0.004992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7000002</td>\n",
       "      <td>0.018071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7000003</td>\n",
       "      <td>0.009714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7000004</td>\n",
       "      <td>0.976520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  prediction\n",
       "0  7000000    0.020993\n",
       "1  7000001    0.004992\n",
       "2  7000002    0.018071\n",
       "3  7000003    0.009714\n",
       "4  7000004    0.976520"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.DataFrame.from_dict({\n",
    "    'id': test['id'],\n",
    "    'prediction': final_test_preds\n",
    "})\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
