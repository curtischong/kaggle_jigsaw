{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import regex\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import tqdm\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from keras.preprocessing import text, sequence\n",
    "from sklearn.model_selection import KFold\n",
    "import multiprocessing as mp\n",
    "from torch import nn\n",
    "import random\n",
    "n_cores = mp.cpu_count()\n",
    "\n",
    "import gensim, logging # for word2vec\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "POS_VEC_SIZE = 50\n",
    "NUM_SPLITS = 5\n",
    "SMALL_DATA = False\n",
    "GEN_POS_TAGS = True\n",
    "NUM_MODELS = 5\n",
    "LSTM_UNITS = 128\n",
    "DENSE_HIDDEN_UNITS = 4 * LSTM_UNITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def train_model(model, train, test, loss_fn, output_dim, lr=0.001,\n",
    "                batch_size=512, n_epochs=4, n_epochs_embed=2,\n",
    "                enable_checkpoint_ensemble=True):\n",
    "    \n",
    "    \n",
    "    param_lrs = [{'params': param, 'lr': lr} for param in model.parameters()]\n",
    "    optimizer = torch.optim.Adam(param_lrs, lr=lr)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda epoch: 0.6 ** epoch)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n",
    "    all_test_preds = []\n",
    "    checkpoint_weights = [2 ** epoch for epoch in range(n_epochs)]\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        model.train() #set model to train mode\n",
    "        avg_loss = 0.\n",
    "        \n",
    "        for data in tqdm(train_loader, disable=False):\n",
    "            \n",
    "            #training loop\n",
    "            x_batch = data[:-1]\n",
    "            y_batch = data[-1]\n",
    "\n",
    "            y_pred = model(*x_batch)  #feed data into model          \n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            \n",
    "            #calculate error and adjust model params\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item() / len(train_loader) #gets the loss per epoch\n",
    "        \n",
    "        \n",
    "        model.eval() #set model to eval mode for test data\n",
    "        test_preds = np.zeros((len(test), output_dim))\n",
    "    \n",
    "        for i, x_batch in enumerate(test_loader):\n",
    "            y_pred = sigmoid(model(*x_batch).detach().cpu().numpy()) #feed data into model\n",
    "\n",
    "            test_preds[i * batch_size:(i+1) * batch_size, :] = y_pred #get test predictions\n",
    "        \n",
    "        #test_preds has the predictions for the entire test set now\n",
    "        all_test_preds.append(test_preds) #append predictions to the record of all past predictions\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print('Epoch {}/{} \\t loss={:.4f} \\t time={:.2f}s'.format(\n",
    "              epoch + 1, n_epochs, avg_loss, elapsed_time))\n",
    "    return model\n",
    "\n",
    "def predict(model, test, output_dim, batch_size=512):\n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    model.eval() #set model to eval mode for test data\n",
    "    test_preds = np.zeros((len(test), output_dim))\n",
    "    \n",
    "    for i, x_batch in enumerate(test_loader):\n",
    "        y_pred = sigmoid(model(*x_batch).detach().cpu().numpy()) #feed data into model\n",
    "        test_preds[i * batch_size:(i+1) * batch_size, :] = y_pred #get test predictions\n",
    "        \n",
    "    return test_preds\n",
    "\n",
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialDropout(nn.Dropout2d):\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(2)    # (N, T, 1, K)\n",
    "        x = x.permute(0, 3, 2, 1)  # (N, K, 1, T)\n",
    "        #call the forward method in Dropout2d (super function specifies the subclass and instance)\n",
    "        x = super(SpatialDropout, self).forward(x)  # (N, K, 1, T), some features are masked\n",
    "        x = x.permute(0, 3, 2, 1)  # (N, T, 1, K)\n",
    "        x = x.squeeze(2)  # (N, T, K)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, embedding_matrix, num_aux_targets):\n",
    "        #call the init mthod in Module (super function specifies the subclass and instance)\n",
    "        super(NeuralNet, self).__init__() \n",
    "        embed_size = embedding_matrix.shape[1]\n",
    "        max_features = 400000\n",
    "        self.embedding = nn.Embedding(max_features, embed_size)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.embedding_dropout = SpatialDropout(0.3)\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(embed_size, LSTM_UNITS, bidirectional=True, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(LSTM_UNITS * 2, LSTM_UNITS, bidirectional=True, batch_first=True)\n",
    "    \n",
    "        self.linear1 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n",
    "        self.linear2 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n",
    "        \n",
    "        self.linear_out = nn.Linear(DENSE_HIDDEN_UNITS, 1)\n",
    "        self.linear_aux_out = nn.Linear(DENSE_HIDDEN_UNITS, num_aux_targets)\n",
    "        \n",
    "    def forward(self, x, lengths=None):\n",
    "        h_embedding = self.embedding(x)\n",
    "        h_embedding = self.embedding_dropout(h_embedding)\n",
    "        \n",
    "        #first variable h_(lstm #) holds the output, _ is the (hidden state, cell state)\n",
    "        h_lstm1, _ = self.lstm1(h_embedding) \n",
    "        h_lstm2, _ = self.lstm2(h_lstm1)\n",
    "        \n",
    "        # global average pooling\n",
    "        avg_pool = torch.mean(h_lstm2, 1) #get the mean value of the first dimension in h_lstm2\n",
    "        # global max pooling\n",
    "        max_pool, _ = torch.max(h_lstm2, 1) #get the max value of the first dimension in h_lstm2\n",
    "        \n",
    "        h_conc = torch.cat((max_pool, avg_pool), 1)\n",
    "        h_conc_linear1  = F.relu(self.linear1(h_conc))\n",
    "        h_conc_linear2  = F.relu(self.linear2(h_conc))\n",
    "        \n",
    "        hidden = h_conc + h_conc_linear1 + h_conc_linear2\n",
    "        \n",
    "        result = self.linear_out(hidden)\n",
    "        aux_result = self.linear_aux_out(hidden)\n",
    "        out = torch.cat([result, aux_result], 1)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the embedding matrix maps the word idx (instead of the word) to the embedding\n",
    "def build_matrix(word_index, pos_model):\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, POS_VEC_SIZE))\n",
    "    unknown_words = []\n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        try:\n",
    "            embedding_matrix[i] = pos_model[word]\n",
    "        except KeyError:\n",
    "            unknown_words.append(word)\n",
    "    return embedding_matrix, unknown_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading Files\n",
      "--- Finished Loading 2.4167864322662354\n",
      "--- creating train pos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/curtis/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/curtis/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "1804874it [09:14, 3253.51it/s]                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Finished Loading 555.1695883274078\n",
      "--- creating test pos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "97320it [00:32, 3035.38it/s]                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Finished Loading 32.65728688240051\n"
     ]
    }
   ],
   "source": [
    "train = pd.DataFrame()\n",
    "test = pd.DataFrame()\n",
    "if GEN_POS_TAGS:\n",
    "    print(\"--- Loading Files\")\n",
    "    start_time = time.time()\n",
    "    train = pd.read_hdf('../input/train.h5')\n",
    "    test = pd.read_hdf('../input/test.h5')\n",
    "    print(\"--- Finished Loading %s\" % (time.time() - start_time))\n",
    "    \n",
    "    \n",
    "    # helper function for reading text files\n",
    "    train_text = train[\"comment_text\"]\n",
    "    test_text = test[\"comment_text\"]\n",
    "\n",
    "\n",
    "    # fill in missing\n",
    "    train_text[train_text.map(len)<=1] = 'neutral'\n",
    "    test_text[test_text.map(len)<=1] = 'neutral'\n",
    "\n",
    "    # map to pos\n",
    "    def get_pos(x):\n",
    "      tokens = nltk.tokenize.word_tokenize(x)\n",
    "      tags = nltk.pos_tag(tokens)\n",
    "      _, pos = zip(*tags) \n",
    "      return ' '.join(pos)\n",
    "\n",
    "    print(\"--- creating train pos\")\n",
    "    start_time = time.time()\n",
    "    if __name__ == '__main__':\n",
    "       with mp.Pool(n_cores) as p:\n",
    "          train_pos = list(tqdm.tqdm(p.imap(get_pos, train_text), total=30))\n",
    "    print(\"--- Finished Loading %s\" % (time.time() - start_time))\n",
    "\n",
    "    print(\"--- creating test pos\")\n",
    "    start_time = time.time()\n",
    "    if __name__ == '__main__':\n",
    "       with mp.Pool(n_cores) as p:\n",
    "          test_pos = list(tqdm.tqdm(p.imap(get_pos, test_text), total=30))\n",
    "    print(\"--- Finished Loading %s\" % (time.time() - start_time))\n",
    "    \n",
    "    # save the tags\n",
    "    train = pd.DataFrame({'pos':train_pos,\n",
    "                          'target': train['target'],\n",
    "                          'severe_toxicity': train['severe_toxicity'],\n",
    "                          'obscene': train['obscene'],\n",
    "                          'identity_attack': train['identity_attack'],\n",
    "                          'insult': train['insult'],\n",
    "                          'threat': train['threat'],\n",
    "                          'sexual_explicit': train['sexual_explicit'] # might not be so good\n",
    "                         })\n",
    "    test = pd.DataFrame({'pos':test_pos})\n",
    "    train.to_hdf('../input/train_pos.h5',key='train_pos')\n",
    "    test.to_hdf('../input/test_pos.h5',key='test_pos')\n",
    "else:\n",
    "    train = pd.read_hdf('../input/train_pos.h5')\n",
    "    test = pd.read_hdf('../input/test_pos.h5')\n",
    "if SMALL_DATA:\n",
    "    print(\"Using small data\")\n",
    "    train = train[:100]\n",
    "    test = test[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_hdf('../input/train_pos.h5')\n",
    "test = pd.read_hdf('../input/test_pos.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos = train[\"pos\"].apply(lambda x: x.split()).values\n",
    "test_pos = test[\"pos\"].apply(lambda x: x.split()).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-04 20:45:14,871 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
      "2019-06-04 20:45:14,871 : INFO : collecting all words and their counts\n",
      "2019-06-04 20:45:14,871 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-06-04 20:45:14,923 : INFO : PROGRESS: at sentence #10000, processed 705467 words, keeping 45 word types\n",
      "2019-06-04 20:45:14,969 : INFO : PROGRESS: at sentence #20000, processed 1323528 words, keeping 45 word types\n",
      "2019-06-04 20:45:15,014 : INFO : PROGRESS: at sentence #30000, processed 1925423 words, keeping 45 word types\n",
      "2019-06-04 20:45:15,058 : INFO : PROGRESS: at sentence #40000, processed 2525330 words, keeping 45 word types\n",
      "2019-06-04 20:45:15,104 : INFO : PROGRESS: at sentence #50000, processed 3142997 words, keeping 45 word types\n",
      "2019-06-04 20:45:15,147 : INFO : PROGRESS: at sentence #60000, processed 3720428 words, keeping 45 word types\n",
      "2019-06-04 20:45:15,190 : INFO : PROGRESS: at sentence #70000, processed 4316144 words, keeping 45 word types\n",
      "2019-06-04 20:45:15,236 : INFO : PROGRESS: at sentence #80000, processed 4941847 words, keeping 45 word types\n",
      "2019-06-04 20:45:15,282 : INFO : PROGRESS: at sentence #90000, processed 5561653 words, keeping 45 word types\n",
      "2019-06-04 20:45:15,329 : INFO : PROGRESS: at sentence #100000, processed 6190116 words, keeping 45 word types\n",
      "2019-06-04 20:45:15,375 : INFO : PROGRESS: at sentence #110000, processed 6820176 words, keeping 45 word types\n",
      "2019-06-04 20:45:15,421 : INFO : PROGRESS: at sentence #120000, processed 7441670 words, keeping 45 word types\n",
      "2019-06-04 20:45:15,465 : INFO : PROGRESS: at sentence #130000, processed 8034854 words, keeping 45 word types\n",
      "2019-06-04 20:45:15,510 : INFO : PROGRESS: at sentence #140000, processed 8632200 words, keeping 45 word types\n",
      "2019-06-04 20:45:15,557 : INFO : PROGRESS: at sentence #150000, processed 9253828 words, keeping 45 word types\n",
      "2019-06-04 20:45:15,608 : INFO : PROGRESS: at sentence #160000, processed 9880319 words, keeping 45 word types\n",
      "2019-06-04 20:45:15,662 : INFO : PROGRESS: at sentence #170000, processed 10506003 words, keeping 45 word types\n",
      "2019-06-04 20:45:15,711 : INFO : PROGRESS: at sentence #180000, processed 11142813 words, keeping 45 word types\n",
      "2019-06-04 20:45:15,756 : INFO : PROGRESS: at sentence #190000, processed 11751103 words, keeping 45 word types\n",
      "2019-06-04 20:45:15,803 : INFO : PROGRESS: at sentence #200000, processed 12385224 words, keeping 45 word types\n",
      "2019-06-04 20:45:15,851 : INFO : PROGRESS: at sentence #210000, processed 13046089 words, keeping 45 word types\n",
      "2019-06-04 20:45:15,898 : INFO : PROGRESS: at sentence #220000, processed 13683986 words, keeping 45 word types\n",
      "2019-06-04 20:45:15,944 : INFO : PROGRESS: at sentence #230000, processed 14308480 words, keeping 45 word types\n",
      "2019-06-04 20:45:15,990 : INFO : PROGRESS: at sentence #240000, processed 14938459 words, keeping 45 word types\n",
      "2019-06-04 20:45:16,040 : INFO : PROGRESS: at sentence #250000, processed 15570763 words, keeping 45 word types\n",
      "2019-06-04 20:45:16,090 : INFO : PROGRESS: at sentence #260000, processed 16211261 words, keeping 45 word types\n",
      "2019-06-04 20:45:16,146 : INFO : PROGRESS: at sentence #270000, processed 16850516 words, keeping 45 word types\n",
      "2019-06-04 20:45:16,197 : INFO : PROGRESS: at sentence #280000, processed 17478461 words, keeping 45 word types\n",
      "2019-06-04 20:45:16,240 : INFO : PROGRESS: at sentence #290000, processed 18059731 words, keeping 45 word types\n",
      "2019-06-04 20:45:16,287 : INFO : PROGRESS: at sentence #300000, processed 18704651 words, keeping 45 word types\n",
      "2019-06-04 20:45:16,334 : INFO : PROGRESS: at sentence #310000, processed 19338736 words, keeping 45 word types\n",
      "2019-06-04 20:45:16,379 : INFO : PROGRESS: at sentence #320000, processed 19950994 words, keeping 45 word types\n",
      "2019-06-04 20:45:16,423 : INFO : PROGRESS: at sentence #330000, processed 20550450 words, keeping 45 word types\n",
      "2019-06-04 20:45:16,469 : INFO : PROGRESS: at sentence #340000, processed 21169489 words, keeping 45 word types\n",
      "2019-06-04 20:45:16,514 : INFO : PROGRESS: at sentence #350000, processed 21770212 words, keeping 45 word types\n",
      "2019-06-04 20:45:16,559 : INFO : PROGRESS: at sentence #360000, processed 22351234 words, keeping 45 word types\n",
      "2019-06-04 20:45:16,608 : INFO : PROGRESS: at sentence #370000, processed 22944635 words, keeping 45 word types\n",
      "2019-06-04 20:45:16,659 : INFO : PROGRESS: at sentence #380000, processed 23538377 words, keeping 45 word types\n",
      "2019-06-04 20:45:16,706 : INFO : PROGRESS: at sentence #390000, processed 24134748 words, keeping 45 word types\n",
      "2019-06-04 20:45:16,750 : INFO : PROGRESS: at sentence #400000, processed 24727021 words, keeping 45 word types\n",
      "2019-06-04 20:45:16,793 : INFO : PROGRESS: at sentence #410000, processed 25316852 words, keeping 45 word types\n",
      "2019-06-04 20:45:16,835 : INFO : PROGRESS: at sentence #420000, processed 25893377 words, keeping 45 word types\n",
      "2019-06-04 20:45:16,879 : INFO : PROGRESS: at sentence #430000, processed 26506371 words, keeping 45 word types\n",
      "2019-06-04 20:45:16,923 : INFO : PROGRESS: at sentence #440000, processed 27100131 words, keeping 45 word types\n",
      "2019-06-04 20:45:16,966 : INFO : PROGRESS: at sentence #450000, processed 27682622 words, keeping 45 word types\n",
      "2019-06-04 20:45:17,009 : INFO : PROGRESS: at sentence #460000, processed 28264475 words, keeping 45 word types\n",
      "2019-06-04 20:45:17,054 : INFO : PROGRESS: at sentence #470000, processed 28863834 words, keeping 45 word types\n",
      "2019-06-04 20:45:17,098 : INFO : PROGRESS: at sentence #480000, processed 29446311 words, keeping 45 word types\n",
      "2019-06-04 20:45:17,142 : INFO : PROGRESS: at sentence #490000, processed 30034312 words, keeping 45 word types\n",
      "2019-06-04 20:45:17,189 : INFO : PROGRESS: at sentence #500000, processed 30628135 words, keeping 45 word types\n",
      "2019-06-04 20:45:17,240 : INFO : PROGRESS: at sentence #510000, processed 31208214 words, keeping 45 word types\n",
      "2019-06-04 20:45:17,287 : INFO : PROGRESS: at sentence #520000, processed 31765157 words, keeping 45 word types\n",
      "2019-06-04 20:45:17,328 : INFO : PROGRESS: at sentence #530000, processed 32320472 words, keeping 45 word types\n",
      "2019-06-04 20:45:17,370 : INFO : PROGRESS: at sentence #540000, processed 32894062 words, keeping 45 word types\n",
      "2019-06-04 20:45:17,414 : INFO : PROGRESS: at sentence #550000, processed 33480442 words, keeping 45 word types\n",
      "2019-06-04 20:45:17,456 : INFO : PROGRESS: at sentence #560000, processed 34063348 words, keeping 45 word types\n",
      "2019-06-04 20:45:17,500 : INFO : PROGRESS: at sentence #570000, processed 34655768 words, keeping 45 word types\n",
      "2019-06-04 20:45:17,545 : INFO : PROGRESS: at sentence #580000, processed 35252292 words, keeping 45 word types\n",
      "2019-06-04 20:45:17,592 : INFO : PROGRESS: at sentence #590000, processed 35843012 words, keeping 45 word types\n",
      "2019-06-04 20:45:17,644 : INFO : PROGRESS: at sentence #600000, processed 36435542 words, keeping 45 word types\n",
      "2019-06-04 20:45:17,694 : INFO : PROGRESS: at sentence #610000, processed 37043357 words, keeping 45 word types\n",
      "2019-06-04 20:45:17,739 : INFO : PROGRESS: at sentence #620000, processed 37641998 words, keeping 45 word types\n",
      "2019-06-04 20:45:17,784 : INFO : PROGRESS: at sentence #630000, processed 38247209 words, keeping 45 word types\n",
      "2019-06-04 20:45:17,832 : INFO : PROGRESS: at sentence #640000, processed 38896925 words, keeping 45 word types\n",
      "2019-06-04 20:45:17,877 : INFO : PROGRESS: at sentence #650000, processed 39495547 words, keeping 45 word types\n",
      "2019-06-04 20:45:17,921 : INFO : PROGRESS: at sentence #660000, processed 40099016 words, keeping 45 word types\n",
      "2019-06-04 20:45:17,966 : INFO : PROGRESS: at sentence #670000, processed 40701909 words, keeping 45 word types\n",
      "2019-06-04 20:45:18,010 : INFO : PROGRESS: at sentence #680000, processed 41301566 words, keeping 45 word types\n",
      "2019-06-04 20:45:18,053 : INFO : PROGRESS: at sentence #690000, processed 41886853 words, keeping 45 word types\n",
      "2019-06-04 20:45:18,099 : INFO : PROGRESS: at sentence #700000, processed 42482958 words, keeping 45 word types\n",
      "2019-06-04 20:45:18,146 : INFO : PROGRESS: at sentence #710000, processed 43098905 words, keeping 45 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-04 20:45:18,195 : INFO : PROGRESS: at sentence #720000, processed 43723028 words, keeping 45 word types\n",
      "2019-06-04 20:45:18,248 : INFO : PROGRESS: at sentence #730000, processed 44339392 words, keeping 45 word types\n",
      "2019-06-04 20:45:18,297 : INFO : PROGRESS: at sentence #740000, processed 44941003 words, keeping 45 word types\n",
      "2019-06-04 20:45:18,342 : INFO : PROGRESS: at sentence #750000, processed 45545893 words, keeping 45 word types\n",
      "2019-06-04 20:45:18,386 : INFO : PROGRESS: at sentence #760000, processed 46144376 words, keeping 45 word types\n",
      "2019-06-04 20:45:18,430 : INFO : PROGRESS: at sentence #770000, processed 46742884 words, keeping 45 word types\n",
      "2019-06-04 20:45:18,475 : INFO : PROGRESS: at sentence #780000, processed 47356066 words, keeping 45 word types\n",
      "2019-06-04 20:45:18,522 : INFO : PROGRESS: at sentence #790000, processed 47984886 words, keeping 45 word types\n",
      "2019-06-04 20:45:18,565 : INFO : PROGRESS: at sentence #800000, processed 48576365 words, keeping 45 word types\n",
      "2019-06-04 20:45:18,613 : INFO : PROGRESS: at sentence #810000, processed 49195745 words, keeping 45 word types\n",
      "2019-06-04 20:45:18,658 : INFO : PROGRESS: at sentence #820000, processed 49805944 words, keeping 45 word types\n",
      "2019-06-04 20:45:18,706 : INFO : PROGRESS: at sentence #830000, processed 50398684 words, keeping 45 word types\n",
      "2019-06-04 20:45:18,759 : INFO : PROGRESS: at sentence #840000, processed 51000851 words, keeping 45 word types\n",
      "2019-06-04 20:45:18,806 : INFO : PROGRESS: at sentence #850000, processed 51599905 words, keeping 45 word types\n",
      "2019-06-04 20:45:18,852 : INFO : PROGRESS: at sentence #860000, processed 52227476 words, keeping 45 word types\n",
      "2019-06-04 20:45:18,897 : INFO : PROGRESS: at sentence #870000, processed 52849578 words, keeping 45 word types\n",
      "2019-06-04 20:45:18,943 : INFO : PROGRESS: at sentence #880000, processed 53467695 words, keeping 45 word types\n",
      "2019-06-04 20:45:18,989 : INFO : PROGRESS: at sentence #890000, processed 54088542 words, keeping 45 word types\n",
      "2019-06-04 20:45:19,037 : INFO : PROGRESS: at sentence #900000, processed 54724007 words, keeping 45 word types\n",
      "2019-06-04 20:45:19,085 : INFO : PROGRESS: at sentence #910000, processed 55353888 words, keeping 45 word types\n",
      "2019-06-04 20:45:19,139 : INFO : PROGRESS: at sentence #920000, processed 55963860 words, keeping 45 word types\n",
      "2019-06-04 20:45:19,195 : INFO : PROGRESS: at sentence #930000, processed 56591389 words, keeping 45 word types\n",
      "2019-06-04 20:45:19,248 : INFO : PROGRESS: at sentence #940000, processed 57193727 words, keeping 45 word types\n",
      "2019-06-04 20:45:19,295 : INFO : PROGRESS: at sentence #950000, processed 57768410 words, keeping 45 word types\n",
      "2019-06-04 20:45:19,340 : INFO : PROGRESS: at sentence #960000, processed 58369054 words, keeping 45 word types\n",
      "2019-06-04 20:45:19,384 : INFO : PROGRESS: at sentence #970000, processed 58956691 words, keeping 45 word types\n",
      "2019-06-04 20:45:19,425 : INFO : PROGRESS: at sentence #980000, processed 59520809 words, keeping 45 word types\n",
      "2019-06-04 20:45:19,470 : INFO : PROGRESS: at sentence #990000, processed 60128758 words, keeping 45 word types\n",
      "2019-06-04 20:45:19,514 : INFO : PROGRESS: at sentence #1000000, processed 60713902 words, keeping 45 word types\n",
      "2019-06-04 20:45:19,557 : INFO : PROGRESS: at sentence #1010000, processed 61293419 words, keeping 45 word types\n",
      "2019-06-04 20:45:19,599 : INFO : PROGRESS: at sentence #1020000, processed 61868386 words, keeping 45 word types\n",
      "2019-06-04 20:45:19,644 : INFO : PROGRESS: at sentence #1030000, processed 62444135 words, keeping 45 word types\n",
      "2019-06-04 20:45:19,689 : INFO : PROGRESS: at sentence #1040000, processed 63023016 words, keeping 45 word types\n",
      "2019-06-04 20:45:19,743 : INFO : PROGRESS: at sentence #1050000, processed 63634050 words, keeping 45 word types\n",
      "2019-06-04 20:45:19,792 : INFO : PROGRESS: at sentence #1060000, processed 64227529 words, keeping 45 word types\n",
      "2019-06-04 20:45:19,834 : INFO : PROGRESS: at sentence #1070000, processed 64792841 words, keeping 45 word types\n",
      "2019-06-04 20:45:19,878 : INFO : PROGRESS: at sentence #1080000, processed 65390355 words, keeping 45 word types\n",
      "2019-06-04 20:45:19,921 : INFO : PROGRESS: at sentence #1090000, processed 65984096 words, keeping 45 word types\n",
      "2019-06-04 20:45:19,965 : INFO : PROGRESS: at sentence #1100000, processed 66571512 words, keeping 45 word types\n",
      "2019-06-04 20:45:20,008 : INFO : PROGRESS: at sentence #1110000, processed 67164955 words, keeping 45 word types\n",
      "2019-06-04 20:45:20,053 : INFO : PROGRESS: at sentence #1120000, processed 67776644 words, keeping 45 word types\n",
      "2019-06-04 20:45:20,097 : INFO : PROGRESS: at sentence #1130000, processed 68371192 words, keeping 45 word types\n",
      "2019-06-04 20:45:20,140 : INFO : PROGRESS: at sentence #1140000, processed 68938957 words, keeping 45 word types\n",
      "2019-06-04 20:45:20,188 : INFO : PROGRESS: at sentence #1150000, processed 69547773 words, keeping 45 word types\n",
      "2019-06-04 20:45:20,236 : INFO : PROGRESS: at sentence #1160000, processed 70091299 words, keeping 45 word types\n",
      "2019-06-04 20:45:20,285 : INFO : PROGRESS: at sentence #1170000, processed 70667670 words, keeping 45 word types\n",
      "2019-06-04 20:45:20,328 : INFO : PROGRESS: at sentence #1180000, processed 71241769 words, keeping 45 word types\n",
      "2019-06-04 20:45:20,370 : INFO : PROGRESS: at sentence #1190000, processed 71818772 words, keeping 45 word types\n",
      "2019-06-04 20:45:20,410 : INFO : PROGRESS: at sentence #1200000, processed 72359874 words, keeping 45 word types\n",
      "2019-06-04 20:45:20,453 : INFO : PROGRESS: at sentence #1210000, processed 72950444 words, keeping 45 word types\n",
      "2019-06-04 20:45:20,498 : INFO : PROGRESS: at sentence #1220000, processed 73550494 words, keeping 45 word types\n",
      "2019-06-04 20:45:20,541 : INFO : PROGRESS: at sentence #1230000, processed 74133065 words, keeping 45 word types\n",
      "2019-06-04 20:45:20,583 : INFO : PROGRESS: at sentence #1240000, processed 74696888 words, keeping 45 word types\n",
      "2019-06-04 20:45:20,628 : INFO : PROGRESS: at sentence #1250000, processed 75277510 words, keeping 45 word types\n",
      "2019-06-04 20:45:20,673 : INFO : PROGRESS: at sentence #1260000, processed 75879083 words, keeping 45 word types\n",
      "2019-06-04 20:45:20,722 : INFO : PROGRESS: at sentence #1270000, processed 76427261 words, keeping 45 word types\n",
      "2019-06-04 20:45:20,772 : INFO : PROGRESS: at sentence #1280000, processed 77000190 words, keeping 45 word types\n",
      "2019-06-04 20:45:20,817 : INFO : PROGRESS: at sentence #1290000, processed 77594188 words, keeping 45 word types\n",
      "2019-06-04 20:45:20,859 : INFO : PROGRESS: at sentence #1300000, processed 78154509 words, keeping 45 word types\n",
      "2019-06-04 20:45:20,900 : INFO : PROGRESS: at sentence #1310000, processed 78720965 words, keeping 45 word types\n",
      "2019-06-04 20:45:20,944 : INFO : PROGRESS: at sentence #1320000, processed 79312082 words, keeping 45 word types\n",
      "2019-06-04 20:45:20,987 : INFO : PROGRESS: at sentence #1330000, processed 79895653 words, keeping 45 word types\n",
      "2019-06-04 20:45:21,031 : INFO : PROGRESS: at sentence #1340000, processed 80496086 words, keeping 45 word types\n",
      "2019-06-04 20:45:21,071 : INFO : PROGRESS: at sentence #1350000, processed 81048444 words, keeping 45 word types\n",
      "2019-06-04 20:45:21,115 : INFO : PROGRESS: at sentence #1360000, processed 81632972 words, keeping 45 word types\n",
      "2019-06-04 20:45:21,159 : INFO : PROGRESS: at sentence #1370000, processed 82224252 words, keeping 45 word types\n",
      "2019-06-04 20:45:21,203 : INFO : PROGRESS: at sentence #1380000, processed 82810977 words, keeping 45 word types\n",
      "2019-06-04 20:45:21,246 : INFO : PROGRESS: at sentence #1390000, processed 83385918 words, keeping 45 word types\n",
      "2019-06-04 20:45:21,291 : INFO : PROGRESS: at sentence #1400000, processed 83957602 words, keeping 45 word types\n",
      "2019-06-04 20:45:21,342 : INFO : PROGRESS: at sentence #1410000, processed 84545055 words, keeping 45 word types\n",
      "2019-06-04 20:45:21,390 : INFO : PROGRESS: at sentence #1420000, processed 85116880 words, keeping 45 word types\n",
      "2019-06-04 20:45:21,432 : INFO : PROGRESS: at sentence #1430000, processed 85682360 words, keeping 45 word types\n",
      "2019-06-04 20:45:21,475 : INFO : PROGRESS: at sentence #1440000, processed 86261129 words, keeping 45 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-04 20:45:21,519 : INFO : PROGRESS: at sentence #1450000, processed 86863397 words, keeping 45 word types\n",
      "2019-06-04 20:45:21,563 : INFO : PROGRESS: at sentence #1460000, processed 87467318 words, keeping 45 word types\n",
      "2019-06-04 20:45:21,607 : INFO : PROGRESS: at sentence #1470000, processed 88049415 words, keeping 45 word types\n",
      "2019-06-04 20:45:21,650 : INFO : PROGRESS: at sentence #1480000, processed 88632588 words, keeping 45 word types\n",
      "2019-06-04 20:45:21,693 : INFO : PROGRESS: at sentence #1490000, processed 89213283 words, keeping 45 word types\n",
      "2019-06-04 20:45:21,736 : INFO : PROGRESS: at sentence #1500000, processed 89788220 words, keeping 45 word types\n",
      "2019-06-04 20:45:21,780 : INFO : PROGRESS: at sentence #1510000, processed 90386448 words, keeping 45 word types\n",
      "2019-06-04 20:45:21,824 : INFO : PROGRESS: at sentence #1520000, processed 90976198 words, keeping 45 word types\n",
      "2019-06-04 20:45:21,867 : INFO : PROGRESS: at sentence #1530000, processed 91560407 words, keeping 45 word types\n",
      "2019-06-04 20:45:21,909 : INFO : PROGRESS: at sentence #1540000, processed 92138992 words, keeping 45 word types\n",
      "2019-06-04 20:45:21,950 : INFO : PROGRESS: at sentence #1550000, processed 92698136 words, keeping 45 word types\n",
      "2019-06-04 20:45:21,993 : INFO : PROGRESS: at sentence #1560000, processed 93275007 words, keeping 45 word types\n",
      "2019-06-04 20:45:22,035 : INFO : PROGRESS: at sentence #1570000, processed 93849717 words, keeping 45 word types\n",
      "2019-06-04 20:45:22,078 : INFO : PROGRESS: at sentence #1580000, processed 94425851 words, keeping 45 word types\n",
      "2019-06-04 20:45:22,122 : INFO : PROGRESS: at sentence #1590000, processed 95014812 words, keeping 45 word types\n",
      "2019-06-04 20:45:22,164 : INFO : PROGRESS: at sentence #1600000, processed 95579828 words, keeping 45 word types\n",
      "2019-06-04 20:45:22,208 : INFO : PROGRESS: at sentence #1610000, processed 96176284 words, keeping 45 word types\n",
      "2019-06-04 20:45:22,254 : INFO : PROGRESS: at sentence #1620000, processed 96790078 words, keeping 45 word types\n",
      "2019-06-04 20:45:22,303 : INFO : PROGRESS: at sentence #1630000, processed 97391912 words, keeping 45 word types\n",
      "2019-06-04 20:45:22,353 : INFO : PROGRESS: at sentence #1640000, processed 97958655 words, keeping 45 word types\n",
      "2019-06-04 20:45:22,400 : INFO : PROGRESS: at sentence #1650000, processed 98539495 words, keeping 45 word types\n",
      "2019-06-04 20:45:22,443 : INFO : PROGRESS: at sentence #1660000, processed 99127207 words, keeping 45 word types\n",
      "2019-06-04 20:45:22,484 : INFO : PROGRESS: at sentence #1670000, processed 99685887 words, keeping 45 word types\n",
      "2019-06-04 20:45:22,526 : INFO : PROGRESS: at sentence #1680000, processed 100247444 words, keeping 45 word types\n",
      "2019-06-04 20:45:22,568 : INFO : PROGRESS: at sentence #1690000, processed 100818108 words, keeping 45 word types\n",
      "2019-06-04 20:45:22,613 : INFO : PROGRESS: at sentence #1700000, processed 101415452 words, keeping 45 word types\n",
      "2019-06-04 20:45:22,655 : INFO : PROGRESS: at sentence #1710000, processed 101991839 words, keeping 45 word types\n",
      "2019-06-04 20:45:22,698 : INFO : PROGRESS: at sentence #1720000, processed 102573070 words, keeping 45 word types\n",
      "2019-06-04 20:45:22,743 : INFO : PROGRESS: at sentence #1730000, processed 103163642 words, keeping 45 word types\n",
      "2019-06-04 20:45:22,787 : INFO : PROGRESS: at sentence #1740000, processed 103726966 words, keeping 45 word types\n",
      "2019-06-04 20:45:22,837 : INFO : PROGRESS: at sentence #1750000, processed 104297546 words, keeping 45 word types\n",
      "2019-06-04 20:45:22,885 : INFO : PROGRESS: at sentence #1760000, processed 104861623 words, keeping 45 word types\n",
      "2019-06-04 20:45:22,926 : INFO : PROGRESS: at sentence #1770000, processed 105419761 words, keeping 45 word types\n",
      "2019-06-04 20:45:22,968 : INFO : PROGRESS: at sentence #1780000, processed 105986739 words, keeping 45 word types\n",
      "2019-06-04 20:45:23,011 : INFO : PROGRESS: at sentence #1790000, processed 106568866 words, keeping 45 word types\n",
      "2019-06-04 20:45:23,052 : INFO : PROGRESS: at sentence #1800000, processed 107125713 words, keeping 45 word types\n",
      "2019-06-04 20:45:23,073 : INFO : collected 45 word types from a corpus of 107404245 raw words and 1804874 sentences\n",
      "2019-06-04 20:45:23,074 : INFO : Loading a fresh vocabulary\n",
      "2019-06-04 20:45:23,074 : INFO : effective_min_count=5 retains 45 unique words (100% of original 45, drops 0)\n",
      "2019-06-04 20:45:23,074 : INFO : effective_min_count=5 leaves 107404245 word corpus (100% of original 107404245, drops 0)\n",
      "2019-06-04 20:45:23,075 : INFO : deleting the raw counts dictionary of 45 items\n",
      "2019-06-04 20:45:23,075 : INFO : sample=0.001 downsamples 32 most-common words\n",
      "2019-06-04 20:45:23,076 : INFO : downsampling leaves estimated 21611501 word corpus (20.1% of prior 107404245)\n",
      "2019-06-04 20:45:23,076 : INFO : estimated required memory for 45 words and 50 dimensions: 40500 bytes\n",
      "2019-06-04 20:45:23,076 : INFO : resetting layer weights\n",
      "/home/curtis/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  \n",
      "2019-06-04 20:45:23,077 : INFO : training model with 12 workers on 45 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-06-04 20:45:24,085 : INFO : EPOCH 1 - PROGRESS: at 6.95% examples, 1550989 words/s, in_qsize 23, out_qsize 0\n",
      "2019-06-04 20:45:25,088 : INFO : EPOCH 1 - PROGRESS: at 13.89% examples, 1555755 words/s, in_qsize 23, out_qsize 0\n",
      "2019-06-04 20:45:26,090 : INFO : EPOCH 1 - PROGRESS: at 20.98% examples, 1562736 words/s, in_qsize 24, out_qsize 0\n",
      "2019-06-04 20:45:27,092 : INFO : EPOCH 1 - PROGRESS: at 28.38% examples, 1567057 words/s, in_qsize 23, out_qsize 0\n",
      "2019-06-04 20:45:28,092 : INFO : EPOCH 1 - PROGRESS: at 35.69% examples, 1568826 words/s, in_qsize 23, out_qsize 0\n",
      "2019-06-04 20:45:29,094 : INFO : EPOCH 1 - PROGRESS: at 42.97% examples, 1573662 words/s, in_qsize 21, out_qsize 2\n",
      "2019-06-04 20:45:30,097 : INFO : EPOCH 1 - PROGRESS: at 50.01% examples, 1573090 words/s, in_qsize 22, out_qsize 1\n",
      "2019-06-04 20:45:31,100 : INFO : EPOCH 1 - PROGRESS: at 57.34% examples, 1573191 words/s, in_qsize 21, out_qsize 2\n",
      "2019-06-04 20:45:32,101 : INFO : EPOCH 1 - PROGRESS: at 64.73% examples, 1574295 words/s, in_qsize 24, out_qsize 2\n",
      "2019-06-04 20:45:33,102 : INFO : EPOCH 1 - PROGRESS: at 72.21% examples, 1573538 words/s, in_qsize 23, out_qsize 1\n",
      "2019-06-04 20:45:34,104 : INFO : EPOCH 1 - PROGRESS: at 79.69% examples, 1573342 words/s, in_qsize 23, out_qsize 0\n",
      "2019-06-04 20:45:35,104 : INFO : EPOCH 1 - PROGRESS: at 87.08% examples, 1572777 words/s, in_qsize 23, out_qsize 0\n",
      "2019-06-04 20:45:36,105 : INFO : EPOCH 1 - PROGRESS: at 94.59% examples, 1573485 words/s, in_qsize 24, out_qsize 2\n",
      "2019-06-04 20:45:36,795 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2019-06-04 20:45:36,797 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2019-06-04 20:45:36,797 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-06-04 20:45:36,799 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-06-04 20:45:36,801 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-06-04 20:45:36,803 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-06-04 20:45:36,804 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-06-04 20:45:36,805 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-06-04 20:45:36,805 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-06-04 20:45:36,806 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-04 20:45:36,807 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-04 20:45:36,807 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-04 20:45:36,808 : INFO : EPOCH - 1 : training on 107404245 raw words (21608977 effective words) took 13.7s, 1574356 effective words/s\n",
      "2019-06-04 20:45:37,813 : INFO : EPOCH 2 - PROGRESS: at 7.01% examples, 1568830 words/s, in_qsize 24, out_qsize 0\n",
      "2019-06-04 20:45:38,813 : INFO : EPOCH 2 - PROGRESS: at 14.01% examples, 1575913 words/s, in_qsize 23, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-04 20:45:39,814 : INFO : EPOCH 2 - PROGRESS: at 21.13% examples, 1578055 words/s, in_qsize 23, out_qsize 0\n",
      "2019-06-04 20:45:40,817 : INFO : EPOCH 2 - PROGRESS: at 28.52% examples, 1576731 words/s, in_qsize 22, out_qsize 1\n",
      "2019-06-04 20:45:41,818 : INFO : EPOCH 2 - PROGRESS: at 35.82% examples, 1576666 words/s, in_qsize 23, out_qsize 0\n",
      "2019-06-04 20:45:42,819 : INFO : EPOCH 2 - PROGRESS: at 43.07% examples, 1578933 words/s, in_qsize 23, out_qsize 0\n",
      "2019-06-04 20:45:43,820 : INFO : EPOCH 2 - PROGRESS: at 50.11% examples, 1577699 words/s, in_qsize 24, out_qsize 0\n",
      "2019-06-04 20:45:44,821 : INFO : EPOCH 2 - PROGRESS: at 57.45% examples, 1577732 words/s, in_qsize 20, out_qsize 4\n",
      "2019-06-04 20:45:45,822 : INFO : EPOCH 2 - PROGRESS: at 64.86% examples, 1578665 words/s, in_qsize 23, out_qsize 0\n",
      "2019-06-04 20:45:46,826 : INFO : EPOCH 2 - PROGRESS: at 72.38% examples, 1578197 words/s, in_qsize 21, out_qsize 2\n",
      "2019-06-04 20:45:47,827 : INFO : EPOCH 2 - PROGRESS: at 79.88% examples, 1578235 words/s, in_qsize 23, out_qsize 0\n",
      "2019-06-04 20:45:48,827 : INFO : EPOCH 2 - PROGRESS: at 87.30% examples, 1577847 words/s, in_qsize 21, out_qsize 2\n",
      "2019-06-04 20:45:49,829 : INFO : EPOCH 2 - PROGRESS: at 94.79% examples, 1577457 words/s, in_qsize 23, out_qsize 0\n",
      "2019-06-04 20:45:50,494 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2019-06-04 20:45:50,494 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2019-06-04 20:45:50,495 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-06-04 20:45:50,499 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-06-04 20:45:50,501 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-06-04 20:45:50,501 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-06-04 20:45:50,502 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-06-04 20:45:50,502 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-06-04 20:45:50,503 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-06-04 20:45:50,504 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-04 20:45:50,505 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-04 20:45:50,505 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-04 20:45:50,506 : INFO : EPOCH - 2 : training on 107404245 raw words (21611551 effective words) took 13.7s, 1578243 effective words/s\n",
      "2019-06-04 20:45:51,512 : INFO : EPOCH 3 - PROGRESS: at 7.00% examples, 1563431 words/s, in_qsize 23, out_qsize 0\n",
      "2019-06-04 20:45:52,516 : INFO : EPOCH 3 - PROGRESS: at 14.01% examples, 1571244 words/s, in_qsize 24, out_qsize 3\n",
      "2019-06-04 20:45:53,518 : INFO : EPOCH 3 - PROGRESS: at 21.13% examples, 1575076 words/s, in_qsize 22, out_qsize 1\n",
      "2019-06-04 20:45:54,522 : INFO : EPOCH 3 - PROGRESS: at 28.58% examples, 1577940 words/s, in_qsize 23, out_qsize 0\n",
      "2019-06-04 20:45:55,522 : INFO : EPOCH 3 - PROGRESS: at 35.95% examples, 1580494 words/s, in_qsize 23, out_qsize 0\n",
      "2019-06-04 20:45:56,524 : INFO : EPOCH 3 - PROGRESS: at 43.21% examples, 1583033 words/s, in_qsize 24, out_qsize 0\n",
      "2019-06-04 20:45:57,524 : INFO : EPOCH 3 - PROGRESS: at 50.28% examples, 1582603 words/s, in_qsize 21, out_qsize 2\n",
      "2019-06-04 20:45:58,526 : INFO : EPOCH 3 - PROGRESS: at 57.69% examples, 1583488 words/s, in_qsize 23, out_qsize 0\n",
      "2019-06-04 20:45:59,526 : INFO : EPOCH 3 - PROGRESS: at 65.14% examples, 1584257 words/s, in_qsize 23, out_qsize 0\n",
      "2019-06-04 20:46:00,526 : INFO : EPOCH 3 - PROGRESS: at 72.45% examples, 1579480 words/s, in_qsize 24, out_qsize 6\n",
      "2019-06-04 20:46:01,529 : INFO : EPOCH 3 - PROGRESS: at 79.83% examples, 1576578 words/s, in_qsize 23, out_qsize 0\n",
      "2019-06-04 20:46:02,532 : INFO : EPOCH 3 - PROGRESS: at 87.30% examples, 1576810 words/s, in_qsize 23, out_qsize 0\n",
      "2019-06-04 20:46:03,535 : INFO : EPOCH 3 - PROGRESS: at 94.85% examples, 1577550 words/s, in_qsize 24, out_qsize 0\n",
      "2019-06-04 20:46:04,203 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2019-06-04 20:46:04,206 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2019-06-04 20:46:04,209 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-06-04 20:46:04,210 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-06-04 20:46:04,212 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-06-04 20:46:04,212 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-06-04 20:46:04,212 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-06-04 20:46:04,213 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-06-04 20:46:04,213 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-06-04 20:46:04,214 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-04 20:46:04,214 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-04 20:46:04,215 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-04 20:46:04,215 : INFO : EPOCH - 3 : training on 107404245 raw words (21612168 effective words) took 13.7s, 1576940 effective words/s\n",
      "2019-06-04 20:46:05,220 : INFO : EPOCH 4 - PROGRESS: at 7.03% examples, 1569353 words/s, in_qsize 23, out_qsize 0\n",
      "2019-06-04 20:46:06,221 : INFO : EPOCH 4 - PROGRESS: at 14.00% examples, 1573178 words/s, in_qsize 24, out_qsize 0\n",
      "2019-06-04 20:46:07,226 : INFO : EPOCH 4 - PROGRESS: at 21.21% examples, 1580396 words/s, in_qsize 23, out_qsize 0\n",
      "2019-06-04 20:46:08,227 : INFO : EPOCH 4 - PROGRESS: at 28.70% examples, 1584654 words/s, in_qsize 24, out_qsize 0\n",
      "2019-06-04 20:46:09,230 : INFO : EPOCH 4 - PROGRESS: at 36.08% examples, 1585528 words/s, in_qsize 23, out_qsize 0\n",
      "2019-06-04 20:46:10,231 : INFO : EPOCH 4 - PROGRESS: at 43.25% examples, 1584320 words/s, in_qsize 24, out_qsize 1\n",
      "2019-06-04 20:46:11,234 : INFO : EPOCH 4 - PROGRESS: at 50.42% examples, 1586859 words/s, in_qsize 23, out_qsize 0\n",
      "2019-06-04 20:46:12,237 : INFO : EPOCH 4 - PROGRESS: at 57.79% examples, 1585297 words/s, in_qsize 23, out_qsize 0\n",
      "2019-06-04 20:46:13,239 : INFO : EPOCH 4 - PROGRESS: at 65.24% examples, 1585717 words/s, in_qsize 23, out_qsize 1\n",
      "2019-06-04 20:46:14,240 : INFO : EPOCH 4 - PROGRESS: at 72.76% examples, 1584655 words/s, in_qsize 23, out_qsize 0\n",
      "2019-06-04 20:46:15,243 : INFO : EPOCH 4 - PROGRESS: at 80.25% examples, 1583796 words/s, in_qsize 24, out_qsize 0\n",
      "2019-06-04 20:46:16,243 : INFO : EPOCH 4 - PROGRESS: at 87.81% examples, 1585290 words/s, in_qsize 23, out_qsize 0\n",
      "2019-06-04 20:46:17,244 : INFO : EPOCH 4 - PROGRESS: at 95.28% examples, 1583991 words/s, in_qsize 24, out_qsize 0\n",
      "2019-06-04 20:46:17,852 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2019-06-04 20:46:17,854 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2019-06-04 20:46:17,855 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-06-04 20:46:17,855 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-06-04 20:46:17,857 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-06-04 20:46:17,859 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-06-04 20:46:17,860 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-06-04 20:46:17,860 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-06-04 20:46:17,861 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-06-04 20:46:17,861 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-04 20:46:17,862 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-04 20:46:17,862 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-04 20:46:17,863 : INFO : EPOCH - 4 : training on 107404245 raw words (21605973 effective words) took 13.6s, 1583587 effective words/s\n",
      "2019-06-04 20:46:18,869 : INFO : EPOCH 5 - PROGRESS: at 7.08% examples, 1581190 words/s, in_qsize 24, out_qsize 1\n",
      "2019-06-04 20:46:19,870 : INFO : EPOCH 5 - PROGRESS: at 14.12% examples, 1588502 words/s, in_qsize 24, out_qsize 4\n",
      "2019-06-04 20:46:20,870 : INFO : EPOCH 5 - PROGRESS: at 21.27% examples, 1586355 words/s, in_qsize 24, out_qsize 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-04 20:46:21,871 : INFO : EPOCH 5 - PROGRESS: at 28.70% examples, 1586579 words/s, in_qsize 23, out_qsize 0\n",
      "2019-06-04 20:46:22,873 : INFO : EPOCH 5 - PROGRESS: at 36.06% examples, 1586814 words/s, in_qsize 23, out_qsize 0\n",
      "2019-06-04 20:46:23,877 : INFO : EPOCH 5 - PROGRESS: at 43.31% examples, 1587428 words/s, in_qsize 23, out_qsize 0\n",
      "2019-06-04 20:46:24,877 : INFO : EPOCH 5 - PROGRESS: at 50.39% examples, 1586541 words/s, in_qsize 24, out_qsize 0\n",
      "2019-06-04 20:46:25,881 : INFO : EPOCH 5 - PROGRESS: at 57.67% examples, 1582568 words/s, in_qsize 20, out_qsize 3\n",
      "2019-06-04 20:46:26,883 : INFO : EPOCH 5 - PROGRESS: at 65.16% examples, 1584271 words/s, in_qsize 22, out_qsize 1\n",
      "2019-06-04 20:46:27,888 : INFO : EPOCH 5 - PROGRESS: at 72.78% examples, 1585298 words/s, in_qsize 23, out_qsize 2\n",
      "2019-06-04 20:46:28,890 : INFO : EPOCH 5 - PROGRESS: at 80.30% examples, 1585092 words/s, in_qsize 21, out_qsize 2\n",
      "2019-06-04 20:46:29,892 : INFO : EPOCH 5 - PROGRESS: at 87.78% examples, 1585125 words/s, in_qsize 23, out_qsize 0\n",
      "2019-06-04 20:46:30,895 : INFO : EPOCH 5 - PROGRESS: at 95.40% examples, 1586024 words/s, in_qsize 23, out_qsize 0\n",
      "2019-06-04 20:46:31,479 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2019-06-04 20:46:31,480 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2019-06-04 20:46:31,482 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-06-04 20:46:31,486 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-06-04 20:46:31,487 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-06-04 20:46:31,488 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-06-04 20:46:31,488 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-06-04 20:46:31,488 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-06-04 20:46:31,488 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-06-04 20:46:31,489 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-04 20:46:31,490 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-04 20:46:31,490 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-04 20:46:31,490 : INFO : EPOCH - 5 : training on 107404245 raw words (21611796 effective words) took 13.6s, 1586521 effective words/s\n",
      "2019-06-04 20:46:31,491 : INFO : training on a 537021225 raw words (108050465 effective words) took 68.4s, 1579382 effective words/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(108050465, 537021225)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "both_pos = train_pos\n",
    "np.append(both_pos,test_pos)\n",
    "\n",
    "w2v_model = gensim.models.Word2Vec(iter=5, workers=n_cores,size=POS_VEC_SIZE)  # an empty model, no training yet\n",
    "w2v_model.build_vocab(both_pos)  # can be a non-repeatable, 1-pass generator\n",
    "w2v_model.train(both_pos,\n",
    "            total_examples=w2v_model.corpus_count,\n",
    "            epochs=w2v_model.iter)  # can be a non-repeatable, 1-pass generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(list(train['pos']) + list(test['pos']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 50\n",
    "x_train = tokenizer.texts_to_sequences(train['pos'])\n",
    "x_test = tokenizer.texts_to_sequences(test['pos'])\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=MAX_LEN)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.where(train['target'] >= 0.5, 1, 0)\n",
    "# note: not using sexual_explicit\n",
    "y_aux_train = train[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.87234</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     target  severe_toxicity  obscene  identity_attack   insult  threat\n",
       "0  0.000000         0.000000      0.0         0.000000  0.00000     0.0\n",
       "1  0.000000         0.000000      0.0         0.000000  0.00000     0.0\n",
       "2  0.000000         0.000000      0.0         0.000000  0.00000     0.0\n",
       "3  0.000000         0.000000      0.0         0.000000  0.00000     0.0\n",
       "4  0.893617         0.021277      0.0         0.021277  0.87234     0.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_aux_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n unknown words (w2v):  34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/curtis/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "w2v_matrix, w2v_unknown_words = build_matrix(tokenizer.word_index, w2v_model)\n",
    "print('n unknown words (w2v): ', len(w2v_unknown_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(data, targets):\n",
    "    ''' Define custom loss function for weighted BCE on 'target' column '''\n",
    "    bce_loss_1 = torch.nn.BCEWithLogitsLoss(weight=targets[:,1:2])(data[:,:1],targets[:,:1])\n",
    "    bce_loss_2 = torch.nn.BCEWithLogitsLoss()(data[:,1:],targets[:,2:])\n",
    "    return (bce_loss_1 * loss_weight) + bce_loss_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model  0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-33e46ab6362a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m                                 \u001b[0moutput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train_torch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m                                 loss_fn=custom_loss)\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-d4b2eeb11036>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train, test, loss_fn, output_dim, lr, batch_size, n_epochs, n_epochs_embed, enable_checkpoint_ensemble)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-33e46ab6362a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;31m#training using training and validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             model = train_model(model, train_dataset, val_dataset, output_dim=y_train_torch.shape[-1], \n\u001b[0;32m---> 59\u001b[0;31m                                  loss_fn=custom_loss)\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;31m#prediction on validation set (used for score measurement)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-d4b2eeb11036>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train, test, loss_fn, output_dim, lr, batch_size, n_epochs, n_epochs_embed, enable_checkpoint_ensemble)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;31m#training loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "all_val_preds = []\n",
    "all_test_preds = []\n",
    "\n",
    "#Add in K fold \n",
    "random_state = 2019\n",
    "    \n",
    "#K fold splits\n",
    "skf = KFold(n_splits=NUM_SPLITS, shuffle=True, random_state=random_state)\n",
    "splits = list(skf.split(x_train, y_train))\n",
    "\n",
    "#final validation predictions\n",
    "final_val_preds = np.zeros((x_train.shape[0]))\n",
    "\n",
    "#final test predictions to be stored in this var\n",
    "final_test_preds = np.zeros((x_test.shape[0]))\n",
    "\n",
    "start_time = time.time()\n",
    "for fold in range(NUM_SPLITS):\n",
    "    tr_ind, val_ind = splits[fold]\n",
    "    all_val_preds = []\n",
    "    all_test_preds = []\n",
    "    \n",
    "    x_training = x_train[tr_ind]\n",
    "    y_training = y_train[tr_ind]\n",
    "    y_aux_training = y_aux_train.values[tr_ind]\n",
    "    \n",
    "    x_val = x_train[val_ind]\n",
    "    y_val = y_train[val_ind]\n",
    "    y_aux_val = y_aux_train.values[val_ind]\n",
    "    \n",
    "    x_train_torch = torch.tensor(x_training, dtype=torch.long).cuda()\n",
    "    x_val_torch = torch.tensor(x_val, dtype=torch.long).cuda()\n",
    "    y_train_torch = torch.tensor(np.hstack([y_training[:, np.newaxis], y_aux_training]), dtype=torch.float32).cuda()\n",
    "    x_test_torch = torch.tensor(x_test, dtype=torch.long).cuda()\n",
    "    \n",
    "    train_dataset = data.TensorDataset(x_train_torch, y_train_torch)\n",
    "    \n",
    "    val_dataset = data.TensorDataset(x_val_torch)\n",
    "    test_dataset = data.TensorDataset(x_test_torch)\n",
    "\n",
    "    for model_idx in range(NUM_MODELS):\n",
    "        print('Model ', model_idx)\n",
    "        seed_everything(1234 + model_idx)\n",
    "\n",
    "        model = NeuralNet(w2v_matrix, y_aux_train.shape[-1])\n",
    "        \n",
    "        try:\n",
    "            model.cuda()\n",
    "            #training using training and validation set\n",
    "            model = train_model(model,\n",
    "                                train_dataset,\n",
    "                                val_dataset,\n",
    "                                output_dim=y_train_torch.shape[-1], \n",
    "                                loss_fn=custom_loss)\n",
    "        except:\n",
    "            model.cuda()\n",
    "            #training using training and validation set\n",
    "            model = train_model(model, train_dataset, val_dataset, output_dim=y_train_torch.shape[-1], \n",
    "                                 loss_fn=custom_loss)\n",
    "        \n",
    "        #prediction on validation set (used for score measurement)\n",
    "        val_pred = predict(model, val_dataset, output_dim=y_train_torch.shape[-1]) #val preds on the val split\n",
    "        all_val_preds.append(val_pred)\n",
    "        \n",
    "        #prediction on entire test set (actual predictions to be submitted)\n",
    "        test_pred = predict(model, test_dataset, output_dim=y_train_torch.shape[-1])\n",
    "        all_test_preds.append(test_pred)\n",
    "        \n",
    "        print()\n",
    "        \n",
    "    #average validation prediction amongst all models\n",
    "    avg_val = np.mean(all_val_preds, axis=0)[:, 0] #will be printed out per split\n",
    "    final_val_preds[val_ind] += avg_val\n",
    "    \n",
    "    avg_test = np.mean(all_test_preds, axis=0)[:, 0]\n",
    "    \n",
    "    final_test_preds += avg_test\n",
    "\n",
    "    y_true = y_train[val_ind] #true scores for this validation set\n",
    "    y_identity = y_train_identity[val_ind] #true scores for the identity groups for this validation set\n",
    "    evaluator = JigsawEvaluator(y_true, y_identity)\n",
    "    auc_score = evaluator.get_final_metric(avg_val)\n",
    "\n",
    "    roc_score = roc_auc_score(y_train[val_ind], avg_val)\n",
    "    print('Kaggle Score: ', auc_score)\n",
    "    print('ROC score: ', roc_score)\n",
    "    \n",
    "    del x_train_torch\n",
    "    del x_val_torch\n",
    "    del y_train_torch\n",
    "    del x_test_torch\n",
    "    del train_dataset\n",
    "    del val_dataset\n",
    "    del test_dataset\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print('=============End-of-Fold================')\n",
    "    \n",
    "end_time = time.time()\n",
    "print('Time: ', end_time - start_time)\n",
    "\n",
    "#Final combined score\n",
    "y_true = y_train\n",
    "y_identity = y_train_identity\n",
    "evaluator = JigsawEvaluator(y_true, y_identity)\n",
    "auc_\n",
    "score = evaluator.get_final_metric(final_val_preds)\n",
    "print('Final Kaggle Score: ', auc_score)\n",
    "print('Final ROC score: ', roc_auc_score(y_train, final_val_preds))\n",
    "\n",
    "#average test predictions AGAIN this time by number of splits\n",
    "final_test_preds /= NUM_SPLITS\n",
    "#print(final_test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
